% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi

\author{}
\date{}

\begin{document}

{
\setcounter{tocdepth}{3}
\tableofcontents
}
\title{MLT Week-4}
\author{Sherry Thomas \\ 21f3001449}

\maketitle
\tableofcontents

\begin{abstract}
The week introduces estimators, and delves deeper into topics like Maximum Likelihood Estimator and Bayesian Estimator. Later, it goes into Gaussian Mixture Models and its implementation.
\end{abstract}

\hypertarget{introduction-to-estimation}{%
\section{Introduction to Estimation}\label{introduction-to-estimation}}

Estimators in machine learning are algorithms or models used to estimate
unknown parameters or predict outcomes based on data. The aim of the
method is to find/predict the unknow parameters describing the
distribution of the data.

Let \(\{x_1, x_2, \ldots, x_n\}\) be a dataset where
\(x_i \in \{0,1\}\). We assume that the datapoints are independent and
identically distributed.

Independence means \(P(x_i|x_j) = P(x_i)\). Identically distributed
means \(P(x_i)=P(x_j)=p\).

\hypertarget{maximum-likelihood-estimation}{%
\section{Maximum Likelihood
Estimation}\label{maximum-likelihood-estimation}}

\hypertarget{fishers-principle-of-maximum-likelihood}{%
\subsection{Fisher's Principle of Maximum
Likelihood}\label{fishers-principle-of-maximum-likelihood}}

Fisher's principle of maximum likelihood is a statistical method used to
estimate the parameters of a statistical model by choosing the parameter
values that maximize the likelihood function, which measures how well
the model fits the observed data.

Applying the likelihood function on the above dataset, we get
\begin{align*}
\mathcal{L}(p;\{x_1, x_2, \ldots, x_n\}) &= P(x_1, x_2, \ldots, x_n;p)\\
&=p(x_1;p)p(x_2;p)\ldots p(x_n;p) \\
&=\prod _{i=1} ^n {p^{x_i}(1-p)^{1-x_i}}
\end{align*} \begin{align*}
\therefore \log(\mathcal{L}(p;\{x_1, x_2, \ldots, x_n\})) &=\underset{p} {\arg \max}\log \left ( \prod _{i=1} ^n {p^{x_i}(1-p)^{1-x_i}} \right ) \\
\text{Differentiating wrt $p$, we get}\\
\therefore \hat{p}_{ML} &= \frac{1}{n}\sum _{i=1} ^n x_i
\end{align*}

\hypertarget{likelihood-estimation-on-gaussian-distributions}{%
\subsection{Likelihood Estimation on Gaussian
Distributions}\label{likelihood-estimation-on-gaussian-distributions}}

Let \(\{x_1, x_2, \ldots, x_n\}\) be a dataset where
\(x_i \sim \mathcal{N}(\mu,\sigma^2)\). We assume that the datapoints
are independent and identically distributed.

\begin{align*}
\mathcal{L}(\mu, \sigma^2;\{x_1, x_2, \ldots, x_n\}) &= f_{x_1, x_2, \ldots, x_n}(x_1, x_2, \ldots, x_n;\mu, \sigma^2) \\
&=\prod _{i=1} ^n  f_{x_i}(x_i;\mu, \sigma^2) \\
&=\prod _{i=1} ^n \left [ \frac{1}{\sqrt{2\pi}\sigma} e^{\frac{-(x_i-\mu)^2}{2\sigma^2}} \right ] \\
\therefore \log(\mathcal{L}(p;\{x_1, x_2, \ldots, x_n\})) &= \sum _{i=1} ^n \left[ \log \left (\frac{1}{\sqrt{2\pi}\sigma}  \right ) - \frac{(x_i-\mu)^2}{2\sigma^2} \right] \\
\end{align*} \[
\text{Differentiating wrt $\mu$ and $\sigma$, we get}
\] \begin{align*}
\hat{\mu}_{ML} &= \frac{1}{n}\sum _{i=1} ^n x_i \\
\hat{\sigma^2}_{ML} &= \frac{1}{n}\sum _{i=1} ^n (x_i-\mu)^2
\end{align*}

\hypertarget{bayesian-estimation}{%
\section{Bayesian Estimation}\label{bayesian-estimation}}

Bayesian estimation is a statistical method that updates the estimates
of model parameters by combining prior knowledge or beliefs with
observed data to calculate the posterior probability distribution of the
parameters.

Let \(\{x_1, x_2, \ldots, x_n\}\) be a dataset where \(x_i\) belongs to
a distribution with parameters \(\theta\). We assume that the datapoints
are independent and identically distributed. We also assume that
\(\theta\) is also from a probability distribution.

Our goal is to update the parameters using the data.

ie. \[
P(\theta)=>P(\theta|\{x_1, x_2, \ldots, x_n\})
\] where, using Bayes Law, we get \[
P(\theta|\{x_1, x_2, \ldots, x_n\})=\left ( \frac{P(\{x_1, x_2, \ldots, x_n\}|\theta)}{P(\{x_1, x_2, \ldots, x_n\})} \right )*P(\theta)
\]

Let \(\{x_1, x_2, \ldots, x_n\}\) be a dataset where \(x_i \in \{0,1\}\)
with parameter \(\theta\). What distribution can be suited for
\(P(\theta)\).

A commonly used distributions for priors is the Beta Distribution. \[
f(p;\alpha,\beta) = \frac{p^{\alpha-1}(1-p)^{\beta-1}}{z} \hspace{2em} \forall p \in [0,1] \\
\] \[
\text{where $z$ is a normalizing factor}
\]

Hence, using the Beta Distribution as the Prior, we get, \begin{align*}
P(\theta|\{x_1, x_2, \ldots, x_n\}) &\propto P(\theta|\{x_1, x_2, \ldots, x_n\})*P(\theta) \\
f_{\theta|\{x_1, x_2, \ldots, x_n\}}(p) &\propto \left [ \prod _{i=1} ^n {p^{x_i}(1-p)^{1-x_i}} \right ]*\left [ p^{\alpha-1}(1-p)^{\beta-1} \right ] \\
f_{\theta|\{x_1, x_2, \ldots, x_n\}}(p) &\propto p^{\sum _{i=1} ^n x_i + \alpha - 1}(1-p)^{\sum _{i=1} ^n(1-x_i) + \beta - 1}
\end{align*} i.e.~we get, \[
\text{BETA PRIOR }(\alpha, \beta) \xrightarrow[Bernoulli]{\{x_1, x_2, \ldots, x_n\}} \text{BETA POSTERIOR }(\alpha + n_h, \beta + n_t)
\] \[
\therefore \hat{p_{ML}} = \mathbb{E}[Posterior]=\mathbb{E}[Beta(\alpha +n_h, \beta + n_t)]= \frac{\alpha + n_h}{\alpha + n_h + \beta + n_t}
\]

\hypertarget{gaussian-mixture-models}{%
\section{Gaussian Mixture Models}\label{gaussian-mixture-models}}

Gaussian Mixture Models are a type of probabilistic model used to
represent complex data distributions by combining multiple Gaussian
distributions.

The procedure is as follows:

\begin{itemize}
\tightlist
\item
  Step 1: Generate a mixture component among \(\{1, 2, \ldots, K\}\)
  where \(z_i \in \{1, 2, \ldots, K\}\). We get, \[
  P(z_i=k) = \pi_k \hspace{2em} \left [ \sum _{i=1} ^K \pi_i = 1 \hspace{1em} 0 \le \pi_i \le 1 \hspace{1em} \forall i \right ]
  \]
\item
  Step 2: Generate \(x_i \sim \mathcal{N}(\mu_{z_i}, \sigma^2_{z_i})\)
\end{itemize}

Hence, there are \(3K\) parameters. Let these parameters be represented
by \(\theta\).

\hypertarget{likelihood-of-gmms}{%
\section{Likelihood of GMM's}\label{likelihood-of-gmms}}

\begin{align*}
\mathcal{L}\left( \begin{array}{cccc}
\mu_1, \mu_2, \ldots, \mu_K \\
\sigma^2_1, \sigma^2_2, \ldots, \sigma^2_K\\
\pi_1, \pi_2, \ldots, \pi_K
\end{array}; x_1, x_2, \ldots, x_n \right )
&= \prod _{i=1} ^n f_{mix} \left( x_i; \begin{array}{cccc}
\mu_1, \mu_2, \ldots, \mu_K \\
\sigma^2_1, \sigma^2_2, \ldots, \sigma^2_K\\
\pi_1, \pi_2, \ldots, \pi_K
\end{array} \right ) \\
&= \prod _{i=1} ^n \left [ \sum _{k=1} ^K \pi_k * f_{mix}(x_i; \mu_k, \sigma_k) \right ] \\
\therefore \log\mathcal{L}(\theta) &= \sum _{i=1} ^n \log \left [ \sum _{k=1} ^K \pi_k * \frac{1}{\sqrt{2\pi}\sigma_k} e^{\frac{-(x_i-\mu_k)^2}{2\sigma^2_k}} \right ] \\
\end{align*} To solve the above equation, we need to understand
convexity.

\hypertarget{convexity-and-jensens-inequality}{%
\subsection{Convexity and Jensen's
Inequality}\label{convexity-and-jensens-inequality}}

Convexity is a property of a function or set that implies a unique line
segment can be drawn between any two points within the function or set.
For a concave function, this property can be expressed as, \[
f \left (\sum _{k=1} ^K \lambda_k a_k \right ) \ge \sum _{k=1} ^K \lambda_k f(a_k)
\] where \[
\sum _{k=1} ^K \lambda _k = 1 
\] \[
a_k \text{ are points of the function}
\] This is also known as \textbf{Jensen's Inequality}.

\hypertarget{estimating-the-parameters}{%
\section{Estimating the Parameters}\label{estimating-the-parameters}}

As log is a concave function, using the above equation, we can also
approximate the likelihood function for GMM's as follows, \[
\log\mathcal{L}(\theta) = \sum _{i=1} ^n \log \left [ \sum _{k=1} ^K \pi_k * \frac{1}{\sqrt{2\pi}\sigma_k} e^{\frac{-(x_i-\mu_k)^2}{2\sigma^2_k}} \right ]
\] Introduce for data point \(x_i\), the parameters
\(\{\lambda_1^i, \lambda_2^i, \ldots, \lambda_k^i\}\) s.t.
\(\forall i,k \displaystyle \sum _{k=1} ^K \lambda _k^i = 1; 0 \le \lambda _k^i \le 1\).
\[
\log\mathcal{L}(\theta) = \sum _{i=1} ^n \log \left [ \sum _{k=1} ^K \lambda _k ^i \left ( \pi_k * \frac{1}{\lambda _k ^i\sqrt{2\pi}\sigma_k} e^{\frac{-(x_i-\mu_k)^2}{2\sigma^2_k}} \right ) \right ]
\] \[ \text{Using Jensen's Inequality, we get} \] \[
\log\mathcal{L}(\theta) \ge \text{modified\_log}\mathcal{L}(\theta)
\] \begin{align}
\therefore \text{modified\_log}\mathcal{L}(\theta) 
&= \sum _{i=1} ^n \sum _{i=k} ^K \lambda _k ^i \log \left ( \pi_k * \frac{1}{\lambda _k ^i\sqrt{2\pi}\sigma_k} e^{\frac{-(x_i-\mu_k)^2}{2\sigma^2_k}} \right )
\end{align} Note that the modified-log likelihood function gives a lower
bound for the true log likelihood function at \(\theta\). Finally, to
get the parameters, we do the following,

\begin{itemize}
\tightlist
\item
  To get \(\theta\): Fix \(\lambda\) and maximize over \(\theta\). \[
  \underset{\theta} {\max} \sum _{i=1} ^n \sum _{i=k} ^K \lambda _k ^i \log \left ( \pi_k * \frac{1}{\lambda _k ^i\sqrt{2\pi}\sigma_k} e^{\frac{-(x_i-\mu_k)^2}{2\sigma^2_k}} \right )
  \] \[
  \text{Differentiate w.r.t. }\mu,\sigma^2,\text{ and }\pi \text{ to get the following}
  \] \begin{align*}
  \hat{\mu_k^{\scriptsize{MML}}} &= \frac{\displaystyle\sum _{i=1} ^n \lambda_k^i x_i}{\displaystyle\sum _{i=1} ^n \lambda_k^i} \\
  \hat{\sigma_k^{2^{\scriptsize{MML}}}} &= \frac{\displaystyle\sum _{i=1} ^n \lambda_k^i (x_i-\hat{\mu_k^{\scriptsize{MML}}})^2}{\displaystyle\sum _{i=1} ^n \lambda_k^i} \\
  \hat{\pi_k^{\scriptsize{MML}}} &= \frac{\displaystyle\sum _{i=1} ^n \lambda_k^i}{n} \\
  \end{align*}
\item
  To get \(\lambda\): Fix \(\theta\) and maximize over \(\lambda\). For
  any \(i\): \[
  \underset{\lambda_1^i, \lambda_2^i, \ldots, \lambda_k^i} {\max} 
  \sum _{k=1} ^K \left [ \lambda _k ^i \log \left ( \pi_k * \frac{1}{\sqrt{2\pi}\sigma_k} e^{\frac{-(x_i-\mu_k)^2}{2\sigma^2_k}} \right ) -  \lambda _k ^i \log( \lambda _k ^i) \right ] \hspace{1em} s.t. \hspace{1em} \sum _{k=1} ^K \lambda _k ^i = 1;  0 \le \lambda _k ^i \le 1
  \] \[
  \text{Solving the above constrained optimization problem analytically, we get}
  \] \begin{align*}
  \hat{\lambda_k^{i^{\scriptsize{MML}}}}
  &= \frac{\left ( \frac{1}{\sqrt{2\pi}\sigma_k} e^{\frac{-(x_i-\mu_k)^2}{2\sigma^2_k}} \right ) * \pi_k}{ \displaystyle \sum _{k=1} ^K \left ( \frac{1}{\sqrt{2\pi}\sigma_k} e^{\frac{-(x_i-\mu_k)^2}{2\sigma^2_k}} * \pi_k \right )} \\
  \text{i.e. }\hat{\lambda_k^{i^{\scriptsize{MML}}}}
  &= \frac{f(x_i|z_i=k)*P(z_i=k)}{P(x_i)} \\
  \hat{\lambda_k^{i^{\scriptsize{MML}}}}
  &= \frac{f(x_i|z_i=k)*P(z_i=k)}{\displaystyle \sum _{k=1} ^K f(x_i|z_i=k)*P(z_i=k)} \\
  \end{align*}
\end{itemize}

\hypertarget{em-algorithm}{%
\section{EM Algorithm}\label{em-algorithm}}

The EM (Expectation-Maximization) algorithm is a popular method for
estimating the parameters of statistical models with incomplete data by
iteratively alternating between expectation and maximization steps until
convergence to a stable solution.

The algorithm is as follows:

\begin{itemize}
\tightlist
\item
  Initialize
  \(\theta^0 = \left \{ \begin{array}{cccc} \mu_1, \mu_2, \ldots, \mu_K \\ \sigma^2_1, \sigma^2_2, \ldots, \sigma^2_K\\ \pi_1, \pi_2, \ldots, \pi_K \end{array} \right \}\)
  using Lloyd's algorithm.
\item
  Until convergence (\(||\theta^{t+1}-\theta^{t} || \le \epsilon\) where
  \(\epsilon\) is the tolerance parameter) do the following:
  \begin{align*}
  \lambda^{t+1} &= \underset{\lambda}{\arg \max} \text{ modified\_log}(\theta^t, \textcolor{red}{\lambda}) &\rightarrow \text{ Expectation Step}\\
  \theta^{t+1} &= \underset{\theta}{\arg \max} \text{ modified\_log}(\textcolor{red}{\theta}, \lambda^{t+1}) &\rightarrow \text{ Maximization Step}\\
  \end{align*}
\end{itemize}

EM algorithm produces soft clustering. For hard clustering using EM, a
further step is involved:

\begin{itemize}
\tightlist
\item
  For a point \(x_i\), assign it to a cluster using the following
  equation: \[
  z_i = \underset{k}{\arg\max} \lambda_k^i
  \]
\end{itemize}

\hypertarget{credits}{%
\section{Credits:}\label{credits}}

Professor Arun Rajkumar: The content as well as the notations are from
his slides and lecture.

\end{document}
