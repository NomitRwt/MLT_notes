\documentclass[letterpaper,11pt]{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{color}
\usepackage{xcolor}
\usepackage{url}
\usepackage{textcomp}
\usepackage{parskip}

\title{MLT Week-2}
\author{Sherry Thomas \\ 21f3001449}

\begin{document}
\maketitle
\tableofcontents

\begin{abstract}
The week's discourse concentrates on the two primary challenges inherent in Principal Component Analysis (PCA) and endeavors to provide solutions. The solutions are achieved through the utilization of kernel functions and culminates in a more comprehensive and generalized algorithm for PCA.
\end{abstract}

\newpage
\section{Introduction}
For a given dataset $D \in \mathbb{R} ^{d \times n}$, the covariance matrix is $C \in \mathbb{R} ^{d \times d}$. PCA for this dataset can have the following two problems:
\begin{enumerate}
    \item 
    Time Complexity: The algorithimic complexity of finding the eigenvalues and eigenvectors of $C$ is $O(d ^3)$. Hence, as $d$ grows, the time taken becomes very large.
    \item 
    Non-Linear Dataset: The dataset may lie in a non-linear subspace. As PCA tries to get linear combination of Principal Components, non-linear datasets may result in non-optimal outputs.  
\end{enumerate}

\section{Reducing the Time Complexity to find Eigenvalues and Eigenvectors}
Let's take a dataset $X$ with a large number of features($d$) i.e. $[d >> n]$ where \\ $d$: no. of features \\ $n$: no. of datapoints
$$
X=\left [
\begin{array}{ccccc}
    | & | & | & & | \\
    x_1 & x_2 & x_3 & \ldots & x_n \\
    | & | & | & & |
\end{array}
\right ]
X \in \mathbb{R} ^{d \times n}
$$ \\
The covariance matrix $C$ of $X$ is given by,
$$
C=\frac{1}{n}(X  X ^T) \text{\hspace{2em}where } C \in \mathbb{R} ^{d \times d} 
\text{\hspace{2em} \ldots}[1]
$$ 
Let $w_k$ be the the eigenvector corresponding to the $k ^{th}$ largest eigenvalue $\lambda _k$ of $C$. \\
We know
$$
Cw_k=\lambda _k w_k
$$
Substituting $C$ from $[1]$ in the above equation, and solving for $w_k$, we get
$$
w_k = \sum _{i=1} ^n (\frac{x_i ^T w_k}{n \lambda _k})\cdot{x_i}
$$
\newpage
$\therefore$ $w_k$ is a linear combination of datapoints! Hence, we can say
$$
w_k= X \alpha _k \text{\hspace{2em} for some } \alpha _k \hspace{2em} \ldots [2]
$$
Let $X ^T X = K$ where $K \in \mathbb{R} ^{n \times n}$. We shall use this to solve for $\alpha _k$. After some algebra (Refer to lectures), we get:
$$
K \alpha _k = (n \lambda _k) \alpha _k
$$
We know that the non-zero eigenvalues of $XX^T$ and $X^T X$ are the same according to the Spectral Theorm.\\ 
\textbf{Explanation for the above}: If $\lambda \ne 0$ is an eigenvalue of $XX^T$ with eigenvector $w$, then $XX^Tw=\lambda w$. \\
Multiply both sides by $X^T$ to get $$X^TX(X^Tw)=\lambda X^Tw$$
But then we see that $\lambda$ is still an eigenvalue for $X^TX$ and the corresponding eigenvector is simply $X^Tw$. \\ \\
Let $\beta _k$ be the the eigenvector corresponding to the $k ^{th}$ largest eigenvalue $n \lambda _k$ of $K$. \\
On solving the eigen equation for $K$, we get
$$
\alpha _k= \frac{\beta _k}{\sqrt{n \lambda _k}} \hspace{2em} \ldots [3]
$$
Using equations [2] and [3], we can get the eigenvalues and eigenvectors of $C$ using $K$ and hence, bringing the time complexity from $O(d ^3)$ to $O(n^3)$.


\section{Finding PCA for Non-Linear Relationships}

\subsection{Transforming Features}
We solve the problem of non-linear relationships by mapping them to higher dimensions. \\
$$
x \to \phi(x) \hspace{2em} \mathbb{R} ^d \to \mathbb{R} ^D \hspace{2em} \text{where } [D >> d]
$$
To compute $D$: \\
Let $x=\left [
\begin{array} {cc}
    f_1 & f_2
\end{array}
\right ]$ be features of a dataset containing datapoints lying on a curve of degree two in a two-dimensional space. \\
To make it linear from quadratic, we map the features to 
$\phi(x)=\left [
\begin{array} {cccccc}
    1 & f_1^2 & f_2^2 & f_1f_2 & f_1 & f_2
\end{array}
\right ]$\\
Mapping $d$ features to the polygonial power $p$ gives $^{d+p} C_d$ new features.\\\textbf{Issue}: Finding $\phi(x)$ may be very hard. \\
Solution for this issue is in the next point.\\
\subsection{Kernel Functions}
A function that maps $k: \mathbb{R}^d \times \mathbb{R}^d \to \mathbb{R}$, and is a "valid", is called a Kernel Function.\\
Proof of a "Valid" Kernel:
\begin{itemize}
    \item Method 1: Exhibit the map to $\phi$ explicitly. [may be hard]
    \item Method 2: Using Mercer's Theorem:\\
    \subitem $k: \mathbb{R}^d \times \mathbb{R}^d \to \mathbb{R}$ is a "valid" kernel if and only if:
    \begin{itemize}
        \item $k$ is symmetric i.e $k(x,x') = k(x',x)$
        \item For any dataset $\{x_1,x_2,\ldots,x_n  \}$, the matrix $K \in \mathbb{R}^{n \times n}$, where $K_{ij} = k(i,j)$, is Positive Semi-Definite.
    \end{itemize}
\end{itemize}
Two Popular Kernel Functions:
\begin{itemize}
    \item Polynomial Kernel: $k(x,x') = (x^Tx + 1)^p$
    \item Radial Basis Function Kernel or Gaussian Kernel: $exp(-\frac{||x-x'||^2}{2\sigma^2})$
\end{itemize}

\newpage
\section{Kernel PCA}
Let's take a dataset $X$ with a large number of features($d$) i.e. $[d >> n]$ where \\ $d$: no. of features \\ $n$: no. of datapoints
$$
X=\left [
\begin{array}{ccccc}
    | & | & | & & | \\
    x_1 & x_2 & x_3 & \ldots & x_4 \\
    | & | & | & & |
\end{array}
\right ]
$$
\begin{itemize}
    \item Step 1: Calculate $K \in \mathbb{R}^{n \times n}$ using a kernel function where $K_{ij}=k(x_i,x_j)$.
    \item Step 2: Center the kernel using the following formula.
    $$
    K^C=K-IK-KI+IKI
    $$
    where $K^C$ is the centered kernel, and $I \in \mathbb{R}^{n \times n}$ where all the elements are $\frac{1}{n}$.
    \item Step 3: Compute the eigenvalues $\{\beta _1, \beta _2, \ldots, \beta _l\}$ and eigenvectors $\{n\lambda _1, n\lambda _2, \ldots, n\lambda _l\}$ of $K^C$ and normalize to get
    $$
    \forall u \hspace{2em} \alpha _u = \frac{\beta _u}{\sqrt{n \lambda _u}}
    $$
    \item Step 4: Compute $\sum _{j=1} ^{n} \alpha _{kj} K^C_{ij} \hspace{1em} \forall k$
    $$
    x_i \in \mathbb{R}^{d} \to 
    \left [
\begin{array}{cccc}
    \sum _{j=1} ^{n} \alpha _{1j} K^C_{ij} & \sum _{j=1} ^{n} \alpha _{2j} K^C_{ij} & \ldots & \sum _{j=1} ^{n} \alpha _{dj} K^C_{ij}
\end{array}
\right ]
    $$
    
\end{itemize}

\section{Credits}
\begin{itemize}
    \item Professor Arun Rajkumar: The content as well as the notations are from his slides and lecture.
\end{itemize}

\end{document}