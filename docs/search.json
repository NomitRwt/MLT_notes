[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MLT Notes",
    "section": "",
    "text": "Note\n\n\n\nThis site is still under development."
  },
  {
    "objectID": "index.html#about-the-course",
    "href": "index.html#about-the-course",
    "title": "MLT Notes",
    "section": "About the Course",
    "text": "About the Course\nTo introduce the main methods and models used in machine learning problems of regression, classification and clustering. To study the properties of these models and methods and learn about their suitability for different problems."
  },
  {
    "objectID": "index.html#what-youll-learn",
    "href": "index.html#what-youll-learn",
    "title": "MLT Notes",
    "section": "What you’ll learn",
    "text": "What you’ll learn\n\nDemonstrating In depth understanding of machine learning algorithms - model, objective or loss function, optimization algorithm and evaluation criteria.\nTweaking machine learning algorithms based on the outcome of experiments - what steps to take in case of underfitting and overfitting.\nBeing able to choose among multiple algorithms for a given task.\nDeveloping an understanding of unsupervised learning techniques."
  },
  {
    "objectID": "index.html#course-instructors",
    "href": "index.html#course-instructors",
    "title": "MLT Notes",
    "section": "Course Instructors",
    "text": "Course Instructors\nArun Rajkumar\nAssistant Professor, Department of Computer Sciences & Engineering, IIT Madras"
  },
  {
    "objectID": "index.html#course-support-team",
    "href": "index.html#course-support-team",
    "title": "MLT Notes",
    "section": "Course Support Team",
    "text": "Course Support Team\n\nKarthik Thiagarajan (B. Tech., IIT Madras)\nNitin Kumar Jha (M.Sc., IIT Madras)"
  },
  {
    "objectID": "index.html#table-of-contents",
    "href": "index.html#table-of-contents",
    "title": "MLT Notes",
    "section": "Table of Contents",
    "text": "Table of Contents\n\nIntroduction; Unsupervised Learning - Representation learning - PCA\nUnsupervised Learning - Representation learning - Kernel PCA\nUnsupervised Learning - Clustering - K-means/Kernel K-means\nUnsupervised Learning - Estimation - Recap of MLE + Bayesian estimation, Gaussian Mixture Model - EM algorithm\nSupervised Learning - Regression - Least Squares; Bayesian view\nSupervised Learning - Regression - Ridge/LASSO\nSupervised Learning - Classification - K-NN, Decision tree\nSupervised Learning - Classification - Generative Models - Naive Bayes\nDiscriminative Models - Perceptron; Logistic Regression\nSupport Vector Machines\nEnsemble methods - Bagging and Boosting (Adaboost)"
  },
  {
    "objectID": "index.html#study-material",
    "href": "index.html#study-material",
    "title": "MLT Notes",
    "section": "Study Material",
    "text": "Study Material\nThe primary study material for this course is the set of videos and assignments posted on the course page. The prescribed textbook for this course is:\n\nPattern Classification by David G. Stork, Peter E. Hart, and Richard O. Duda\nPattern Recognition and Machine Learning by Christopher M. Bishop\nThe Elements of Statistical Learning: Data Mining, Inference, and Prediction by Trevor Hastie, Robert Tibshirani, and Jerome Friedman\n\nThe learners are advised to make best use of the interaction sessions with the course support members to clarify their doubts.\nTo know more about course syllabus, Instructors and course contents, please click on the below link\nhttps://onlinedegree.iitm.ac.in/course_pages/BSCCS2007.html\nPlease click on the below tab to view the Course Specific Calendar:\nhttps://calendar.google.com/calendar/u/2?cid=Y19vODg1amdtMTVrbjJzZW01dGlkM2szcjhnNEBncm91cC5jYWxlbmRhci5nb29nbGUuY29t"
  },
  {
    "objectID": "pages/Wk06.html",
    "href": "pages/Wk06.html",
    "title": "Supervised Learning - Regression - Ridge/LASSO",
    "section": "",
    "text": "PDF Link: notes"
  },
  {
    "objectID": "pages/Wk06.html#introduction",
    "href": "pages/Wk06.html#introduction",
    "title": "Supervised Learning - Regression - Ridge/LASSO",
    "section": "Introduction",
    "text": "Introduction\nLinear regression is a widely used technique for modeling the relationship between a dependent variable and one or more independent variables. The maximum likelihood estimator (MLE) is commonly employed to estimate the parameters of a linear regression model. Here, we discuss the goodness of the MLE for linear regression, explore cross-validation techniques to minimize mean squared error (MSE), examine Bayesian modeling as an alternative approach, and finally, delve into ridge and lasso regression as methods to mitigate overfitting."
  },
  {
    "objectID": "pages/Wk03.html",
    "href": "pages/Wk03.html",
    "title": "Unsupervised Learning - Clustering - K-means/Kernel K-means",
    "section": "",
    "text": "PDF Link: notes"
  },
  {
    "objectID": "pages/Wk03.html#the-algorithm",
    "href": "pages/Wk03.html#the-algorithm",
    "title": "Unsupervised Learning - Clustering - K-means/Kernel K-means",
    "section": "The Algorithm",
    "text": "The Algorithm\nThe algorithm is as follows:\nStep 1: Initialization: Assign random datapoints from the dataset as the cluster centers\nStep 2: Reassignment Step: \\[\nz _i ^{t} = \\underset{k}{\\arg \\min} {|| \\mathbf{x}_i - \\boldsymbol{\\mu} _{k} ^t ||}_2 ^2 \\hspace{2em} \\forall i\n\\]\nStep 3: Compute Means: \\[\n\\boldsymbol{\\mu} _k ^{t+1} = \\frac{\\displaystyle \\sum _{i = 1} ^{n} {\\mathbf{x}_i \\cdot \\mathbb{1}(z_i^t=k)}}{\\displaystyle \\sum _{i = 1} ^{n} {\\mathbb{1}(z_i^t=k)}} \\hspace{2em} \\forall k\n\\]\nStep 4: Loop until Convergence: Repeat steps 2 and 3 until the cluster assignments do not change."
  },
  {
    "objectID": "pages/Wk02.html",
    "href": "pages/Wk02.html",
    "title": "Unsupervised Learning - Representation learning - Kernel PCA",
    "section": "",
    "text": "PDF Link: notes"
  },
  {
    "objectID": "pages/Wk02.html#transforming-features",
    "href": "pages/Wk02.html#transforming-features",
    "title": "Unsupervised Learning - Representation learning - Kernel PCA",
    "section": "Transforming Features",
    "text": "Transforming Features\nWe solve the problem of non-linear relationships by mapping them to higher dimensions. \\[\n\\mathbf{x} \\to \\phi(\\mathbf{x}) \\quad \\mathbb{R} ^d \\to \\mathbb{R} ^D \\quad \\text{where } [D &gt;&gt; d]\n\\] To compute \\(D\\):\nLet \\(\\mathbf{x}=\\left [ \\begin{array} {cc}  f_1 & f_2 \\end{array} \\right ]\\) be features of a dataset containing datapoints lying on a curve of degree two in a two-dimensional space.\nTo make it linear from quadratic, we map the features to \\(\\phi(\\mathbf{x})=\\left [ \\begin{array} {cccccc}  1 & f_1^2 & f_2^2 & f_1f_2 & f_1 & f_2 \\end{array} \\right ]\\)\nMapping \\(d\\) features to the polygonial power \\(p\\) gives \\(^{d+p} C_d\\) new features.\nIssue: Finding \\(\\phi(\\mathbf{x})\\) may be very hard.\nSolution for this issue is in the next point."
  },
  {
    "objectID": "pages/Wk02.html#kernel-functions",
    "href": "pages/Wk02.html#kernel-functions",
    "title": "Unsupervised Learning - Representation learning - Kernel PCA",
    "section": "Kernel Functions",
    "text": "Kernel Functions\nA function that maps \\(k: \\mathbb{R}^d \\times \\mathbb{R}^d \\to \\mathbb{R}\\), and is a “valid”, is called a Kernel Function.\nProof of a “Valid” Kernel:\n\nMethod 1: Exhibit the map to \\(\\phi\\) explicitly. [may be hard]\nMethod 2: Using Mercer’s Theorem:\n\n\\(k: \\mathbb{R}^d \\times \\mathbb{R}^d \\to \\mathbb{R}\\) is a “valid” kernel if and only if:\n\n\\(k\\) is symmetric i.e \\(k(\\mathbf{x},\\mathbf{x}') = k(\\mathbf{x}',\\mathbf{x})\\)\nFor any dataset \\(\\{\\mathbf{x}_1,\\mathbf{x}_2,\\ldots,\\mathbf{x}_n \\}\\), the matrix \\(\\mathbf{K} \\in \\mathbb{R}^{n \\times n}\\), where \\(\\mathbf{K}_{ij} = k(i,j)\\), is Positive Semi-Definite.\n\n\n\nTwo Popular Kernel Functions:\n\nPolynomial Kernel: \\(k(\\mathbf{x},\\mathbf{x}') = (\\mathbf{x}^T\\mathbf{x} + 1)^p\\)\nRadial Basis Function Kernel or Gaussian Kernel: \\(exp(-\\displaystyle \\frac{||\\mathbf{x}-\\mathbf{x}'||^2}{2\\sigma^2})\\)"
  },
  {
    "objectID": "pages/Wk11.html",
    "href": "pages/Wk11.html",
    "title": "Ensemble methods - Bagging and Boosting (Adaboost)",
    "section": "",
    "text": "PDF Link: notes"
  },
  {
    "objectID": "pages/Wk11.html#dual-formulation",
    "href": "pages/Wk11.html#dual-formulation",
    "title": "Ensemble methods - Bagging and Boosting (Adaboost)",
    "section": "Dual Formulation",
    "text": "Dual Formulation\nMaximizing the Lagrangian function w.r.t. \\(\\alpha\\) and \\(\\beta\\), and minimizing it w.r.t. \\(w\\) and \\(\\epsilon\\), we get,\n\\[\n\\min _{w, \\epsilon}\\left [\\max _{\\alpha \\ge 0; \\beta \\ge 0}\\frac{1}{2}||w||^2_2 + C\\sum _{i=1} ^n\\epsilon_i + \\sum _{i=1} ^n \\alpha_i(1-(w^Tx_i)y_i - \\epsilon_i) + \\sum _{i=1} ^n \\beta(-\\epsilon_i) \\right ]\n\\]\nThe dual of this is given by,\n\\[\n\\max _{\\alpha \\ge 0; \\beta \\ge 0}\\left [\\min _{w, \\epsilon}\\frac{1}{2}||w||^2_2 + C\\sum _{i=1} ^n\\epsilon_i + \\sum _{i=1} ^n \\alpha_i(1-(w^Tx_i)y_i - \\epsilon_i) + \\sum _{i=1} ^n \\beta(-\\epsilon_i) \\right ]\n\\]\n\\[\n\\max _{\\alpha \\ge 0; \\beta \\ge 0}\\left [\\min _{w, \\epsilon}\\mathcal{L}(w, \\epsilon, \\alpha, \\beta) \\right ] \\quad \\ldots[1]\n\\]\nDifferentiating the above function\\([1]\\) w.r.t. \\(w\\) while fixing \\(\\alpha\\) and \\(\\beta\\), we get, \\[\n\\frac{d\\mathcal{L}}{dw}  = 0\n\\] \\[\n\\frac{d}{dw} \\frac{1}{2}||w||^2_2 + C\\sum _{i=1} ^n\\epsilon_i + \\sum _{i=1} ^n \\alpha_i(1-(w^Tx_i)y_i - \\epsilon_i) + \\sum _{i=1} ^n \\beta(-\\epsilon_i) = 0\\\\\n\\] \\[\nw_{\\alpha, \\beta}^* - \\alpha_ix_iy_i = 0\n\\] \\[\n\\therefore w_{\\alpha, \\beta}^* = \\alpha_ix_iy_i \\quad \\ldots [2]\n\\]\nDifferentiating the above function\\([1]\\) w.r.t. \\(\\epsilon_i \\forall i\\) while fixing \\(\\alpha\\) and \\(\\beta\\), we get,\n\\[\n\\frac{\\partial\\mathcal{L}}{\\partial\\epsilon_i}  = 0\n\\] \\[\n\\frac{\\partial}{\\partial\\epsilon_i} \\frac{1}{2}||w||^2_2 + C\\sum _{i=1} ^n\\epsilon_i + \\sum _{i=1} ^n \\alpha_i(1-(w^Tx_i)y_i - \\epsilon_i) + \\sum _{i=1} ^n \\beta(-\\epsilon_i) = 0\n\\] \\[\nC - \\alpha_i -\\beta_i = 0\n\\] \\[\n\\therefore C = \\alpha_i + \\beta_i \\quad \\ldots [3]\n\\]\nSubstituting the values of \\(w\\) and \\(\\beta\\) from \\([2]\\) and \\([3]\\) in \\([1]\\), we get,\n\\[\n\\max _{\\alpha \\ge 0; \\beta \\ge 0; C = \\alpha_i + \\beta_i}\\left [\\frac{1}{2}||\\alpha_ix_iy_i||^2_2 + C\\sum _{i=1} ^n\\epsilon_i + \\sum _{i=1} ^n \\alpha_i(1-((\\alpha_ix_iy_i)^Tx_i)y_i - \\epsilon_i) + \\sum _{i=1} ^n (C-\\alpha_i)(-\\epsilon_i) \\right ]\n\\] \\[\n\\max _{\\alpha \\ge 0; \\beta \\ge 0; C = \\alpha_i + \\beta_i}\\left [\\frac{1}{2}\\alpha_i^Tx_i^Ty_i^Ty_ix_i\\alpha_i + C\\sum _{i=1} ^n\\epsilon_i + \\sum _{i=1} ^n \\alpha_i-\\alpha_i^Tx_i^Ty_i^Ty_ix_i\\alpha_i - \\sum _{i=1} ^n \\alpha_i\\epsilon_i - C\\sum _{i=1} ^n\\epsilon_i + \\sum _{i=1} ^n \\alpha_i\\epsilon_i \\right ]\n\\] \\[\n\\max _{\\alpha \\ge 0; \\beta \\ge 0; C = \\alpha_i + \\beta_i}\\left [\\sum _{i=1} ^n \\alpha_i - \\frac{1}{2}\\alpha_i^Tx_i^Ty_i^Ty_ix_i\\alpha_i\\right ]\n\\] \\[\n\\therefore \\max _{0 \\le \\alpha \\le C}\\left [\\sum _{i=1} ^n \\alpha_i - \\frac{1}{2}\\alpha_i^Tx_i^Ty_i^Ty_ix_i\\alpha_i\\right ]\n\\]"
  },
  {
    "objectID": "pages/Wk09.html",
    "href": "pages/Wk09.html",
    "title": "Discriminative Models - Perceptron; Logistic Regression",
    "section": "",
    "text": "PDF Link: notes"
  },
  {
    "objectID": "pages/Wk09.html#analysis-of-the-update-rule",
    "href": "pages/Wk09.html#analysis-of-the-update-rule",
    "title": "Discriminative Models - Perceptron; Logistic Regression",
    "section": "Analysis of the Update Rule",
    "text": "Analysis of the Update Rule\nFor a training example \\((x, y)\\), where \\(x\\) is the input and \\(y\\) is the correct output (either \\(1\\) or \\(-1\\)), the perceptron algorithm updates its weight vector \\(w\\) as follows:\n\nIf the prediction of the perceptron on \\(x\\) is correct (i.e., \\(\\text{sign}(w^Tx_i)==y_i\\)), then no update is performed.\nIf the prediction of the perceptron on \\(x\\) is incorrect (i.e., \\(\\text{sign}(w^Tx_i)\\ne y_i\\)), then the weights are updated by adding the product of the input vector and the correct output to the current weight vector: \\(w^{(t+1)} = w^t + x_iy_i\\).\n\nThis update rule effectively moves the decision boundary in the direction of the correct classification for the misclassified example. It is guaranteed to converge to a linearly separable solution if the data is linearly separable. However, if the data is not linearly separable, the perceptron algorithm may not converge to a solution."
  },
  {
    "objectID": "pages/Wk09.html#further-assumptions",
    "href": "pages/Wk09.html#further-assumptions",
    "title": "Discriminative Models - Perceptron; Logistic Regression",
    "section": "Further Assumptions",
    "text": "Further Assumptions\nWe make three further assumptions:\n\nLinear Separability with \\(\\gamma\\)-Margin: A dataset \\(D=\\{(x_1, y_1), \\ldots, (x_n,y_n)\\}\\) is linearly separable with \\(\\gamma\\)-margin if \\(\\exists w^* \\in \\mathbb{R}^d\\) s.t. \\((w^{*T}x_i)y_i\\ge\\gamma\\) \\(\\forall i\\) for some \\(\\gamma&gt;0\\).\n\n\n\n\nLinear Separability with \\(\\gamma\\)-Margin\n\n\n\nRadius Assumption: Let some \\(R&gt;0 \\in \\mathbb{R}\\), \\(\\forall i \\in D\\) \\(||x_i||\\le R\\). In short, let \\(R\\) be the length of the datapoint furthest from the center.\nNormal Length for \\(w^*\\): Let \\(w^*\\) be of unit length."
  },
  {
    "objectID": "pages/Wk09.html#sigmoid-function",
    "href": "pages/Wk09.html#sigmoid-function",
    "title": "Discriminative Models - Perceptron; Logistic Regression",
    "section": "Sigmoid Function",
    "text": "Sigmoid Function\nUntil now, we used the \\(\\text{sign}\\) function to get the class for the output. But can we also provide the probabilities for these outputs?\nLet \\(z=w^Tx\\) and \\(z \\in \\mathbb{R}\\). How can we map \\([-\\infty, \\infty]\\rightarrow[0,1]\\)? For this, we use the Sigmoid Function. It is given by, \\[\ng(z) = \\frac{1}{1+e^{-z}}\n\\]\n\n\n\nSigmoid Function\n\n\nThe sigmoid function is often used in machine learning as an activation function for neural networks. It has a characteristic S-shaped curve, which makes it useful for modeling processes that have a threshold or saturation point, such as logistic growth or binary classification problems.\nWhen the input value is large and positive, the sigmoid function output approaches 1, and when the input value is large and negative, the sigmoid function output approaches 0. When the input value is 0, the sigmoid function output is exactly 0.5.\nThe term “sigmoid” comes from the Greek word “sigmoides,” which means “shaped like the letter sigma” (\\(\\Sigma\\)). The letter sigma has a similar shape to the sigmoid function’s characteristic S-shaped curve, which is likely the reason for the function’s name."
  },
  {
    "objectID": "pages/Wk09.html#logistic-regression-1",
    "href": "pages/Wk09.html#logistic-regression-1",
    "title": "Discriminative Models - Perceptron; Logistic Regression",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nLogistic regression is a statistical method used to analyze and model the relationship between a binary (two-valued) dependent variable and one or more independent variables, which can be either continuous or categorical. The goal of logistic regression is to estimate the probability that the dependent variable is one of the two possible values, given the values of the independent variables.\nIn logistic regression, the dependent variable is modeled as a function of the independent variables using a logistic(sigmoid) function, which produces an S-shaped curve that ranges between 0 and 1. The logistic function transforms the output of a linear combination of the independent variables into a probability estimate, which can then be used to classify new observations.\nLet \\(D=\\{(x_1, y_1), \\ldots, (x_n,y_n)\\}\\) be the dataset, where \\(x_i \\in \\mathbb{R}^d\\) and \\(y_i \\in \\{0, 1\\}\\).\nWe know, \\[\nP(y=1|x) = g(w^Tx_i) = \\frac{1}{1+e^{-w^Tx}}\n\\] Using Maximum Likelihood, we get \\[\\begin{align*}\n\\mathcal{L}(w;\\text{Data}) &= \\prod _{i=1} ^{n} (g(w^Tx_i))^{y_i}(1- g(w^Tx_i))^{1-y_i} \\\\\n\\log(\\mathcal{L}(w;\\text{Data})) &= \\sum _{i=1} ^{n} y_i\\log(g(w^Tx_i))+(1-y_i)\\log(1- g(w^Tx_i)) \\\\\n&= \\sum _{i=1} ^{n} y_i\\log\\left(\\frac{1}{1+e^{-w^Tx_i}}\\right)+(1-y_i)\\log\\left(\\frac{e^{-w^Tx_i}}{1+e^{-w^Tx_i}}\\right) \\\\\n&= \\sum _{i=1} ^{n} \\left [ (1-y_i)(-w^Tx_i) - \\log(1+e^{-w^Tx_i}) \\right ]\n\\end{align*}\\] Therefore, our objective, which is maximizing the log-likelihood function, is given by, \\[\n\\max _{w}\\sum _{i=1} ^{n} \\left [ (1-y_i)(-w^Tx_i) - \\log(1+e^{-w^Tx_i}) \\right ]\n\\] But, there is no closed form solution for this. And hence, we use gradient descent for convergence.\nThe gradient is given by, \\[\\begin{align*}\n\\nabla \\log(\\mathcal{L}(w;\\text{Data})) &= \\sum _{i=1} ^{n} \\left [ (1-y_i)(-x_i) - \\left( \\frac{e^{-w^Tx_i}}{1+e^{-w^Tx_i}} \\right ) (-x_i) \\right ] \\\\\n&= \\sum _{i=1} ^{n} \\left [ -x_i + x_iy_i + x_i \\left( \\frac{e^{-w^Tx_i}}{1+e^{-w^Tx_i}} \\right ) \\right ] \\\\\n&= \\sum _{i=1} ^{n} \\left [ x_iy_i - x_i \\left( \\frac{1}{1+e^{-w^Tx_i}} \\right ) \\right ] \\\\\n\\nabla \\log(\\mathcal{L}(w;\\text{Data})) &= \\sum _{i=1} ^{n} \\left [ x_i \\left(y_i - \\frac{1}{1+e^{-w^Tx_i}} \\right ) \\right ]\n\\end{align*}\\] Using the Gradient Descent update rule, we get, \\[\\begin{align*}\nw_{t+1} &= w_t + \\eta_t\\nabla \\log(\\mathcal{L}(w;\\text{Data})) \\\\\n&= w_t + \\eta_t  \\left ( \\sum _{i=1} ^{n} x_i \\left(y_i - \\frac{1}{1+e^{-w^Tx_i}} \\right ) \\right )\n\\end{align*}\\]\n\nKernel and Regularized Versions\nWe can argue that \\(w^*=\\displaystyle\\sum _{i=1} ^{n}\\alpha_ix_i\\), and therefore, can be Kernelized. For further details, refer to this link.\nThe regularized version is given by, \\[\n\\min _{w}\\sum _{i=1} ^{n} \\left [ \\log(1+e^{-w^Tx_i}) + w^Tx_i(1-y_i) \\right ] + \\frac{\\lambda}{2}||w||^2\n\\] where \\(\\frac{\\lambda}{2}||w||^2\\) is the regualizer and \\(\\lambda\\) is found using cross-validation."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "pages/Wk08.html",
    "href": "pages/Wk08.html",
    "title": "Supervised Learning - Classification - Generative Models - Naive Bayes",
    "section": "",
    "text": "PDF Link: notes"
  },
  {
    "objectID": "pages/Wk08.html#alternate-generative-model",
    "href": "pages/Wk08.html#alternate-generative-model",
    "title": "Supervised Learning - Classification - Generative Models - Naive Bayes",
    "section": "Alternate Generative Model",
    "text": "Alternate Generative Model\nAn alternate model begins with the class conditional independence assumption. The class conditional independence assumption is a common assumption made in machine learning algorithms that assumes the features of an object are conditionally independent given its class label.\nLet \\(D=\\{(x_1, y_1), \\ldots, (x_n,y_n)\\}\\) be the dataset, where \\(x_i \\in \\{0, 1\\}^d\\) and \\(y_i \\in \\{0, 1\\}\\).\nGeneral Steps in the algorithm are:\n\nDecide the labels by tossing a coin \\(P(y_i=1)=p\\).\nDecide features for \\(x\\) given \\(y\\) using: \\[\nP(x = [f_1, f_2, \\ldots, f_d]|y) = \\prod_{i=1}^d(p^y_i)^{f_i}(1-p^y_i)^{1-f_i}\n\\]\n\nThe parameters in the model are as follows:\n\nParameter \\(\\hat{p}\\) to decide the label: 1\nParameters for \\(P(x|y=1)\\): \\(d\\)\nParameters for \\(P(x|y=0)\\): \\(d\\)\n\nHence, the total number of parameters \\[\\begin{align*}\n    &=1 + d + d\\\\\n    &=2d+1\n\\end{align*}\\]\nThe parameters are estimated using Maximum Likelihood Estimation."
  },
  {
    "objectID": "pages/Wk08.html#prediction-using-the-parameters",
    "href": "pages/Wk08.html#prediction-using-the-parameters",
    "title": "Supervised Learning - Classification - Generative Models - Naive Bayes",
    "section": "Prediction using the parameters",
    "text": "Prediction using the parameters\nGiven \\(x^{test}\\in\\{0,1\\}^d\\), prediction for \\(\\hat{y}^{test}\\) is done using the following: \\[\nP(\\hat{y}^{test}=1|x^{test}) \\ge P(\\hat{y}^{test}=0|x^{test})\n\\] If the above is true, \\(\\hat{y}^{test}=1\\), otherwise \\(0\\).\nUsing Bayes rule, we can get the values for \\(P(\\hat{y}^{test}=1|x^{test})\\) and \\(P(\\hat{y}^{test}=0|x^{test})\\): \\[\\begin{align*}\nP(\\hat{y}^{test}=1|x^{test})&=\\frac{P(x^{test}|\\hat{y}^{test}=1)*P(\\hat{y}^{test}=1)}{P(x^{test})}\\\\\nP(\\hat{y}^{test}=0|x^{test})&=\\frac{P(x^{test}|\\hat{y}^{test}=0)*P(\\hat{y}^{test}=0)}{P(x^{test})}\n\\end{align*}\\] As we predict by comparing the two values, we can do this without actually solving for \\(P(x^{test})\\).\nSolving for \\(P(x^{test}|\\hat{y}^{test}=1)*P(\\hat{y}^{test}=1)\\), we get, \\[\\begin{align*}\n&=P(x^{test} = [f_1, f_2, \\ldots, f_d]|y^{test}=1)*P(\\hat{y}^{test}=1)\\\\\n&=\\left(\\prod_{i=1}^d(\\hat{p}^1_i)^{f_i}(1-\\hat{p}^1_i)^{1-f_i}\\right)*\\hat{p}\n\\end{align*}\\] Similarly we solve for \\(P(x^{test}|\\hat{y}^{test}=0)*P(\\hat{y}^{test}=0)\\).\nTherefore, if \\[\n\\left(\\prod_{i=1}^d(\\hat{p}^1_i)^{f_i}(1-\\hat{p}^1_i)^{1-f_i}\\right)*\\hat{p} \\ge \\left(\\prod_{i=1}^d(\\hat{p}^0_i)^{f_i}(1-\\hat{p}^0_i)^{1-f_i}\\right)*(1-\\hat{p})\n\\] we predict \\(y^{test}=1\\), othewise \\(y^{test}=0\\).\nThe model implements two main things:\n\nClass Conditional Independence Assumption\nBayes Rule\n\nTherefore, we call this algorithm Naive Bayes.\nIn short, Naive Bayes is a classification algorithm based on Bayes’ theorem, which assumes that the features are independent of each other given the class label. It calculates the probability of a sample belonging to a class by estimating the conditional probability of each feature given the class and then multiplying them together using Bayes’ theorem. Despite its simple assumption, Naive Bayes is known to perform well in various applications, particularly when there are many features but relatively few training examples."
  },
  {
    "objectID": "pages/Wk08.html#pitfalls-of-naive-bayes",
    "href": "pages/Wk08.html#pitfalls-of-naive-bayes",
    "title": "Supervised Learning - Classification - Generative Models - Naive Bayes",
    "section": "Pitfalls of Naive Bayes",
    "text": "Pitfalls of Naive Bayes\nThe most prominent issue with Naive Bayes is that if a feature is not seen in the training set but seen in the testing set, the prediction probability for both the classes would be zero. \\[\\begin{align*}\nP(\\hat{y}^{test}=1|x^{test} = [f_1, f_2, \\ldots, f_d])&\\propto\\left(\\prod_{i=1}^d(\\hat{p}^1_i)^{f_i}(1-\\hat{p}^1_i)^{1-f_i}\\right)*\\hat{p}\\\\\nP(\\hat{y}^{test}=0|x^{test} = [f_1, f_2, \\ldots, f_d])&\\propto\\left(\\prod_{i=1}^d(\\hat{p}^0_i)^{f_i}(1-\\hat{p}^0_i)^{1-f_i}\\right)*(1-\\hat{p})\n\\end{align*}\\] Even if one feature \\(f_i\\) was zero in training set, we get \\(\\hat{p}^1_i=\\hat{p}^0_i=0\\), which ultimately results in \\(P(\\hat{y}^{test}=0|x^{test})=P(\\hat{y}^{test}=1|x^{test})=0\\).\nA popular fix for this is to introduce two “pseudo” datapoints with labels \\(1\\) and \\(0\\) each into the dataset whose features are all ones. This technique is also known as Laplace smoothing.\nBriefly speaking, Laplace smoothing is a technique used to address the zero-frequency problem in probabilistic models, particularly in text classification. It involves adding a small constant value to the count of each feature and the number of unique classes to avoid zero probability estimates, which can cause problems during model training and prediction. By adding this smoothing term, the model becomes more robust and can handle unseen data more effectively."
  },
  {
    "objectID": "pages/Wk08.html#prediction-using-bayes-rule",
    "href": "pages/Wk08.html#prediction-using-bayes-rule",
    "title": "Supervised Learning - Classification - Generative Models - Naive Bayes",
    "section": "Prediction using Bayes Rule",
    "text": "Prediction using Bayes Rule\nPrediction is based on the following equation: \\[\nP(y_{test}=1|x_{test})\\propto P(x_{test}|y_{test})*P(y_{test})\n\\] where \\(P(x_{test}|y_{test})\\equiv f(x_{test};\\hat{\\mu}_{y_{test}}, \\hat{\\Sigma})\\) and \\(P(y_{test})\\equiv \\hat{p}\\).\nPredict \\(y_{test}=1\\) if: \\[\\begin{align*}\nf(x_{test};\\hat{\\mu}_1, \\hat{\\Sigma})*\\hat{p}&\\ge f(x_{test};\\hat{\\mu}_0, \\hat{\\Sigma})*(1-\\hat{p}) \\\\\ne^{-(x_{test}-\\hat{\\mu}_1)^T\\hat{\\Sigma}(x_{test}-\\hat{\\mu}_1)}*\\hat{p}&\\ge e^{-(x_{test}-\\hat{\\mu}_0)^T\\hat{\\Sigma}(x_{test}-\\hat{\\mu}_0)}*(1-\\hat{p}) \\\\\n-(x_{test}-\\hat{\\mu}_1)^T\\hat{\\Sigma}(x_{test}-\\hat{\\mu}_1)+\\log(\\hat{p})&\\ge -(x_{test}-\\hat{\\mu}_0)^T\\hat{\\Sigma}(x_{test}-\\hat{\\mu}_0) + \\log(1-\\hat{p}) \\\\\n\\end{align*}\\] \\[\n\\left( (\\hat{\\mu}_1-\\hat{\\mu}_0)^T\\hat{\\Sigma}^{-1} \\right)x_{test} + \\hat{\\mu}_0^T\\hat{\\Sigma}^{-1}\\hat{\\mu}_0 - \\hat{\\mu}_1^T\\hat{\\Sigma}^{-1}\\hat{\\mu}_1 + log(\\frac{1-\\hat{p}}{\\hat{p}}) \\ge 0\n\\] Hence, we can say that the decision function is of the form \\(w^Tx+b\\ge0\\) where \\(w\\in\\mathbb{R}^d\\), \\(w_i= (\\hat{\\mu}_1-\\hat{\\mu}_0)^T\\hat{\\Sigma}^{-1}\\) and \\(b=\\hat{\\mu}_0^T\\hat{\\Sigma}^{-1}\\hat{\\mu}_0 - \\hat{\\mu}_1^T\\hat{\\Sigma}^{-1}\\hat{\\mu}_1 + log(\\frac{1-\\hat{p}}{\\hat{p}})\\).\nTherefore, the decision function of Gaussian Naive Bayes when the covariance matrix is equal for both classes is Linear."
  },
  {
    "objectID": "pages/Wk08.html#decision-boundaries-for-different-covariances",
    "href": "pages/Wk08.html#decision-boundaries-for-different-covariances",
    "title": "Supervised Learning - Classification - Generative Models - Naive Bayes",
    "section": "Decision Boundaries for Different Covariances",
    "text": "Decision Boundaries for Different Covariances\n\nWhen the covariance matrices are equal for both classes: As seen previously, the decision boundary is linear.\n\n\n\n\nWhen the covariance matrices are equal for both classes\n\n\n\nWhen the covariance matrices are Identity matrices for both classes: The decision boundary is linear as well as the perpendicular bisector of the line drawn from \\(\\hat{\\mu}_1\\) to \\(\\hat{\\mu}_0\\).\n\n\n\n\nWhen the covariance matrices are Identity matrices for both classes\n\n\n\nWhen the covariance matrices are not equal for both classes: Let \\(\\hat{\\Sigma}_1\\) and \\(\\hat{\\Sigma}_0\\) be the covariance matrices for classes \\(1\\) and \\(0\\) respectively. They are given by, \\[\\begin{align*}\n\\hat{\\Sigma}_1 &= \\frac{1}{n} \\displaystyle \\sum_{i=1}^n(\\mathbb{1}(y_i=1)*x_i-\\hat{\\mu}_1)(\\mathbb{1}(y_i=1)*x_i-\\hat{\\mu}_1)^T \\\\\n\\hat{\\Sigma}_0 &= \\frac{1}{n} \\displaystyle \\sum_{i=1}^n(\\mathbb{1}(y_i=0)*x_i-\\hat{\\mu}_0)(\\mathbb{1}(y_i=0)*x_i-\\hat{\\mu}_0)^T\n\\end{align*}\\] Predict \\(y_{test}=1\\) if: \\[\\begin{align*}\nf(x_{test};\\hat{\\mu}_1, \\hat{\\Sigma_1})*\\hat{p}&\\ge f(x_{test};\\hat{\\mu}_0, \\hat{\\Sigma_0})*(1-\\hat{p}) \\\\\ne^{-(x_{test}-\\hat{\\mu}_1)^T\\hat{\\Sigma_1}(x_{test}-\\hat{\\mu}_1)}*\\hat{p}&\\ge e^{-(x_{test}-\\hat{\\mu}_0)^T\\hat{\\Sigma_1}(x_{test}-\\hat{\\mu}_0)}*(1-\\hat{p}) \\\\\n-(x_{test}-\\hat{\\mu}_1)^T\\hat{\\Sigma_1}(x_{test}-\\hat{\\mu}_1)+\\log(\\hat{p})&\\ge -(x_{test}-\\hat{\\mu}_0)^T\\hat{\\Sigma_0}(x_{test}-\\hat{\\mu}_0) + \\log(1-\\hat{p}) \\\\\n\\end{align*}\\] \\[\nx_{test}^T(\\hat{\\Sigma}_1^{-1}-\\hat{\\Sigma}_0^{-1})x_{test}-2(\\hat{\\mu}_1^T\\hat{\\Sigma}_1^{-1}-\\hat{\\mu}_0^T\\hat{\\Sigma}_0^{-1})x_{test}+(\\hat{\\mu}_0^T\\hat{\\Sigma}_0^{-1}\\hat{\\mu}_0-\\hat{\\mu}_1^T\\hat{\\Sigma}_1^{-1}\\hat{\\mu}_1) + log(\\frac{1-\\hat{p}}{\\hat{p}}) \\ge 0\n\\] Hence, we can say that the decision function is of the form \\(x^TQx-2b^Tx+c\\ge0\\) where \\(Q=\\hat{\\Sigma}_1^{-1}-\\hat{\\Sigma}_0^{-1}\\), \\(b=\\hat{\\mu}_1^T\\hat{\\Sigma}_1^{-1}-\\hat{\\mu}_0^T\\hat{\\Sigma}_0^{-1}\\), and \\(c=(\\hat{\\mu}_0^T\\hat{\\Sigma}_0^{-1}\\hat{\\mu}_0-\\hat{\\mu}_1^T\\hat{\\Sigma}_1^{-1}\\hat{\\mu}_1) + log(\\frac{1-\\hat{p}}{\\hat{p}})\\). Hence, the decision boundary is a quadratic function when the covariance matrices are not equal for both classes.\n\n\n\n\nWhen the covariance matrices are not equal for both classes"
  },
  {
    "objectID": "pages/Wk10.html",
    "href": "pages/Wk10.html",
    "title": "Support Vector Machines",
    "section": "",
    "text": "PDF Link: notes"
  },
  {
    "objectID": "pages/Wk10.html#margin-maximization",
    "href": "pages/Wk10.html#margin-maximization",
    "title": "Support Vector Machines",
    "section": "Margin Maximization",
    "text": "Margin Maximization\nFrom the previous analysis, it is clear that a single dataset could have multiple linear classifiers with varying margins. The following diagram illustrates this phenomenon,\n\n\n\nMultiple Classifiers\n\n\nTherefore, for getting the best classifier, our goal can be written as, \\[\n\\max_{w,\\gamma} \\gamma\n\\] \\[\\begin{align*}\ns.t. (w^Tx_i)y_i &\\ge \\gamma \\hspace{1em} \\forall i \\\\\n||w||^2 &= 1\\\n\\end{align*}\\]\\end{align*} The boundary of the margin is given by, \\[\\begin{align*}\n\\{x:(w^Tx_i)y_i &= \\gamma\\}\\\\\n\\{x:(\\frac{w}{\\gamma}^Tx_i)y_i &= 1\\}\\\\\n\\end{align*}\\] From the above equation, we can see that \\(\\gamma\\) depends on the width of \\(w\\). Therefore, we reformulate our goal as, \\[\n\\max_{w} \\text{width}(w)\n\\] \\[\\begin{align*}\ns.t. (w^Tx_i)y_i &\\ge 1 \\hspace{1em} \\forall i \\\\\n\\end{align*}\\] Let the width be the distance between the two parallel margins, and let \\(x\\) and \\(z\\) be two points who are on the two lines exactly opposite to each other s.t. \\(w^Tx=-1\\) and \\(w^Tz=1\\) or vice versa.\nLet \\(x_1\\) and \\(x_2\\) be two points which lie on opposite side of the decision boundary as well as on the margins.\n\n\n\nMargin Width\n\n\nTherefore, the width is given by, \\[\\begin{align*}\n||x_1^Tw - x_2^Tw||_2^2 &= 2 \\\\\n||x_1-x_2||_2^2||w||^2_2 &= 2\\\\\n\\therefore ||x_1 - x_2||^2_2 &= \\frac{2}{||w||^2_2}\n\\end{align*}\\]\nTherefore, our objective function can be written as, \\[\n\\max_{w}  \\frac{2}{||w||^2_2} \\hspace{1em} s.t. (w^Tx_i)y_i \\ge 1 \\hspace{1em} \\forall i\n\\] Equivalently, \\[\n\\min_{w}  \\frac{1}{2}||w||^2_2 \\hspace{1em} s.t. (w^Tx_i)y_i \\ge 1 \\hspace{1em} \\forall i\n\\] Therefore ﬁnding the separating hyperplane with maximum margin is equivalent to ﬁnding the one with the smallest possible normal vector \\(w\\)."
  },
  {
    "objectID": "pages/Wk10.html#hard-margin-svm-algorithm",
    "href": "pages/Wk10.html#hard-margin-svm-algorithm",
    "title": "Support Vector Machines",
    "section": "Hard-Margin SVM Algorithm",
    "text": "Hard-Margin SVM Algorithm\nThis algorithm only works if the dataset is linearly separable with a \\(\\gamma &gt; 0\\).\n\nCalculate \\(Q=X^TX\\) directly or using a kernel as per the dataset.\nUse the gradient of the dual formula (\\(\\alpha^T1 - \\frac{1}{2}\\alpha^TY^TQY\\alpha\\)), in the gradient descent algorithm to find a satisfactory \\(\\alpha\\). Let the intial \\(\\alpha\\) be a zero vector \\(\\in \\mathbb{R}^n_+\\).\nTo predict:\n\nFor non-kernelized SVM: \\(\\text{label}(x_{test}) = w^Tx_{test} = \\sum _{i=1} ^n \\alpha _i y_i(x_i^Tx_{test})\\)\nFor kernelized SVM: \\(\\text{label}(x_{test}) = w^T\\phi(x_{test}) = \\sum _{i=1} ^n \\alpha _i y_ik(x_i^Tx_{test})\\)"
  },
  {
    "objectID": "pages/Wk01.html",
    "href": "pages/Wk01.html",
    "title": "Introduction; Unsupervised Learning - Representation learning - PCA",
    "section": "",
    "text": "PDF Link: notes"
  },
  {
    "objectID": "pages/Wk01.html#broad-paradigms-of-machine-learning",
    "href": "pages/Wk01.html#broad-paradigms-of-machine-learning",
    "title": "Introduction; Unsupervised Learning - Representation learning - PCA",
    "section": "Broad Paradigms of Machine Learning",
    "text": "Broad Paradigms of Machine Learning\n\nSupervised Learning:Supervised Machine Learning is a type of machine learning where the algorithm is trained on a labeled dataset, meaning that the data includes both inputs and their corresponding outputs. The goal of supervised learning is to build a model that can accurately predict the output for new, unseen input data. Few examples:\n\n\nLinear regression for predicting a continuous output\nLogistic regression for binary classification problems\nDecision trees for non-linear classification and regression problems\nSupport Vector Machines for binary and multi-class classification problems\nNeural Networks for complex non-linear problems in various domains such as computer vision, natural language processing, and speech recognition\n\n\nUnsupervised Learning: Unsupervised Machine Learning is a type of machine learning where the algorithm is trained on an unlabeled dataset, meaning that only the inputs are provided and no corresponding outputs. The goal of unsupervised learning is to uncover patterns or relationships within the data without any prior knowledge or guidance. Few examples:\n\n\nClustering algorithms such as K-means, hierarchical clustering, and density-based clustering, used to group similar data points together into clusters\nDimensionality reduction techniques such as Principal Component Analysis (PCA), used to reduce the number of features in a dataset while preserving the maximum amount of information\nAnomaly detection algorithms used to identify unusual data points that deviate from the normal patterns in the data\n\n\nSequential learning: Sequential Machine Learning (also known as time-series prediction) is a type of machine learning that is focused on making predictions based on sequences of data. It involves training the model on a sequence of inputs, such that the predictions for each time step depend on the previous time steps. Few examples:\n\n\nTime series forecasting, used to predict future values based on past trends and patterns in data such as stock prices, weather patterns, and energy consumption\nSpeech recognition, used to transcribe speech into text by recognizing patterns in audio signals\nNatural language processing, used to analyze and make predictions about sequences of text data"
  },
  {
    "objectID": "pages/Wk01.html#potential-algorithm",
    "href": "pages/Wk01.html#potential-algorithm",
    "title": "Introduction; Unsupervised Learning - Representation learning - PCA",
    "section": "Potential Algorithm",
    "text": "Potential Algorithm\nWith this information, we can give the following algorithm:\\ Given a dataset \\(\\{\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n\\}\\) where \\(\\mathbf{x}_i \\in \\mathbb{R}^{d}\\),\n\nCenter the dataset \\[\n\\mathbf{\\mu} = \\frac{1}{n} \\sum _{i=1} ^{n} \\mathbf{x}_i\n\\] \\[\n\\mathbf{x}_i = \\mathbf{x}_i - \\mathbf{\\mu}  \\hspace{2em} \\forall i\n\\]\nFind the best representation \\(\\mathbf{w} \\in \\mathbb{R}^d\\) and \\(||\\mathbf{w}|| = 1\\).\nReplace \\(\\mathbf{x}_i = \\mathbf{x}_i - (\\mathbf{x}_i^T\\mathbf{w})\\mathbf{w} \\hspace{1em} \\forall i\\)\nRepeat the first three steps until residues become zero and we obtain \\(\\mathbf{w}_2, \\mathbf{w}_3, \\ldots, \\mathbf{w}_d\\).\n\nBut is this the best way? How many \\(\\mathbf{w}\\) do we need for optimal compression?"
  },
  {
    "objectID": "pages/Wk01.html#approximate-representation",
    "href": "pages/Wk01.html#approximate-representation",
    "title": "Introduction; Unsupervised Learning - Representation learning - PCA",
    "section": "Approximate Representation",
    "text": "Approximate Representation\nIf the data can be approximately represented by a lower sub-space, is it enough that we use only those \\(k\\) projections? How much variance should be covered?\nGiven a centered dataset \\(\\{\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n\\}\\) where \\(\\mathbf{x}_i \\in \\mathbb{R}^{d}\\), let \\(\\mathbf{C}\\) be its covariance matrix, and let \\(\\{\\lambda_1, \\lambda_2, \\ldots, \\lambda_d \\}\\) be its eigenvalues, which are non-negative because the covariance matrix is a positive semi-definite matrix, placed in descending order, and let \\(\\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_d \\}\\) be its corresponding eigenvectors of unit length.\nThe eigen equation for the covariance matrix can be given by, \\[\\begin{align*}\n    \\mathbf{C}\\mathbf{w} &= \\lambda \\mathbf{w} \\\\\n    \\mathbf{w}^T\\mathbf{C}\\mathbf{w} &= \\mathbf{w}^T\\lambda \\mathbf{w}\\\\\n    \\therefore \\lambda &= \\mathbf{w}^T\\mathbf{C}\\mathbf{w} \\hspace{2em} \\{\\mathbf{w}^T\\mathbf{w} = 1\\} \\\\\n    \\lambda &= \\frac{1}{n} \\sum _{i=1} ^{n} (\\mathbf{x}_i^T\\mathbf{w})^2 \\\\\n\\end{align*}\\] Therefore, as the mean is zero, \\(\\lambda\\) gives the variance captured by the eigenvector \\(\\mathbf{w}\\).\nA good rule of thumb is that the variance captured by P.C.A. should be at least 95%. If the first \\(k\\) eigenvectors capture the required variance, this is given by, \\[\n\\frac{\\displaystyle \\sum _{j=1} ^{k} \\lambda_j}{\\displaystyle \\sum _{i=1} ^{d} \\lambda_i} \\ge 0.95\n\\] Hence, the higher the variance captured, the lower is the error obtained."
  },
  {
    "objectID": "pages/Wk01.html#p.c.a.-algorithm",
    "href": "pages/Wk01.html#p.c.a.-algorithm",
    "title": "Introduction; Unsupervised Learning - Representation learning - PCA",
    "section": "P.C.A. Algorithm",
    "text": "P.C.A. Algorithm\nGiven a centered dataset \\(\\{\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n\\}\\) where \\(\\mathbf{x}_i \\in \\mathbb{R}^{d}\\), let \\(\\mathbf{C}\\) be its covariance matrix.\\ The algorithm is as follows:\n\nStep 1: Find the eigenvalues and eigenvectors of \\(\\mathbf{C}\\). Let \\(\\{\\lambda_1, \\lambda_2, \\ldots, \\lambda_d \\}\\) be its eigenvalues placed in the descending order, and let \\(\\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_d \\}\\) be its corresponding eigenvectors of unit length.\nStep 2: Calculate \\(k\\), where \\(k\\) is the number of required top eigenvalues and eigenvectors, according to the required variance that needs to be covered.\nStep 3: Project the data onto the eigenvectors, and obtain the required representation as a linear combination of those projections.\n\n\n\n\nThe dataset depicted in the diagram has two principal components: the green vector represents the first PC, whereas the red vector corresponds to the second PC.\n\n\nIn short, we can say that P.C.A. is a dimensionality reduction technique that finds combination of features that are de-correlated (independent of each other)."
  },
  {
    "objectID": "pages/Wk04.html",
    "href": "pages/Wk04.html",
    "title": "Unsupervised Learning - Estimation - Recap of MLE + Bayesian estimation, Gaussian Mixture Model - EM algorithm",
    "section": "",
    "text": "PDF Link: notes"
  },
  {
    "objectID": "pages/Wk04.html#fishers-principle-of-maximum-likelihood",
    "href": "pages/Wk04.html#fishers-principle-of-maximum-likelihood",
    "title": "Unsupervised Learning - Estimation - Recap of MLE + Bayesian estimation, Gaussian Mixture Model - EM algorithm",
    "section": "Fisher’s Principle of Maximum Likelihood",
    "text": "Fisher’s Principle of Maximum Likelihood\nFisher’s principle of maximum likelihood is a statistical method used to estimate the parameters of a statistical model by choosing the parameter values that maximize the likelihood function, which measures how well the model fits the observed data."
  },
  {
    "objectID": "pages/Wk04.html#likelihood-estimation-on-bernoulli-distributions",
    "href": "pages/Wk04.html#likelihood-estimation-on-bernoulli-distributions",
    "title": "Unsupervised Learning - Estimation - Recap of MLE + Bayesian estimation, Gaussian Mixture Model - EM algorithm",
    "section": "Likelihood Estimation on Bernoulli Distributions",
    "text": "Likelihood Estimation on Bernoulli Distributions\nApplying the likelihood function on the above dataset, we get \\[\\begin{align*}\n\\mathcal{L}(p;\\{\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n\\}) &= P(\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n;p)\\\\\n&=p(\\mathbf{x}_1;p)p(\\mathbf{x}_2;p)\\ldots p(\\mathbf{x}_n;p) \\\\\n&=\\prod _{i=1} ^n {p^{\\mathbf{x}_i}(1-p)^{1-\\mathbf{x}_i}}\n\\end{align*}\\] \\[\\begin{align*}\n\\therefore \\log(\\mathcal{L}(p;\\{\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n\\})) &=\\underset{p} {\\arg \\max}\\log \\left ( \\prod _{i=1} ^n {p^{\\mathbf{x}_i}(1-p)^{1-\\mathbf{x}_i}} \\right ) \\\\\n\\text{Differentiating wrt $p$, we get}\\\\\n\\therefore \\hat{p}_{\\text{ML}} &= \\frac{1}{n}\\sum _{i=1} ^n \\mathbf{x}_i\n\\end{align*}\\]"
  },
  {
    "objectID": "pages/Wk04.html#likelihood-estimation-on-gaussian-distributions",
    "href": "pages/Wk04.html#likelihood-estimation-on-gaussian-distributions",
    "title": "Unsupervised Learning - Estimation - Recap of MLE + Bayesian estimation, Gaussian Mixture Model - EM algorithm",
    "section": "Likelihood Estimation on Gaussian Distributions",
    "text": "Likelihood Estimation on Gaussian Distributions\nLet \\(\\{\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n\\}\\) be a dataset where \\(\\mathbf{x}_i \\sim \\mathcal{N}(\\boldsymbol{\\mu},\\boldsymbol{\\sigma}^2)\\). We assume that the datapoints are independent and identically distributed.\n\\[\\begin{align*}\n\\mathcal{L}(\\boldsymbol{\\mu}, \\boldsymbol{\\sigma}^2;\\{\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n\\}) &= f_{\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n}(\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n;\\boldsymbol{\\mu}, \\boldsymbol{\\sigma}^2) \\\\\n&=\\prod _{i=1} ^n  f_{\\mathbf{x}_i}(\\mathbf{x}_i;\\boldsymbol{\\mu}, \\boldsymbol{\\sigma}^2) \\\\\n&=\\prod _{i=1} ^n \\left [ \\frac{1}{\\sqrt{2\\pi}\\boldsymbol{\\sigma}} e^{\\frac{-(\\mathbf{x}_i-\\boldsymbol{\\mu})^2}{2\\boldsymbol{\\sigma}^2}} \\right ] \\\\\n\\therefore \\log(\\mathcal{L}(p;\\{\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n\\})) &= \\sum _{i=1} ^n \\left[ \\log \\left (\\frac{1}{\\sqrt{2\\pi}\\boldsymbol{\\sigma}}  \\right ) - \\frac{(\\mathbf{x}_i-\\boldsymbol{\\mu})^2}{2\\boldsymbol{\\sigma}^2} \\right] \\\\\n\\end{align*}\\] \\[\n\\text{Differentiating wrt $\\boldsymbol{\\mu}$ and $\\boldsymbol{\\sigma}$, we get}\n\\] \\[\\begin{align*}\n\\hat{\\boldsymbol{\\mu}}_{\\text{ML}} &= \\frac{1}{n}\\sum _{i=1} ^n \\mathbf{x}_i \\\\\n\\hat{\\boldsymbol{\\sigma}^2}_{\\text{ML}} &= \\frac{1}{n}\\sum _{i=1} ^n (\\mathbf{x}_i-\\boldsymbol{\\mu})^T(\\mathbf{x}_i-\\boldsymbol{\\mu})\n\\end{align*}\\]"
  },
  {
    "objectID": "pages/Wk04.html#bayesian-estimation-for-a-bernoulli-distribution",
    "href": "pages/Wk04.html#bayesian-estimation-for-a-bernoulli-distribution",
    "title": "Unsupervised Learning - Estimation - Recap of MLE + Bayesian estimation, Gaussian Mixture Model - EM algorithm",
    "section": "Bayesian Estimation for a Bernoulli Distribution",
    "text": "Bayesian Estimation for a Bernoulli Distribution\nLet \\(\\{x_1, x_2, \\ldots, x_n\\}\\) be a dataset where \\(x_i \\in \\{0,1\\}\\) with parameter \\(\\theta\\). What distribution can be suited for \\(P(\\theta)\\)?\nA commonly used distribution for priors is the Beta Distribution. \\[\nf(p;\\alpha,\\beta) = \\frac{p^{\\alpha-1}(1-p)^{\\beta-1}}{z} \\hspace{2em} \\forall p \\in [0,1] \\\\\n\\] \\[\n\\text{where $z$ is a normalizing factor}\n\\]\nHence, using the Beta Distribution as the Prior, we get, \\[\\begin{align*}\nP(\\theta|\\{x_1, x_2, \\ldots, x_n\\}) &\\propto P(\\theta|\\{x_1, x_2, \\ldots, x_n\\})*P(\\theta) \\\\\nf_{\\theta|\\{x_1, x_2, \\ldots, x_n\\}}(p) &\\propto \\left [ \\prod _{i=1} ^n {p^{x_i}(1-p)^{1-x_i}} \\right ]*\\left [ p^{\\alpha-1}(1-p)^{\\beta-1} \\right ] \\\\\nf_{\\theta|\\{x_1, x_2, \\ldots, x_n\\}}(p) &\\propto p^{\\sum _{i=1} ^n x_i + \\alpha - 1}(1-p)^{\\sum _{i=1} ^n(1-x_i) + \\beta - 1}\n\\end{align*}\\] i.e. we get, \\[\n\\text{BETA PRIOR }(\\alpha, \\beta) \\xrightarrow[Bernoulli]{\\{x_1, x_2, \\ldots, x_n\\}} \\text{BETA POSTERIOR }(\\alpha + n_h, \\beta + n_t)\n\\] \\[\n\\therefore \\hat{p_{\\text{ML}}} = \\mathbb{E}[\\text{Posterior}]=\\mathbb{E}[\\text{Beta}(\\alpha +n_h, \\beta + n_t)]= \\frac{\\alpha + n_h}{\\alpha + n_h + \\beta + n_t}\n\\]"
  },
  {
    "objectID": "pages/Wk04.html#convexity-and-jensens-inequality",
    "href": "pages/Wk04.html#convexity-and-jensens-inequality",
    "title": "Unsupervised Learning - Estimation - Recap of MLE + Bayesian estimation, Gaussian Mixture Model - EM algorithm",
    "section": "Convexity and Jensen’s Inequality",
    "text": "Convexity and Jensen’s Inequality\nConvexity is a property of a function or set that implies a unique line segment can be drawn between any two points within the function or set. For a concave function, this property can be expressed as, \\[\nf \\left (\\sum _{k=1} ^K \\lambda_k a_k \\right ) \\ge \\sum _{k=1} ^K \\lambda_k f(a_k)\n\\] where \\[\n\\sum _{k=1} ^K \\lambda _k = 1\n\\] \\[\na_k \\text{ are points of the function}\n\\] This is also known as Jensen’s Inequality."
  },
  {
    "objectID": "pages/Wk05.html",
    "href": "pages/Wk05.html",
    "title": "Supervised Learning - Regression - Least Squares; Bayesian view",
    "section": "",
    "text": "PDF Link: notes"
  },
  {
    "objectID": "pages/Wk05.html#stochastic-gradient-descent",
    "href": "pages/Wk05.html#stochastic-gradient-descent",
    "title": "Supervised Learning - Regression - Least Squares; Bayesian view",
    "section": "Stochastic Gradient Descent",
    "text": "Stochastic Gradient Descent\nStochastic gradient descent (SGD) is an optimization algorithm widely employed in machine learning to minimize the loss function of a model by determining the optimal parameters. Unlike traditional gradient descent, which updates the model parameters based on the entire dataset, SGD updates the parameters using a randomly selected subset of the data, known as a batch. This approach leads to faster training times and makes SGD particularly suitable for handling large datasets.\nInstead of updating \\(\\mathbf{w}\\) using the entire dataset at each step \\(t\\), SGD leverages a small randomly selected subset of \\(k\\) data points to update \\(\\mathbf{w}\\). Consequently, the new gradient becomes \\(2(\\tilde{\\mathbf{X}}\\tilde{\\mathbf{X}}^T\\mathbf{w}^t - \\tilde{\\mathbf{X}}\\tilde{\\mathbf{y}})\\), where \\(\\tilde{\\mathbf{X}}\\) and \\(\\tilde{\\mathbf{y}}\\) represent small samples randomly chosen from the dataset. This strategy is feasible since \\(\\tilde{\\mathbf{X}} \\in \\mathbb{R}^{d \\times k}\\), which is considerably smaller compared to \\(\\mathbf{X}\\).\nAfter \\(T\\) rounds of training, the final estimate is obtained as follows:\n\\[\n\\mathbf{w}_{\\text{SGD}}^T = \\frac{1}{T} \\sum_{i=1}^T \\mathbf{w}^i\n\\]\nThe stochastic nature of SGD contributes to optimal convergence to a certain extent."
  },
  {
    "objectID": "pages/Wk07.html",
    "href": "pages/Wk07.html",
    "title": "Supervised Learning - Classification - K-NN, Decision tree",
    "section": "",
    "text": "PDF Link: notes"
  },
  {
    "objectID": "pages/Wk07.html#issues-with-k-nn",
    "href": "pages/Wk07.html#issues-with-k-nn",
    "title": "Supervised Learning - Classification - K-NN, Decision tree",
    "section": "Issues with K-NN",
    "text": "Issues with K-NN\nThe K-NN algorithm suffers from several limitations:\n\nThe choice of distance function can yield different results. The Euclidean distance, commonly used, might not always be the best fit for all scenarios.\nComputationally, the algorithm can be demanding. When making predictions for a single test data point, the distances between that data point and all training points must be calculated and sorted. Consequently, the algorithm has a complexity of \\(O(n \\log(n))\\), where \\(n\\) represents the size of the dataset.\nThe algorithm does not learn a model but instead relies on the training dataset for making predictions."
  },
  {
    "objectID": "pages/Wk07.html#goodness-of-a-question",
    "href": "pages/Wk07.html#goodness-of-a-question",
    "title": "Supervised Learning - Classification - K-NN, Decision tree",
    "section": "Goodness of a Question",
    "text": "Goodness of a Question\nTo evaluate the quality of a question, we need a measure of “impurity” for a set of labels \\(\\{y_1, \\ldots, y_n\\}\\). Various measures can be employed, but we will use the Entropy function.\nThe Entropy function is defined as:\n\\[\n\\text{Entropy}(\\{y_1, \\ldots, y_n\\}) = \\text{Entropy}(p) = -\\left( p\\log(p)+(1-p)\\log(1-p) \\right )\n\\]\nHere, \\(\\log(0)\\) is conventionally treated as \\(0\\).\nPictorial representation of the Entropy function:\n\nInformation Gain is then utilized to measure the quality of a split in the decision tree algorithm.\nInformation gain is a commonly used criterion in decision tree algorithms that quantifies the reduction in entropy or impurity of a dataset after splitting based on a given feature. High information gain signifies features that effectively differentiate between the different classes of data and lead to accurate predictions.\nInformation gain is calculated as:\n\\[\n\\text{Information Gain}(\\text{feature}, \\text{value}) = \\text{Entropy}(D) - \\left[\\gamma \\cdot \\text{Entropy}(D_{\\text{yes}}) + (1-\\gamma) \\cdot \\text{Entropy}(D_{\\text{no}}) \\right]\n\\]\nwhere \\(\\gamma\\) is defined as:\n\\[\n\\gamma = \\frac{|D_{\\text{yes}}|}{|D|}\n\\]"
  },
  {
    "objectID": "pages/Wk07.html#decision-tree-algorithm",
    "href": "pages/Wk07.html#decision-tree-algorithm",
    "title": "Supervised Learning - Classification - K-NN, Decision tree",
    "section": "Decision Tree Algorithm",
    "text": "Decision Tree Algorithm\nThe decision tree algorithm follows these steps:\n\nDiscretize each feature within the range [min, max].\nSelect the question that provides the highest information gain.\nRepeat the procedure for subsets \\(D_{\\text{yes}}\\) and \\(D_{\\text{no}}\\).\nStop growing the tree when a node becomes sufficiently “pure” according to a predefined criterion.\n\nDifferent measures, such as the Gini Index, can also be employed to evaluate the quality of a question.\nPictorial depiction of the decision boundary and its decision tree:"
  }
]