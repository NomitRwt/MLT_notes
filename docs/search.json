[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CS2007: Machine Learning Techniques",
    "section": "",
    "text": "Note\n\n\n\nThis site is still under development.\nFeedback/Correction: Click here!.\n\n\n\n\n\n\nWeek\nTopic\nLecture Videos\nLecture Slides\nNotes PDF\nTutorial Video\nTutorial Slides\nTutorial Colab\n\n\n\n\n1\nIntroduction; Unsupervised Learning - Representation learning - PCA\n🖥️\n🎫\n📝\n🖥️\n🖥️\n🎫\n\n\n2\nUnsupervised Learning - Representation learning - Kernel PCA\n🖥️\n🎫\n📝\n🖥️\n🖥️\n🎫\n\n\n3\nUnsupervised Learning - Clustering - K-means/Kernel K-means\n🖥️\n🎫\n📝\n🖥️\n🖥️\n🎫\n\n\n4\nUnsupervised Learning - Estimation - Recap of MLE + Bayesian estimation, Gaussian Mixture Model - EM algorithm\n🖥️\n🎫\n📝\n\n\n\n\n\n5\nSupervised Learning - Regression - Least Squares; Bayesian view\n🖥️\n🎫\n📝\n🖥️\n🖥️\n🎫\n\n\n6\nSupervised Learning - Regression - Ridge/LASSO\n🖥️\n🎫\n📝\n\n\n\n\n\n7\nSupervised Learning - Classification - K-NN, Decision tree\n🖥️\n🎫\n📝\n\n\n\n\n\n8\nSupervised Learning - Classification - Generative Models - Naive Bayes\n🖥️\n🎫\n📝\n\n\n\n\n\n9\nDiscriminative Models - Perceptron; Logistic Regression\n🖥️\n🎫\n📝\n\n\n\n\n\n10\nSupport Vector Machines\n🖥️\n🎫\n📝\n\n\n\n\n\n11\nEnsemble methods - Bagging and Boosting (Adaboost)\n🖥️\n🎫\n\n\n\n\n\n\n12\nArtificial Neural networks; Multiclass classification\n🖥️\n🎫"
  },
  {
    "objectID": "pages/Wk10.html",
    "href": "pages/Wk10.html",
    "title": "Support Vector Machines",
    "section": "",
    "text": "Note\n\n\n\nFeedback/Correction: Click here!.\nPDF Link: Click here!"
  },
  {
    "objectID": "pages/Wk10.html#defining-key-parameters",
    "href": "pages/Wk10.html#defining-key-parameters",
    "title": "Support Vector Machines",
    "section": "Defining Key Parameters",
    "text": "Defining Key Parameters\nWe begin by introducing essential parameters:\n\nWeight Vector: Let \\(\\mathbf{w^*} \\in \\mathbb{R}^d\\) be the weight vector such that \\((\\mathbf{w^{*T}x_i})y_i\\ge\\gamma\\) for all \\(i\\). This weight vector ensures the desired margin \\(\\gamma\\).\nBounding Radius: We define \\(R&gt;0 \\in \\mathbb{R}\\) such that for all \\(i\\), \\(||\\mathbf{x_i}||\\le R\\). This represents a constraint on the norm of input data.\n\nWith these parameters established, we can formulate the upper bound on the number of mistakes made by the perceptron learning algorithm: \\[\n\\text{\\#mistakes} \\le \\frac{R^2}{\\gamma^2}\n\\]"
  },
  {
    "objectID": "pages/Wk10.html#observations-and-objectives",
    "href": "pages/Wk10.html#observations-and-objectives",
    "title": "Support Vector Machines",
    "section": "Observations and Objectives",
    "text": "Observations and Objectives\n\nObservations\nWe make several observations regarding the perceptron learning algorithm:\n\nThe “quality” of the solution depends on the margin \\(\\gamma\\).\nThe number of mistakes is influenced by the margin associated with \\(\\mathbf{w^*}\\).\nThe weight vector \\(\\mathbf{w_{perc}}\\) need not be identical to \\(\\mathbf{w^*}\\).\n\n\n\nGoal: Margin Maximization\nGiven these observations, our overarching goal is to find the solution that maximizes the margin. It is crucial to note that a single dataset can have multiple linear classifiers with varying margins, as depicted in the diagram below:\n\n\n\nMultiple Classifiers\n\n\nTo formalize our objective, we aim to maximize the margin \\(\\gamma\\): \\[\n\\max_{\\mathbf{w},\\gamma} \\gamma\n\\]\nSubject to the following constraints: \\[\\begin{align*}\n(\\mathbf{w^T x_i})y_i &\\ge \\gamma \\quad \\text{for all } i \\\\\n||\\mathbf{w}||^2 &= 1\n\\end{align*}\\]"
  },
  {
    "objectID": "pages/Wk10.html#reformulating-the-objective",
    "href": "pages/Wk10.html#reformulating-the-objective",
    "title": "Support Vector Machines",
    "section": "Reformulating the Objective",
    "text": "Reformulating the Objective\nTo simplify our objective, we can express \\(\\gamma\\) in terms of the width of \\(\\mathbf{w}\\): \\[\n\\max_{\\mathbf{w}} \\text{width}(\\mathbf{w})\n\\]\nSubject to the constraint: \\[\n(\\mathbf{w^T x_i})y_i \\ge 1 \\quad \\text{for all } i\n\\]\n\nWidth Calculation\nThe width of the margin, denoted as \\(\\text{width}(\\mathbf{w})\\), can be calculated as the distance between two parallel margins. Consider two points \\(\\mathbf{x}\\) and \\(\\mathbf{z}\\) lying on opposite sides of the decision boundary such that \\(\\mathbf{w^T x}=-1\\) and \\(\\mathbf{w^T z}=1\\) or vice versa.\nLet \\(\\mathbf{x_1}\\) and \\(\\mathbf{x_2}\\) be two points on the margins and opposite sides of the decision boundary.\n\n\n\nMargin Width\n\n\nThe width is given by: \\[\\begin{align*}\n\\mathbf{x_1^T w} - \\mathbf{x_2^T w} &= 2 \\\\\n(\\mathbf{x_1}-\\mathbf{x_2})^T\\mathbf{w} &= 2\\\\\n||\\mathbf{x_1} - \\mathbf{x_2}||_2||\\mathbf{w}||_2 \\cos(\\theta) &= 2 \\\\\n\\therefore ||\\mathbf{x_1} - \\mathbf{x_2}||_2 &= \\frac{2}{||\\mathbf{w}||_2}\n\\end{align*}\\]\nHence, our objective can be restated as: \\[\n\\max_{\\mathbf{w}}  \\frac{2}{||\\mathbf{w}||^2_2} \\quad \\text{subject to} \\quad (\\mathbf{w^T x_i})y_i \\ge 1 \\quad \\text{for all } i\n\\]\nEquivalently, we can frame it as a minimization problem: \\[\n\\min_{\\mathbf{w}}  \\frac{1}{2}||\\mathbf{w}||^2_2 \\quad \\text{subject to} \\quad (\\mathbf{w^T x_i})y_i \\ge 1 \\quad \\text{for all } i\n\\]\nIn summary, finding the separating hyperplane with the maximum margin is equivalent to finding the one with the smallest possible normal vector \\(\\mathbf{w}\\)."
  },
  {
    "objectID": "pages/Wk10.html#lagrange-function",
    "href": "pages/Wk10.html#lagrange-function",
    "title": "Support Vector Machines",
    "section": "Lagrange Function",
    "text": "Lagrange Function\nFor the optimization problem described above, the Lagrange function, denoted as \\(\\mathcal{L}(\\mathbf{w}, \\boldsymbol{\\alpha})\\), is defined as follows: \\[\n\\mathcal{L}(\\mathbf{w}, \\boldsymbol{\\alpha}) = f(\\mathbf{w}) + \\boldsymbol{\\alpha}^T g(\\mathbf{w}) \\quad \\text{for all } \\mathbf{w}\n\\]\nHere, \\(\\boldsymbol{\\alpha}\\) is a vector of Lagrange multipliers, constrained to be non-negative (\\(\\boldsymbol{\\alpha} \\ge \\mathbf{0}\\)).\nMaximizing the Lagrange function with respect to \\(\\boldsymbol{\\alpha}\\) leads us to the following formulation: \\[\\begin{align*}\n\\max_{\\boldsymbol{\\alpha} \\ge \\mathbf{0}} \\mathcal{L}(\\mathbf{w}, \\boldsymbol{\\alpha}) &= \\max_{\\boldsymbol{\\alpha} \\ge \\mathbf{0}} \\left(f(\\mathbf{w}) + \\boldsymbol{\\alpha}^T g(\\mathbf{w})\\right) \\\\\n&=\n\\begin{cases}\n\\infty & \\text{if } g(\\mathbf{w}) &gt; \\mathbf{0} \\\\\nf(\\mathbf{w}) & \\text{if } g(\\mathbf{w}) \\le \\mathbf{0}\n\\end{cases}\n\\end{align*}\\]\nSince the Lagrange function is equivalent to \\(f(\\mathbf{w})\\) when \\(g(\\mathbf{w}) \\le \\mathbf{0}\\), we can rewrite our original problem as follows: \\[\\begin{align*}\n\\min_{\\mathbf{w}} f(\\mathbf{w}) &= \\min_{\\mathbf{w}} \\left[\\max_{\\boldsymbol{\\alpha} \\ge \\mathbf{0}} \\mathcal{L}(\\mathbf{w}, \\boldsymbol{\\alpha})\\right] \\\\\n&= \\min_{\\mathbf{w}} \\left[\\max_{\\boldsymbol{\\alpha} \\ge \\mathbf{0}} \\left(f(\\mathbf{w}) + \\boldsymbol{\\alpha}^T g(\\mathbf{w})\\right)\\right]\n\\end{align*}\\]\nIn general, interchanging the positions of the min and max functions is not permissible unless all involved functions are convex. In our case, both \\(f\\) and \\(g\\) are convex functions, allowing us to express this as: \\[\n\\min_{\\mathbf{w}} \\left[\\max_{\\boldsymbol{\\alpha} \\ge \\mathbf{0}} \\left(f(\\mathbf{w}) + \\boldsymbol{\\alpha}^T g(\\mathbf{w})\\right)\\right] \\equiv \\max_{\\boldsymbol{\\alpha} \\ge \\mathbf{0}} \\left[\\min_{\\mathbf{w}} \\left(f(\\mathbf{w}) + \\boldsymbol{\\alpha}^T g(\\mathbf{w})\\right)\\right]\n\\]"
  },
  {
    "objectID": "pages/Wk10.html#multiple-constraints",
    "href": "pages/Wk10.html#multiple-constraints",
    "title": "Support Vector Machines",
    "section": "Multiple Constraints",
    "text": "Multiple Constraints\nNow, extending our discussion to scenarios with \\(m\\) constraints, denoted as \\(g_i(\\mathbf{w}) \\le \\mathbf{0}\\) for \\(i \\in [1, m]\\), we can represent the problem as follows: \\[\\begin{align*}\n\\min_{\\mathbf{w}} f(\\mathbf{w}) &\\equiv \\min_{\\mathbf{w}} \\left[\\max_{\\boldsymbol{\\alpha} \\ge \\mathbf{0}} f(\\mathbf{w}) + \\sum _{i=1} ^m \\alpha _i g_i(\\mathbf{w})\\right] \\\\\n&\\equiv \\max_{\\boldsymbol{\\alpha} \\ge \\mathbf{0}} \\left[\\min_{\\mathbf{w}} f(\\mathbf{w}) + \\sum _{i=1} ^m \\alpha _i g_i(\\mathbf{w})\\right]\n\\end{align*}\\]"
  },
  {
    "objectID": "pages/Wk10.html#duality-revisited",
    "href": "pages/Wk10.html#duality-revisited",
    "title": "Support Vector Machines",
    "section": "Duality Revisited",
    "text": "Duality Revisited\nRevisiting the Lagrangian function: \\[\n\\min_{\\mathbf{w}} \\left [\\max_{\\boldsymbol{\\alpha} \\ge \\mathbf{0}} f(\\mathbf{w}) + \\boldsymbol{\\alpha}^T g(\\mathbf{w}) \\right ] \\equiv \\max_{\\boldsymbol{\\alpha} \\ge \\mathbf{0}} \\left [ \\min_{\\mathbf{w}} f(\\mathbf{w}) + \\boldsymbol{\\alpha}^T g(\\mathbf{w}) \\right ]\n\\]\nHere, the primal function is on the left-hand side, and the dual function is on the right. The solutions for the primal and dual functions are denoted as \\(\\mathbf{w}^∗\\) and \\(\\boldsymbol{\\alpha}^*\\), respectively. When these solutions are substituted into the equation, we obtain: \\[\n\\max_{\\boldsymbol{\\alpha} \\ge \\mathbf{0}} f(\\mathbf{w}^*) + \\boldsymbol{\\alpha}^T g(\\mathbf{w}^*) =  \\min_{\\mathbf{w}} f(\\mathbf{w}) + \\boldsymbol{\\alpha}^{*T} g(\\mathbf{w})\n\\]\nGiven that \\(g(\\mathbf{w}^*) \\le \\mathbf{0}\\), the left-hand side simplifies to \\(f(\\mathbf{w}^*)\\): \\[\nf(\\mathbf{w}^*) =  \\min_{\\mathbf{w}} f(\\mathbf{w}) + \\boldsymbol{\\alpha}^{*T} g(\\mathbf{w})\n\\]\nSubstituting \\(\\mathbf{w}^*\\) for \\(\\mathbf{w}\\) in the right-hand side yields a new right-hand side that is greater than or equal to the current one: \\[\nf(\\mathbf{w}^*) \\le f(\\mathbf{w}^*) + \\boldsymbol{\\alpha}^{*T} g(\\mathbf{w}^*)\n\\]\nHence, we can infer: \\[\n\\boldsymbol{\\alpha}^{*T} g(\\mathbf{w}^*) \\ge \\mathbf{0}\n\\]\nHowever, considering the constraints, where \\(\\boldsymbol{\\alpha}^{*} \\ge \\mathbf{0}\\) and \\(g(\\mathbf{w}^*)\\le \\mathbf{0}\\), we arrive at: \\[\n\\boldsymbol{\\alpha}^{*T} g(\\mathbf{w}^*) \\le \\mathbf{0}\n\\]\nFrom this, we deduce: \\[\n\\boldsymbol{\\alpha}^{*T} g(\\mathbf{w}^*) = \\mathbf{0}\n\\]\nExtending this equation for multiple constraints, we obtain: \\[\n\\alpha_i^{*} g(w^*_i) = 0 \\quad \\forall i\n\\]\nHence, if one of the two values is greater than zero, the other must be zero. Considering \\(g(\\mathbf{w}^*) = 1-(\\mathbf{w}^T\\mathbf{x}_i)y_i\\), we can express this as: \\[\n\\alpha_i^{*} (1-(\\mathbf{w}^T\\mathbf{x}_i)y_i) = 0 \\quad \\forall i\n\\]\nImportantly, when \\(\\alpha_i &gt; \\mathbf{0}\\), we deduce: \\[\n(\\mathbf{w}^T\\mathbf{x}_i)y_i = 1\n\\]\nThis implies that the \\(i^{th}\\) data point resides on the “Supporting” hyperplane and significantly contributes to the determination of \\(\\mathbf{w}^*\\).\nConsequently, data points with \\(\\alpha_i &gt; \\mathbf{0}\\) earn the title of Support Vectors, and this algorithm is known as the Support Vector Machine (SVM)."
  },
  {
    "objectID": "pages/Wk10.html#definition-of-support-vector-machines-svms",
    "href": "pages/Wk10.html#definition-of-support-vector-machines-svms",
    "title": "Support Vector Machines",
    "section": "Definition of Support Vector Machines (SVMs)",
    "text": "Definition of Support Vector Machines (SVMs)\nSupport Vector Machines (SVMs) stand as a category of supervised learning algorithms designed for classification and regression analysis. SVMs aim to identify the optimal hyperplane that maximizes the margin between data points from different classes. In scenarios where data is not linearly separable, SVMs employ kernel functions to map the data into a higher-dimensional space where a linear decision boundary can effectively segregate the classes.\nInsight: \\(\\mathbf{w}^*\\) represents a sparse linear combination of data points."
  },
  {
    "objectID": "pages/Wk10.html#hard-margin-svm-algorithm",
    "href": "pages/Wk10.html#hard-margin-svm-algorithm",
    "title": "Support Vector Machines",
    "section": "Hard-Margin SVM Algorithm",
    "text": "Hard-Margin SVM Algorithm\nThe Hard-Margin SVM algorithm is applicable only when the dataset is linearly separable with a margin parameter \\(\\gamma &gt; 0\\). Its key steps include:\n\nDirect or Kernelized Calculation of Q: Compute the matrix \\(Q=\\mathbf{X}^T\\mathbf{X}\\) directly or using a kernel, based on the dataset.\nGradient Descent: Employ the gradient of the dual formula, \\(\\boldsymbol{\\alpha}^T\\mathbf{1} - \\frac{1}{2}\\boldsymbol{\\alpha}^T\\mathbf{Y}^T Q\\mathbf{Y}\\boldsymbol{\\alpha}\\), in a gradient descent algorithm to iteratively find a satisfactory set of Lagrange multipliers \\(\\boldsymbol{\\alpha}\\). Initialize \\(\\boldsymbol{\\alpha}\\) as a zero vector in \\(\\mathbb{R}^n_+\\).\nPrediction: For prediction, follow these formulas:\n\nFor non-kernelized SVM:\n\n\\(\\textbf{label}(\\mathbf{x}_{\\text{test}}) = \\text{sign}( \\mathbf{w}^T\\mathbf{x}_{\\text{test}}) = \\text{sign}\\left(\\displaystyle\\sum _{i=1} ^n \\alpha _i y_i(\\mathbf{x}_i^T\\mathbf{x}_{\\text{test}})\\right)\\)\n\nFor kernelized SVM:\n\n\\(\\textbf{label}(\\mathbf{x}_{\\text{test}}) = \\text{sign}(\\mathbf{w}^T\\boldsymbol{\\phi}(\\mathbf{x}_{\\text{test}})) = \\text{sign}\\left(\\displaystyle\\sum _{i=1} ^n \\alpha _i y_i k(\\mathbf{x}_i^T\\mathbf{x}_{\\text{test}})\\right)\\)"
  },
  {
    "objectID": "pages/Wk02.html",
    "href": "pages/Wk02.html",
    "title": "Unsupervised Learning - Representation learning - Kernel PCA",
    "section": "",
    "text": "Note\n\n\n\nFeedback/Correction: Click here!.\nPDF Link: Click here!"
  },
  {
    "objectID": "pages/Wk02.html#transforming-features",
    "href": "pages/Wk02.html#transforming-features",
    "title": "Unsupervised Learning - Representation learning - Kernel PCA",
    "section": "Transforming Features",
    "text": "Transforming Features\nTo address non-linear relationships, we propose mapping the dataset to higher dimensions as follows: \\[\n\\mathbf{x} \\to \\phi(\\mathbf{x}) \\quad \\mathbb{R}^d \\to \\mathbb{R}^D \\quad \\text{where } [D &gt;&gt; d]\n\\]\nTo compute \\(D\\), let \\(\\mathbf{x}=\\left [ \\begin{array} {cc}  f_1 & f_2 \\end{array} \\right ]\\) represent features of a dataset containing datapoints lying on a second-degree curve in a two-dimensional space.\nTo convert it from quadratic to linear, we map the features to: \\[\n\\phi(\\mathbf{x})=\\left [\n\\begin{array} {cccccc}\n    1 & f_1^2 & f_2^2 & f_1f_2 & f_1 & f_2\n\\end{array}\n\\right ]\n\\]\nMapping \\(d\\) features to the polynomial power \\(p\\) results in \\(^{d+p} C_d\\) new features.\nHowever, it is essential to note that finding \\(\\phi(\\mathbf{x})\\) may be computationally demanding.\nTo overcome this issue, we present the solution in the subsequent section."
  },
  {
    "objectID": "pages/Wk02.html#kernel-functions",
    "href": "pages/Wk02.html#kernel-functions",
    "title": "Unsupervised Learning - Representation learning - Kernel PCA",
    "section": "Kernel Functions",
    "text": "Kernel Functions\nA function \\(k: \\mathbb{R}^d \\times \\mathbb{R}^d \\to \\mathbb{R}\\) is considered a “valid” Kernel Function if it maps data points to the real numbers.\nProof of a “Valid” Kernel: There are two methods to establish the validity of a kernel:\n\nMethod 1: Explicitly exhibit the mapping to \\(\\phi\\), which may be challenging in certain cases.\nMethod 2: Utilize Mercer’s Theorem, which states that \\(k: \\mathbb{R}^d \\times \\mathbb{R}^d \\to \\mathbb{R}\\) is a valid kernel if and only if:\n\n\\(k\\) is symmetric, i.e., \\(k(\\mathbf{x},\\mathbf{x}') = k(\\mathbf{x}',\\mathbf{x})\\)\nFor any dataset \\(\\{\\mathbf{x}_1,\\mathbf{x}_2,\\ldots,\\mathbf{x}_n\\}\\), the matrix \\(\\mathbf{K} \\in \\mathbb{R}^{n \\times n}\\), where \\(\\mathbf{K}_{ij} = k(\\mathbf{x}_i,\\mathbf{x}_j)\\), is Positive Semi-Definite.\n\n\nTwo popular kernel functions are:\n\nPolynomial Kernel: \\(k(\\mathbf{x},\\mathbf{x}') = (\\mathbf{x}^T\\mathbf{x} + 1)^p\\)\nRadial Basis Function Kernel or Gaussian Kernel: \\(k(\\mathbf{x},\\mathbf{x}') = \\exp\\left(\\displaystyle-\\frac{\\|\\mathbf{x}-\\mathbf{x}'\\|^2}{2\\sigma^2}\\right)\\)"
  },
  {
    "objectID": "pages/Wk08.html",
    "href": "pages/Wk08.html",
    "title": "Supervised Learning - Classification - Generative Models - Naive Bayes",
    "section": "",
    "text": "Note\n\n\n\nFeedback/Correction: Click here!.\nPDF Link: Click here!"
  },
  {
    "objectID": "pages/Wk08.html#alternate-generative-model",
    "href": "pages/Wk08.html#alternate-generative-model",
    "title": "Supervised Learning - Classification - Generative Models - Naive Bayes",
    "section": "Alternate Generative Model",
    "text": "Alternate Generative Model\nAn alternative generative model starts with the class conditional independence assumption, which is a common assumption in various machine learning algorithms. This assumption states that the features of an object are conditionally independent given its class label.\nLet us again consider the dataset \\(D=\\{(\\mathbf{x}_1, y_1), \\ldots, (\\mathbf{x}_n,y_n)\\}\\), with \\(\\mathbf{x}_i \\in \\{0, 1\\}^d\\) and \\(y_i \\in \\{0, 1\\}\\).\nThe general steps of the algorithm under this alternative model are as follows:\n\nDecide the labels by tossing a coin with \\(P(y_i=1)=p\\).\nDetermine the features for \\(\\mathbf{x}\\) given \\(y\\) using the following conditional probability: \\[\nP(\\mathbf{x} = [f_1, f_2, \\ldots, f_d]|y) = \\prod_{i=1}^d(p^{y_i}_i)^{f_i}(1-p^{y_i}_i)^{1-f_i}\n\\]\n\nThe parameters in this alternative model are as follows:\n\nParameter \\(\\hat{p}\\) to decide the label: 1\nParameters for \\(P(\\mathbf{x}|y=1)\\): \\(d\\)\nParameters for \\(P(\\mathbf{x}|y=0)\\): \\(d\\)\n\nThus, the total number of parameters is given by: \\[\\begin{align*}\n    & = 1 + d + d \\\\\n    & = 2d + 1\n\\end{align*}\\]\nThe parameters are estimated using Maximum Likelihood Estimation."
  },
  {
    "objectID": "pages/Wk08.html#prediction-using-the-parameters",
    "href": "pages/Wk08.html#prediction-using-the-parameters",
    "title": "Supervised Learning - Classification - Generative Models - Naive Bayes",
    "section": "Prediction using the parameters",
    "text": "Prediction using the parameters\nGiven \\(\\mathbf{x}^{test}\\in\\{0,1\\}^d\\), the prediction for \\(\\hat{y}^{test}\\) is done using the following criterion:\n\\[\nP(\\hat{y}^{test}=1|\\mathbf{x}^{test}) \\ge P(\\hat{y}^{test}=0|\\mathbf{x}^{test})\n\\]\nIf the above inequality holds, then \\(\\hat{y}^{test}=1\\), otherwise \\(\\hat{y}^{test}=0\\).\nUsing Bayes’ rule, we can express \\(P(\\hat{y}^{test}=1|\\mathbf{x}^{test})\\) and \\(P(\\hat{y}^{test}=0|\\mathbf{x}^{test})\\) as follows:\n\\[\\begin{align*}\nP(\\hat{y}^{test}=1|\\mathbf{x}^{test}) & = \\frac{P(\\mathbf{x}^{test}|\\hat{y}^{test}=1)*P(\\hat{y}^{test}=1)}{P(\\mathbf{x}^{test})} \\\\\nP(\\hat{y}^{test}=0|\\mathbf{x}^{test}) & = \\frac{P(\\mathbf{x}^{test}|\\hat{y}^{test}=0)*P(\\hat{y}^{test}=0)}{P(\\mathbf{x}^{test})}\n\\end{align*}\\]\nHowever, since we are only interested in the comparison of these probabilities, we can avoid calculating \\(P(\\mathbf{x}^{test})\\).\nBy solving for \\(P(\\mathbf{x}^{test}|\\hat{y}^{test}=1)*P(\\hat{y}^{test}=1)\\), we find:\n\\[\\begin{align*}\n&=P(\\mathbf{x}^{test} = [f_1, f_2, \\ldots, f_d]|y^{test}=1)*P(\\hat{y}^{test}=1) \\\\\n&=\\left(\\prod_{i=1}^d(\\hat{p}^1_i)^{f_i}(1-\\hat{p}^1_i)^{1-f_i}\\right)*\\hat{p}\n\\end{align*}\\]\nSimilarly, we can obtain \\(P(\\mathbf{x}^{test}|\\hat{y}^{test}=0)*P(\\hat{y}^{test}=0)\\).\nTherefore, we predict \\(\\hat{y}^{test}=1\\) if:\n\\[\n\\left(\\prod_{i=1}^d(\\hat{p}^1_i)^{f_i}(1-\\hat{p}^1_i)^{1-f_i}\\right)*\\hat{p} \\ge \\left(\\prod_{i=1}^d(\\hat{p}^0_i)^{f_i}(1-\\hat{p}^0_i)^{1-f_i}\\right)*(1-\\hat{p})\n\\]\nOtherwise, we predict \\(\\hat{y}^{test}=0\\).\nThe Naive Bayes algorithm employs two main techniques:\n\nThe Class Conditional Independence Assumption.\nUtilizing Bayes’ Rule.\n\nAs a result, this algorithm is commonly referred to as Naive Bayes.\nIn summary, Naive Bayes is a classification algorithm based on Bayes’ theorem, which assumes that the features are independent of each other given the class label. It estimates the conditional probabilities of features given the class and uses these probabilities to make predictions for new data. Despite its naive assumption, Naive Bayes has demonstrated good performance across various applications, particularly when dealing with high-dimensional data and limited training examples."
  },
  {
    "objectID": "pages/Wk08.html#pitfalls-of-naive-bayes",
    "href": "pages/Wk08.html#pitfalls-of-naive-bayes",
    "title": "Supervised Learning - Classification - Generative Models - Naive Bayes",
    "section": "Pitfalls of Naive Bayes",
    "text": "Pitfalls of Naive Bayes\nOne prominent issue with Naive Bayes is that if a feature is not observed in the training set but present in the testing set, the prediction probabilities for both classes become zero.\n\\[\\begin{align*}\nP(\\hat{y}^{test}=1|\\mathbf{x}^{test} = [f_1, f_2, \\ldots, f_d]) & \\propto \\left(\\prod_{i=1}^d(\\hat{p}^1_i)^{f_i}(1-\\hat{p}^1_i)^{1-f_i}\\right)*\\hat{p} \\\\\nP(\\hat{y}^{test}=0|\\mathbf{x}^{test} = [f_1, f_2, \\ldots, f_d]) & \\propto \\left(\\prod_{i=1}^d(\\hat{p}^0_i)^{f_i}(1-\\hat{p}^0_i)^{1-f_i}\\right)*(1-\\hat{p})\n\\end{align*}\\]\nIf any feature \\(f_i\\) was absent in the training set, it results in \\(\\hat{p}^1_i=\\hat{p}^0_i=0\\), leading to \\(P(\\hat{y}^{test}=0|\\mathbf{x}^{test})=P(\\hat{y}^{test}=1|\\mathbf{x}^{test})=0\\).\nA popular remedy for this issue is to introduce two “pseudo” data points with labels 1 and 0, respectively, into the dataset, where all their features are set to 1. This technique is also known as Laplace smoothing.\nIn brief, Laplace smoothing is a technique employed to address the zero-frequency problem in probabilistic models, particularly in text classification. It involves adding a small constant value to the count of each feature and the number of unique classes to avoid zero probability estimates, which can cause problems during model training and prediction. By incorporating this smoothing term, the model becomes more robust and better suited for handling unseen data."
  },
  {
    "objectID": "pages/Wk08.html#prediction-using-bayes-rule",
    "href": "pages/Wk08.html#prediction-using-bayes-rule",
    "title": "Supervised Learning - Classification - Generative Models - Naive Bayes",
    "section": "Prediction using Bayes’ Rule",
    "text": "Prediction using Bayes’ Rule\nPrediction is based on the following equation: \\[\nP(y_{test}=1|\\mathbf{x}_{test})\\propto P(\\mathbf{x}_{test}|y_{test})*P(y_{test})\n\\]\nWhere \\(P(\\mathbf{x}_{test}|y_{test})\\equiv f(\\mathbf{x}_{test};\\hat{\\boldsymbol{\\mu}}_{y_{test}}, \\hat{\\boldsymbol{\\Sigma}})\\) and \\(P(y_{test})\\equiv \\hat{p}\\).\nTo predict \\(y_{test}=1\\), we compare the probabilities:\n\\[\\begin{align*}\nf(\\mathbf{x}_{i} ;\\hat{\\boldsymbol{\\mu} }_{1} ,\\hat{\\boldsymbol{\\Sigma} }_{1} )\\hat{p} & \\geq f(\\mathbf{x}_{i} ;\\hat{\\boldsymbol{\\mu} }_{0} ,\\hat{\\boldsymbol{\\Sigma} }_{0} )(1-\\hat{p} )\\\\\ne^{-(\\mathbf{x}_{i} -\\hat{\\boldsymbol{\\mu} }_{1} )^{T}\\hat{\\boldsymbol{\\Sigma} }_{1}^{-1} (\\mathbf{x}_{i} -\\hat{\\boldsymbol{\\mu} }_{1} )}\\hat{p} & \\geq e^{-(\\mathbf{x}_{i} -\\hat{\\boldsymbol{\\mu} }_{0} )^{T}\\hat{\\boldsymbol{\\Sigma} }_{0}^{-1} (\\mathbf{x}_{i} -\\hat{\\boldsymbol{\\mu} }_{0} )} (1-\\hat{p} )\\\\\n-(\\mathbf{x}_{i} -\\hat{\\boldsymbol{\\mu} }_{1} )^{T}\\hat{\\boldsymbol{\\Sigma} }_{1}^{-1} (\\mathbf{x}_{i} -\\hat{\\boldsymbol{\\mu} }_{1} )+\\log (\\hat{p} ) & \\geq -(\\mathbf{x}_{i} -\\hat{\\boldsymbol{\\mu} }_{0} )^{T}\\hat{\\boldsymbol{\\Sigma} }_{0}^{-1} (\\mathbf{x}_{i} -\\hat{\\boldsymbol{\\mu} }_{0} )+\\log (1-\\hat{p} )\n\\end{align*}\\]\nThis inequality can be expressed as a linear decision function:\n\\[\n\\left( (\\hat{\\boldsymbol{\\mu}}_1-\\hat{\\boldsymbol{\\mu}}_0)^T\\hat{\\boldsymbol{\\Sigma}}^{-1} \\right)\\mathbf{x}_{test} + \\hat{\\boldsymbol{\\mu}}_0^T\\hat{\\boldsymbol{\\Sigma}}^{-1}\\hat{\\boldsymbol{\\mu}}_0 - \\hat{\\boldsymbol{\\mu}}_1^T\\hat{\\boldsymbol{\\Sigma}}^{-1}\\hat{\\boldsymbol{\\mu}}_1 + \\log(\\frac{1-\\hat{p}}{\\hat{p}}) \\ge 0\n\\]\nThus, the decision function of Gaussian Naive Bayes is linear."
  },
  {
    "objectID": "pages/Wk08.html#decision-boundaries-for-different-covariances",
    "href": "pages/Wk08.html#decision-boundaries-for-different-covariances",
    "title": "Supervised Learning - Classification - Generative Models - Naive Bayes",
    "section": "Decision Boundaries for Different Covariances",
    "text": "Decision Boundaries for Different Covariances\n\nWhen the covariance matrices are equal for both classes: As previously discussed, the decision boundary is linear.\n\n\n\n\nWhen the covariance matrices are equal for both classes\n\n\n\nWhen the covariance matrices are Identity matrices for both classes: The decision boundary is both linear and the perpendicular bisector of the line drawn from \\(\\hat{\\boldsymbol{\\mu}}_1\\) to \\(\\hat{\\boldsymbol{\\mu}}_0\\).\n\n\n\n\nWhen the covariance matrices are Identity matrices for both classes\n\n\n\nWhen the covariance matrices are not equal for both classes: Let \\(\\hat{\\boldsymbol{\\Sigma}}_1\\) and \\(\\hat{\\boldsymbol{\\Sigma}}_0\\) be the covariance matrices for classes 1 and 0, respectively. They are given by, \\[\\begin{align*}\n\\hat{\\boldsymbol{\\Sigma}}_1 &= \\frac{\\displaystyle \\sum_{i=1}^n(\\mathbb{1}(y_i=1)*\\mathbf{x}_i-\\hat{\\boldsymbol{\\mu}}_1)(\\mathbb{1}(y_i=1)*\\mathbf{x}_i-\\hat{\\boldsymbol{\\mu}}_1)^T}{\\displaystyle \\sum_{i=1}^n\\mathbb{1}(y_i=1)} \\\\\n\\hat{\\boldsymbol{\\Sigma}}_0 &= \\frac{\\displaystyle \\sum_{i=1}^n(\\mathbb{1}(y_i=0)*\\mathbf{x}_i-\\hat{\\boldsymbol{\\mu}}_0)(\\mathbb{1}(y_i=0)*\\mathbf{x}_i-\\hat{\\boldsymbol{\\mu}}_0)^T}{\\displaystyle \\sum_{i=1}^n\\mathbb{1}(y_i=0)}\n\\end{align*}\\] To predict \\(y_{test}=1\\), we compare the probabilities: \\[\\begin{align*}\nf(\\mathbf{x}_{test};\\hat{\\boldsymbol{\\mu}}_1, \\hat{\\boldsymbol{\\Sigma}}_1)*\\hat{p}&\\ge f(\\mathbf{x}_{test};\\hat{\\boldsymbol{\\mu}}_0, \\hat{\\boldsymbol{\\Sigma}}_0)*(1-\\hat{p}) \\\\\ne^{-(\\mathbf{x}_{test}-\\hat{\\boldsymbol{\\mu}}_1)^T\\hat{\\boldsymbol{\\Sigma}}_1(\\mathbf{x}_{test}-\\hat{\\boldsymbol{\\mu}}_1)}*\\hat{p}&\\ge e^{-(\\mathbf{x}_{test}-\\hat{\\boldsymbol{\\mu}}_0)^T\\hat{\\boldsymbol{\\Sigma}}_1(\\mathbf{x}_{test}-\\hat{\\boldsymbol{\\mu}}_0)}*(1-\\hat{p}) \\\\\n-(\\mathbf{x}_{test}-\\hat{\\boldsymbol{\\mu}}_1)^T\\hat{\\boldsymbol{\\Sigma}}_1(\\mathbf{x}_{test}-\\hat{\\boldsymbol{\\mu}}_1)+\\log(\\hat{p})&\\ge -(\\mathbf{x}_{test}-\\hat{\\boldsymbol{\\mu}}_0)^T\\hat{\\boldsymbol{\\Sigma}}_0(\\mathbf{x}_{test}-\\hat{\\boldsymbol{\\mu}}_0) + \\log(1-\\hat{p}) \\\\\n\\end{align*}\\] This inequality leads to a quadratic decision function: \\[\n\\mathbf{x}_{test}^T(\\hat{\\boldsymbol{\\Sigma}}_1^{-1}-\\hat{\\boldsymbol{\\Sigma}}_0^{-1})\\mathbf{x}_{test}-2(\\hat{\\boldsymbol{\\mu}}_1^T\\hat{\\boldsymbol{\\Sigma}}_1^{-1}-\\hat{\\boldsymbol{\\mu}}_0^T\\hat{\\boldsymbol{\\Sigma}}_0^{-1})\\mathbf{x}_{test}+(\\hat{\\boldsymbol{\\mu}}_0^T\\hat{\\boldsymbol{\\Sigma}}_0^{-1}\\hat{\\boldsymbol{\\mu}}_0-\\hat{\\boldsymbol{\\mu}}_1^T\\hat{\\boldsymbol{\\Sigma}}_1^{-1}\\hat{\\boldsymbol{\\mu}}_1) + \\log(\\frac{1-\\hat{p}}{\\hat{p}}) \\ge 0\n\\] Hence, the decision boundary is a quadratic function when the covariance matrices are not equal for both classes.\n\n\n\n\nWhen the covariance matrices are not equal for both classes"
  },
  {
    "objectID": "pages/Wk06.html",
    "href": "pages/Wk06.html",
    "title": "Supervised Learning - Regression - Ridge/LASSO",
    "section": "",
    "text": "Note\n\n\n\nFeedback/Correction: Click here!.\nPDF Link: Click here!"
  },
  {
    "objectID": "pages/Wk06.html#introduction",
    "href": "pages/Wk06.html#introduction",
    "title": "Supervised Learning - Regression - Ridge/LASSO",
    "section": "Introduction",
    "text": "Introduction\nLinear regression is a widely used technique for modeling the relationship between a dependent variable and one or more independent variables. The maximum likelihood estimator (MLE) is commonly employed to estimate the parameters of a linear regression model. Here, we discuss the goodness of the MLE for linear regression, explore cross-validation techniques to minimize mean squared error (MSE), examine Bayesian modeling as an alternative approach, and finally, delve into ridge and lasso regression as methods to mitigate overfitting."
  },
  {
    "objectID": "pages/Wk04.html",
    "href": "pages/Wk04.html",
    "title": "Unsupervised Learning - Estimation - Recap of MLE + Bayesian estimation, Gaussian Mixture Model - EM algorithm",
    "section": "",
    "text": "Note\n\n\n\nFeedback/Correction: Click here!.\nPDF Link: Click here!"
  },
  {
    "objectID": "pages/Wk04.html#fishers-principle-of-maximum-likelihood",
    "href": "pages/Wk04.html#fishers-principle-of-maximum-likelihood",
    "title": "Unsupervised Learning - Estimation - Recap of MLE + Bayesian estimation, Gaussian Mixture Model - EM algorithm",
    "section": "Fisher’s Principle of Maximum Likelihood",
    "text": "Fisher’s Principle of Maximum Likelihood\nFisher’s principle of maximum likelihood is a statistical method used to estimate parameters of a statistical model by selecting values that maximize the likelihood function. This function quantifies how well the model fits the observed data."
  },
  {
    "objectID": "pages/Wk04.html#likelihood-estimation-for-bernoulli-distributions",
    "href": "pages/Wk04.html#likelihood-estimation-for-bernoulli-distributions",
    "title": "Unsupervised Learning - Estimation - Recap of MLE + Bayesian estimation, Gaussian Mixture Model - EM algorithm",
    "section": "Likelihood Estimation for Bernoulli Distributions",
    "text": "Likelihood Estimation for Bernoulli Distributions\nApplying the likelihood function on the aforementioned dataset, we obtain: \\[\\begin{align*}\n\\mathcal{L}(p;\\{\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n\\}) &= P(\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n;p)\\\\\n&= p(\\mathbf{x}_1;p)p(\\mathbf{x}_2;p)\\ldots p(\\mathbf{x}_n;p) \\\\\n&=\\prod _{i=1} ^n {p^{\\mathbf{x}_i}(1-p)^{1-\\mathbf{x}_i}}\n\\end{align*}\\] \\[\\begin{align*}\n\\therefore \\log(\\mathcal{L}(p;\\{\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n\\})) &=\\underset{p} {\\arg \\max}\\log \\left ( \\prod _{i=1} ^n {p^{\\mathbf{x}_i}(1-p)^{1-\\mathbf{x}_i}} \\right ) \\\\\n\\text{Differentiating wrt $p$, we get}\\\\\n\\therefore \\hat{p}_{\\text{ML}} &= \\frac{1}{n}\\sum _{i=1} ^n \\mathbf{x}_i\n\\end{align*}\\]"
  },
  {
    "objectID": "pages/Wk04.html#likelihood-estimation-for-gaussian-distributions",
    "href": "pages/Wk04.html#likelihood-estimation-for-gaussian-distributions",
    "title": "Unsupervised Learning - Estimation - Recap of MLE + Bayesian estimation, Gaussian Mixture Model - EM algorithm",
    "section": "Likelihood Estimation for Gaussian Distributions",
    "text": "Likelihood Estimation for Gaussian Distributions\nLet \\(\\{\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n\\}\\) be a dataset where \\(\\mathbf{x}_i \\sim \\mathcal{N}(\\boldsymbol{\\mu},\\boldsymbol{\\sigma}^2)\\). We assume that the data points are independent and identically distributed.\n\\[\\begin{align*}\n\\mathcal{L}(\\boldsymbol{\\mu}, \\boldsymbol{\\sigma}^2;\\{\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n\\}) &= f_{\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n}(\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n;\\boldsymbol{\\mu}, \\boldsymbol{\\sigma}^2) \\\\\n&=\\prod _{i=1} ^n  f_{\\mathbf{x}_i}(\\mathbf{x}_i;\\boldsymbol{\\mu}, \\boldsymbol{\\sigma}^2) \\\\\n&=\\prod _{i=1} ^n \\left [ \\frac{1}{\\sqrt{2\\pi}\\boldsymbol{\\sigma}} e^{\\frac{-(\\mathbf{x}_i-\\boldsymbol{\\mu})^2}{2\\boldsymbol{\\sigma}^2}} \\right ] \\\\\n\\therefore \\log(\\mathcal{L}(p;\\{\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n\\})) &= \\sum _{i=1} ^n \\left[ \\log \\left (\\frac{1}{\\sqrt{2\\pi}\\boldsymbol{\\sigma}}  \\right ) - \\frac{(\\mathbf{x}_i-\\boldsymbol{\\mu})^2}{2\\boldsymbol{\\sigma}^2} \\right] \\\\\n\\end{align*}\\] \\[\n\\text{By differentiating with respect to $\\boldsymbol{\\mu}$ and $\\boldsymbol{\\sigma}$, we get}\n\\] \\[\\begin{align*}\n\\hat{\\boldsymbol{\\mu}}_{\\text{ML}} &= \\frac{1}{n}\\sum _{i=1} ^n \\mathbf{x}_i \\\\\n\\hat{\\boldsymbol{\\sigma}^2}_{\\text{ML}} &= \\frac{1}{n}\\sum _{i=1} ^n (\\mathbf{x}_i-\\boldsymbol{\\mu})^T(\\mathbf{x}_i-\\boldsymbol{\\mu})\n\\end{align*}\\]"
  },
  {
    "objectID": "pages/Wk04.html#bayesian-estimation-for-a-bernoulli-distribution",
    "href": "pages/Wk04.html#bayesian-estimation-for-a-bernoulli-distribution",
    "title": "Unsupervised Learning - Estimation - Recap of MLE + Bayesian estimation, Gaussian Mixture Model - EM algorithm",
    "section": "Bayesian Estimation for a Bernoulli Distribution",
    "text": "Bayesian Estimation for a Bernoulli Distribution\nLet \\(\\{x_1, x_2, \\ldots, x_n\\}\\) be a dataset where \\(x_i \\in \\{0,1\\}\\) with parameter \\(\\theta\\). What distribution can be suitable for \\(P(\\theta)\\)?\nA commonly used distribution for priors is the Beta Distribution. \\[\nf(p;\\alpha,\\beta) = \\frac{p^{\\alpha-1}(1-p)^{\\beta-1}}{z} \\hspace{2em} \\forall p \\in [0,1] \\\\\n\\] \\[\n\\text{where $z$ is a normalizing factor}\n\\]\nHence, utilizing the Beta Distribution as the Prior, we obtain, \\[\\begin{align*}\nP(\\theta|\\{x_1, x_2, \\ldots, x_n\\}) &\\propto P(\\theta|\\{x_1, x_2, \\ldots, x_n\\})*P(\\theta) \\\\\nf_{\\theta|\\{x_1, x_2, \\ldots, x_n\\}}(p) &\\propto \\left [ \\prod _{i=1} ^n {p^{x_i}(1-p)^{1-x_i}} \\right ]*\\left [ p^{\\alpha-1}(1-p)^{\\beta-1} \\right ] \\\\\nf_{\\theta|\\{x_1, x_2, \\ldots, x_n\\}}(p) &\\propto p^{\\sum _{i=1} ^n x_i + \\alpha - 1}(1-p)^{\\sum _{i=1} ^n(1-x_i) + \\beta - 1}\n\\end{align*}\\] i.e. we obtain, \\[\n\\text{BETA PRIOR }(\\alpha, \\beta) \\xrightarrow[Bernoulli]{\\{x_1, x_2, \\ldots, x_n\\}} \\text{BETA POSTERIOR }(\\alpha + n_h, \\beta + n_t)\n\\] \\[\n\\therefore \\hat{p_{\\text{ML}}} = \\mathbb{E}[\\text{Posterior}]=\\mathbb{E}[\\text{Beta}(\\alpha +n_h, \\beta + n_t)]= \\frac{\\alpha + n_h}{\\alpha + n_h + \\beta + n_t}\n\\]"
  },
  {
    "objectID": "pages/Wk04.html#convexity-and-jensens-inequality",
    "href": "pages/Wk04.html#convexity-and-jensens-inequality",
    "title": "Unsupervised Learning - Estimation - Recap of MLE + Bayesian estimation, Gaussian Mixture Model - EM algorithm",
    "section": "Convexity and Jensen’s Inequality",
    "text": "Convexity and Jensen’s Inequality\nConvexity is a property of a function or set that implies a unique line segment can be drawn between any two points within the function or set. For a concave function, this property can be expressed as, \\[\nf \\left (\\sum _{k=1} ^K \\lambda_k a_k \\right ) \\ge \\sum _{k=1} ^K \\lambda_k f(a_k)\n\\] where \\[\n\\sum _{k=1} ^K \\lambda _k = 1\n\\] \\[\na_k \\text{ are points of the function}\n\\] This is also known as Jensen’s Inequality."
  },
  {
    "objectID": "pages/Wk01.html",
    "href": "pages/Wk01.html",
    "title": "Introduction; Unsupervised Learning - Representation learning - PCA",
    "section": "",
    "text": "Note\n\n\n\nFeedback/Correction: Click here!.\nPDF Link: Click here!"
  },
  {
    "objectID": "pages/Wk01.html#broad-paradigms-of-machine-learning",
    "href": "pages/Wk01.html#broad-paradigms-of-machine-learning",
    "title": "Introduction; Unsupervised Learning - Representation learning - PCA",
    "section": "Broad Paradigms of Machine Learning",
    "text": "Broad Paradigms of Machine Learning\n\nSupervised Learning:Supervised Machine Learning is a type of machine learning where the algorithm is trained on a labeled dataset, meaning that the data includes both inputs and their corresponding outputs. The goal of supervised learning is to build a model that can accurately predict the output for new, unseen input data. Few examples:\n\n\nLinear regression for predicting a continuous output\nLogistic regression for binary classification problems\nDecision trees for non-linear classification and regression problems\nSupport Vector Machines for binary and multi-class classification problems\nNeural Networks for complex non-linear problems in various domains such as computer vision, natural language processing, and speech recognition\n\n\nUnsupervised Learning: Unsupervised Machine Learning is a type of machine learning where the algorithm is trained on an unlabeled dataset, meaning that only the inputs are provided and no corresponding outputs. The goal of unsupervised learning is to uncover patterns or relationships within the data without any prior knowledge or guidance. Few examples:\n\n\nClustering algorithms such as K-means, hierarchical clustering, and density-based clustering, used to group similar data points together into clusters\nDimensionality reduction techniques such as Principal Component Analysis (PCA), used to reduce the number of features in a dataset while preserving the maximum amount of information\nAnomaly detection algorithms used to identify unusual data points that deviate from the normal patterns in the data\n\n\nSequential learning: Sequential Machine Learning (also known as time-series prediction) is a type of machine learning that is focused on making predictions based on sequences of data. It involves training the model on a sequence of inputs, such that the predictions for each time step depend on the previous time steps. Few examples:\n\n\nTime series forecasting, used to predict future values based on past trends and patterns in data such as stock prices, weather patterns, and energy consumption\nSpeech recognition, used to transcribe speech into text by recognizing patterns in audio signals\nNatural language processing, used to analyze and make predictions about sequences of text data"
  },
  {
    "objectID": "pages/Wk01.html#potential-algorithm",
    "href": "pages/Wk01.html#potential-algorithm",
    "title": "Introduction; Unsupervised Learning - Representation learning - PCA",
    "section": "Potential Algorithm",
    "text": "Potential Algorithm\nBased on the above concepts, we can outline the following algorithm for representation learning:\nGiven a dataset \\(\\{\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n\\}\\) where \\(\\mathbf{x}_i \\in \\mathbb{R}^{d}\\),\n\nCenter the dataset: \\[\n\\mathbf{\\mu} = \\frac{1}{n} \\sum _{i=1} ^{n} \\mathbf{x}_i\n\\] \\[\n\\mathbf{x}_i = \\mathbf{x}_i - \\mathbf{\\mu}  \\hspace{2em} \\forall i\n\\]\nFind the best representation \\(\\mathbf{w} \\in \\mathbb{R}^d\\) with \\(||\\mathbf{w}|| = 1\\).\nUpdate the dataset with the representation: \\[\n\\mathbf{x}_i = \\mathbf{x}_i - (\\mathbf{x}_i^T\\mathbf{w})\\mathbf{w}  \\hspace{1em} \\forall i\n\\]\nRepeat steps 2 and 3 until the residues become zero, resulting in \\(\\mathbf{w}_2, \\mathbf{w}_3, \\ldots, \\mathbf{w}_d\\).\n\nThe question arises: Is this the most effective approach, and how many \\(\\mathbf{w}\\) do we need to achieve optimal compression?"
  },
  {
    "objectID": "pages/Wk01.html#approximate-representation",
    "href": "pages/Wk01.html#approximate-representation",
    "title": "Introduction; Unsupervised Learning - Representation learning - PCA",
    "section": "Approximate Representation",
    "text": "Approximate Representation\nThe question arises: If the data can be approximately represented by a lower-dimensional subspace, would it suffice to use only those \\(k\\) projections? Additionally, how much variance should be covered?\nLet us consider a centered dataset \\(\\{\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n\\}\\) where \\(\\mathbf{x}_i \\in \\mathbb{R}^{d}\\). Let \\(\\mathbf{C}\\) represent its covariance matrix, and \\(\\{\\lambda_1, \\lambda_2, \\ldots, \\lambda_d \\}\\) be the corresponding eigenvalues, which are non-negative due to the positive semi-definiteness of the covariance matrix. These eigenvalues are arranged in descending order, with \\(\\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_d \\}\\) as their corresponding eigenvectors of unit length.\nThe eigen equation for the covariance matrix can be expressed as follows: \\[\\begin{align*}\n    \\mathbf{C}\\mathbf{w} &= \\lambda \\mathbf{w} \\\\\n    \\mathbf{w}^T\\mathbf{C}\\mathbf{w} &= \\mathbf{w}^T\\lambda \\mathbf{w}\\\\\n    \\therefore \\lambda &= \\mathbf{w}^T\\mathbf{C}\\mathbf{w} \\hspace{2em} \\{\\mathbf{w}^T\\mathbf{w} = 1\\} \\\\\n    \\lambda &= \\frac{1}{n} \\sum _{i=1} ^{n} (\\mathbf{x}_i^T\\mathbf{w})^2 \\\\\n\\end{align*}\\]\nHence, the mean of the dataset being zero, \\(\\lambda\\) represents the variance captured by the eigenvector \\(\\mathbf{w}\\).\nA commonly accepted heuristic suggests that PCA should capture at least 95% of the variance. If the first \\(k\\) eigenvectors capture the desired variance, it can be stated as: \\[\n\\frac{\\displaystyle \\sum _{j=1} ^{k} \\lambda_j}{\\displaystyle \\sum _{i=1} ^{d} \\lambda_i} \\ge 0.95\n\\]\nThus, the higher the variance captured, the lower the error incurred."
  },
  {
    "objectID": "pages/Wk01.html#p.c.a.-algorithm",
    "href": "pages/Wk01.html#p.c.a.-algorithm",
    "title": "Introduction; Unsupervised Learning - Representation learning - PCA",
    "section": "P.C.A. Algorithm",
    "text": "P.C.A. Algorithm\nThe Principal Component Analysis algorithm can be summarized as follows for a centered dataset \\(\\{\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n\\}\\) where \\(\\mathbf{x}_i \\in \\mathbb{R}^{d}\\), and \\(\\mathbf{C}\\) represents its covariance matrix:\n\nStep 1: Find the eigenvalues and eigenvectors of \\(\\mathbf{C}\\). Let \\(\\{\\lambda_1, \\lambda_2, \\ldots, \\lambda_d \\}\\) be the eigenvalues arranged in descending order, and \\(\\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_d \\}\\) be their corresponding eigenvectors of unit length.\nStep 2: Calculate \\(k\\), the number of top eigenvalues and eigenvectors required, based on the desired variance to be covered.\nStep 3: Project the data onto the eigenvectors and obtain the desired representation as a linear combination of these projections.\n\n\n\n\nThe dataset depicted in the diagram has two principal components: the green vector represents the first PC, whereas the red vector corresponds to the second PC.\n\n\nIn essence, PCA is a dimensionality reduction technique that identifies feature combinations that are de-correlated (independent of each other)."
  },
  {
    "objectID": "notes/Not03.html",
    "href": "notes/Not03.html",
    "title": "Week 3: K-means",
    "section": "",
    "text": "Colab Link: Click here!"
  },
  {
    "objectID": "notes/Not03.html#problem-definition",
    "href": "notes/Not03.html#problem-definition",
    "title": "Week 3: K-means",
    "section": "Problem Definition",
    "text": "Problem Definition\nIn this context, the objective becomes the following:\n\n\\underset{z \\in S}{\\text{min }} \\sum_{i=1}^{n}||x_i - μ_{z_i}||^2_2\n\nWhere: - x_i denotes the i’th datapoint - z_i denotes the cluster indicator of x_i - μ_{z_i} denotes the mean of the cluster with indicator z_i - S denotes the set of all possible cluster assignments. Note that S is finite\n\ndef obj(X, cluster_centers):\n  return sum([np.min([np.linalg.norm(x_i - cluster_center)**2 for cluster_center in cluster_centers]) for x_i in X])"
  },
  {
    "objectID": "notes/Not03.html#algorithm-strategy",
    "href": "notes/Not03.html#algorithm-strategy",
    "title": "Week 3: K-means",
    "section": "Algorithm Strategy",
    "text": "Algorithm Strategy\n\nInitialization Step : Assign random datapoints from the dataset as the cluster centers\nCluster Assignment Step : For every datapoint x_i, assign a cluster indicator z_i = \\underset{j \\in [1, 2, ..., n]}{\\text{min }} ||x_i - μ_j||^2_2\nRecompute cluster centers : For every cluster indicator j \\in [1, 2, ..., n] recompute μ_j = \\frac{\\sum_{i=1}^{n}x_i \\cdot \\mathbf{1}(z_i=j)}{\\sum_{i=1}^{n} \\mathbf{1}(z_i=j)}\nRepeat steps 2 and 3 until convergence.\n\nConvergence in accordance to the objective is established, since the following can be shown: - The set of all possible cluster assignments S is finite. - The objective function value strictly decreases after every iteration of Lloyd’s.\nThe initialization for K-Means can be done in smarter ways than a random initialization - which improve the chance of lloyd’s converging to a good cluster assignment; with a lesser number of iterations.\nIt is important to note that the final assignment need not necessarily be the best answer (global optima) to the objective function, but it is good enough in practice."
  },
  {
    "objectID": "notes/Not03.html#initialization-step",
    "href": "notes/Not03.html#initialization-step",
    "title": "Week 3: K-means",
    "section": "Initialization Step",
    "text": "Initialization Step\nn points from the dataset are randomly chosen as cluster centers, where n is the number of clusters - a hyperparameter\n\nn = 3\n# cluster_centers = X[np.random.choice(len(X), 3)]\ncluster_centers = X[[70, 85, 80]]"
  },
  {
    "objectID": "notes/Not03.html#cluster-assignment-step",
    "href": "notes/Not03.html#cluster-assignment-step",
    "title": "Week 3: K-means",
    "section": "Cluster Assignment Step",
    "text": "Cluster Assignment Step\nFor every datapoint, the cluster indicator whose center is closest to the datapoint is assigned as its cluster.\n\nfrom scipy.spatial import Voronoi, voronoi_plot_2d\n\ndef cluster_assignment(X, cluster_centers):\n  z = np.zeros(X.shape[0])\n  for i in range(X.shape[0]):\n    z[i] = np.argmin([np.linalg.norm(X[i] - cluster_center) for cluster_center in cluster_centers])\n  return z\n\nz = cluster_assignment(X, cluster_centers)\n\nfig, (ax) = plt.subplots(1, 1)\nfig.set_size_inches(5, 5)\n\nax.scatter(X[:, 0], X[:, 1], c=z, s=10);\nax.scatter(cluster_centers[:, 0], cluster_centers[:, 1], marker = 'x', s = 100, color = 'red', linewidth=1)\n\nvor = Voronoi(cluster_centers)\nvoronoi_plot_2d(vor, ax=ax, show_points=False, show_vertices=False);\n\nax.axis('equal');\n\n\n\n\nFor every cluster, there is a corresponding interesction of half-spaces - called Voronoi regions. The K-Means algorithm, equivalently, is trying to find the most optimal Voronoi partition of the space, that minimizes the objective function."
  },
  {
    "objectID": "notes/Not03.html#recompute-meanscluster-centers",
    "href": "notes/Not03.html#recompute-meanscluster-centers",
    "title": "Week 3: K-means",
    "section": "Recompute Means/Cluster Centers",
    "text": "Recompute Means/Cluster Centers\nFor every cluster, the cluster center is updated to the mean of the points in the cluster.\n\ndef recompute_clusters(X, z):\n  cluster_centers = np.array([np.mean(X[z == i], axis = 0) for i in range(n)])\n  return cluster_centers"
  },
  {
    "objectID": "notes/Not03.html#generate-a-complex-dataset-with-6-clusters",
    "href": "notes/Not03.html#generate-a-complex-dataset-with-6-clusters",
    "title": "Week 3: K-means",
    "section": "Generate a complex dataset with 6 clusters",
    "text": "Generate a complex dataset with 6 clusters\nWe make use of the convienient make_blobs data generator from the scikit-learn ibrary.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.datasets import make_blobs\n\ncen = [(-3, 3), (-2, 1), (0, 0), (1, 2), (0, -2), (3, -1)]\nX, ideal_z = make_blobs(n_samples=1000, centers=cen, n_features=2, cluster_std=0.3, random_state=13, center_box=(-3, 3))\n\nfig, ax = plt.subplots()\nax.scatter(X[:, 0], X[:, 1], s=10, c=ideal_z)\nvor = Voronoi(cen)\nvoronoi_plot_2d(vor, ax=ax, show_points=False, show_vertices=False);\nfig.set_size_inches(5, 4.5)\nax.axis([-5, 5, -4, 5])\n\n(-5.0, 5.0, -4.0, 5.0)\n\n\n\n\n\n\n# the make_blobs generator returns the optimal/good cluster assignment along with the dataset\n# this function checks whether the result from lloyd's is equivalent to the optimal assignment\n\ndef ideal_check(ideal, obtained):\n  mapping = dict([(i, -1) for i in range(n)])\n\n  for i in range(len(ideal)):\n    if mapping[ideal[i]] == -1:\n      mapping[ideal[i]] = obtained[i]\n    elif mapping[ideal[i]] != obtained[i]:\n      return False\n\n  return True"
  },
  {
    "objectID": "notes/Not03.html#lloyds-algorithm-definition",
    "href": "notes/Not03.html#lloyds-algorithm-definition",
    "title": "Week 3: K-means",
    "section": "Lloyd’s Algorithm Definition",
    "text": "Lloyd’s Algorithm Definition\n\nfrom IPython.display import HTML\nfrom matplotlib import animation\n\ndef lloyds(cluster_centers, X, z, artists = [], animate=True, fig=None, ax=None, ax1=None, n_iter=0, n = len(cen)):\n  loss = []\n  if fig is None and animate:\n    fig, (ax, ax1) = plt.subplots(1, 2, figsize=(10, 4.15))\n    title = ax.set_title('')\n    artists = []\n\n  if animate:\n      frame = []\n      frame.append(ax.scatter(X[:, 0], X[:, 1], c=z, s=10))\n      frame.append(ax.scatter(cluster_centers[:, 0], cluster_centers[:, 1], marker = 'x', s = 100, color = 'red', linewidth=1))\n      frame.append(ax.text(0.5, 1.05, f'Iteration {n_iter} | Cluster Assignment', transform=ax.transAxes, ha=\"center\"))\n\n      vor = Voronoi(cluster_centers)\n      d = voronoi_plot_2d(vor, ax=ax, show_points=False, show_vertices=False);\n      frame += list(d.axes[0].lines[-1:] + d.axes[0].collections[-2:])\n      ax.axis([-5, 5, -4, 5])\n\n      loss.append(obj(X, cluster_centers))\n      m = 1\n      frame.append(ax1.scatter([0], [loss[0]], color='red', marker='x', s=30))\n      frame.append(ax1.text(0.5, 1.05, 'Objective Function', transform=ax1.transAxes, ha=\"center\"))\n      frame.append(ax1.text(0.5, -0.1, 'Iterations', transform=ax1.transAxes, ha=\"center\"))\n\n      artists.append(frame)\n\n  converged = False\n  while not converged:\n\n    # cluster_centers = recompute_clusters(X, z)\n    for i in range(n):\n      cluster_points = X[z==i]\n      if len(cluster_points)&gt;0:\n        cluster_centers[i] = np.mean(cluster_points, axis=0)\n\n    n_iter += 1\n\n    # Modified cluster assignment step\n    converged = True\n\n    if animate:\n      frame = []\n      frame.append(ax.scatter(X[:, 0], X[:, 1], c=z, s=10))\n      frame.append(ax.scatter(cluster_centers[:, 0], cluster_centers[:, 1], marker = 'x', s = 100, color = 'red', linewidth=1))\n      frame.append(ax.text(0.5, 1.05, f'Iteration {n_iter} | Cluster Recomputation', transform=ax.transAxes, ha=\"center\"))\n\n      vor = Voronoi(cluster_centers)\n      d = voronoi_plot_2d(vor, ax=ax, show_points=False, show_vertices=False);\n      frame += list(d.axes[0].lines[-1:] + d.axes[0].collections[-2:])\n      ax.axis([-5, 5, -4, 5])\n\n      for i in range(0, len(loss)-1, 2):\n        frame.append(list(ax1.plot([i/2, (i+2)/2], [loss[i], loss[i+1]], color='red', marker='x', markersize=5))[0])\n\n      loss.append(obj(X, cluster_centers))\n      frame.append(list(ax1.plot([(len(loss)-2)/2, (len(loss)-1)/2], [loss[-2], loss[-1]], color='red', linestyle=':'))[0])\n      # lines = list(ax1.plot(np.arange(len(loss))/2, loss, color='red', marker='xo', markersize=6))\n      # frame += lines\n      frame.append(ax1.text(0.5, 1.05, 'Objective Function', transform=ax1.transAxes, ha=\"center\"))\n      frame.append(ax1.text(0.5, -0.1, 'Iterations', transform=ax1.transAxes, ha=\"center\"))\n\n      artists.append(frame)\n\n    for i in range(len(X)):\n      z_i = np.argmin([np.linalg.norm(X[i] - cluster_center) for cluster_center in cluster_centers])\n\n      if z_i != z[i]:\n        z[i] = z_i\n        converged = False\n\n    if animate and not converged:\n      frame = []\n      frame.append(ax.scatter(X[:, 0], X[:, 1], c=z, s=10))\n      frame.append(ax.scatter(cluster_centers[:, 0], cluster_centers[:, 1], marker = 'x', s = 100, color = 'red', linewidth=1))\n      frame.append(ax.text(0.5, 1.05, f'Iteration {n_iter} | Cluster Re-assignment', transform=ax.transAxes, ha=\"center\"))\n\n      vor = Voronoi(cluster_centers)\n      d = voronoi_plot_2d(vor, ax=ax, show_points=False, show_vertices=False);\n      frame += list(d.axes[0].lines[-1:] + d.axes[0].collections[-2:])\n      ax.axis([-5, 5, -4, 5])\n\n      loss.append(obj(X, cluster_centers))\n      m = 1\n      for i in range(0, len(loss)-1, 2):\n        frame.append(list(ax1.plot([i/2, (i+2)/2], [loss[i], loss[i+1]], color='red', marker='x', markersize=5))[0])\n\n      frame.append(ax1.text(0.5, 1.05, 'Objective Function', transform=ax1.transAxes, ha=\"center\"))\n      frame.append(ax1.text(0.5, -0.1, 'Iterations', transform=ax1.transAxes, ha=\"center\"))\n\n      artists.append(frame)\n\n  if animate:\n    plt.close()\n    return fig, (ax, ax1), cluster_centers, artists\n  else:\n    return cluster_centers, n_iter"
  },
  {
    "objectID": "notes/Not03.html#random-initialization",
    "href": "notes/Not03.html#random-initialization",
    "title": "Week 3: K-means",
    "section": "Random Initialization",
    "text": "Random Initialization\nWe now run Lloyd’s algorithm on the dataset with random initialization. We use the ideal_check function to see whether the obtained cluster from lloyd’s is optimal; else we re-run it again.\nAn animation using Matplotlib’s ArtistAnimation is shown to illustrate the working of this strategy.\n\ncluster_centers = X[np.random.choice(len(X), n)]\nz = cluster_assignment(X, cluster_centers)\n\nfig, ax, final_clusters, artists = lloyds(cluster_centers, X, z)\n\nwhile not ideal_check(cluster_assignment(X, final_clusters), ideal_z):\n\n  cluster_centers = X[np.random.choice(len(X), n)]\n  z = cluster_assignment(X, cluster_centers)\n  fig, ax, final_clusters, artists = lloyds(cluster_centers, X, z)\n\nanim = animation.ArtistAnimation(fig, artists, interval=500, repeat=False, blit=False);\nHTML(anim.to_jshtml())\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\nSome initializations do not give a good clustering\n\ncluster_centers = X[np.random.choice(len(X), n)]\nz = cluster_assignment(X, cluster_centers)\nfig, ax, final_clusters, artists = lloyds(cluster_centers, X, z)\n\nwhile ideal_check(cluster_assignment(X, final_clusters), ideal_z):\n  cluster_centers = X[np.random.choice(len(X), n)]\n  z = cluster_assignment(X, cluster_centers)\n  fig, ax, final_clusters, artists = lloyds(cluster_centers, X, z)\n\nanim = animation.ArtistAnimation(fig, artists, interval=500, repeat=False, blit=False);\nHTML(anim.to_jshtml())\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "notes/Not03.html#experiments",
    "href": "notes/Not03.html#experiments",
    "title": "Week 3: K-means",
    "section": "Experiments",
    "text": "Experiments\n\nRandom Initialization\n\nfrom timeit import default_timer as timer\n\nnoof_trials = 1000\n\nrandom = {'ideals' : 0,\n          'SSE' : [],\n          'iters' : [],\n          'time' : []}\n\nexp_start = timer()\nfor _ in range(noof_trials):\n\n  start = timer()\n  cluster_centers = X[np.random.choice(len(X), n)]\n  final_clusters, n_iter = lloyds(cluster_centers, X, cluster_assignment(X, cluster_centers), animate=False)\n\n  random['time'].append(timer()-start)\n  random['ideals'] += int(ideal_check(ideal_z, cluster_assignment(X, final_clusters)))\n  random['SSE'].append(obj(X, final_clusters))\n  random['iters'].append(n_iter)\n\nprint(f'Experiment done in {timer()-exp_start:.2f} seconds')\n\nExperiment done in 670.24 seconds\n\n\n\n\nK-Means++ Initialization\n\nkmplusplus = {'ideals' : 0,\n              'SSE' : [],\n              'iters' : [],\n              'time' : []}\n\nexp_start = timer()\nfor _ in range(noof_trials):\n\n  start = timer()\n  cluster_centers = plusplus(animate=False)\n  final_clusters, n_iter = lloyds(cluster_centers, X, cluster_assignment(X, cluster_centers), animate=False)\n\n  kmplusplus['time'].append(timer()-start)\n  kmplusplus['ideals'] += int(ideal_check(ideal_z, cluster_assignment(X, final_clusters)))\n  kmplusplus['SSE'].append(obj(X, final_clusters))\n  kmplusplus['iters'].append(n_iter)\n\nprint(f'Experiment done in {timer()-exp_start:.2f} seconds')\n\nExperiment done in 539.66 seconds"
  },
  {
    "objectID": "notes/Not03.html#results",
    "href": "notes/Not03.html#results",
    "title": "Week 3: K-means",
    "section": "Results",
    "text": "Results\n\nSSE Comparison\n\nplt.style.use('seaborn-v0_8')\n\nfig = plt.figure(figsize=(12, 5))\nfig.suptitle('Initialization Comparison | SSE')\n\nax1 = plt.subplot(121)\nax1.hist(random['SSE'], bins=30)\nax1.set_title('Random')\nax1.set_ylabel('Frequency')\nax1.set_xlabel('SSE')\n\nax2 = plt.subplot(122, sharex=ax1, sharey=ax1)\nax2.hist(kmplusplus['SSE'], bins=30)\nax2.set_title('K-Means++')\nax2.set_ylabel('Frequency')\nax2.set_xlabel('SSE')\n\nplt.show()\n\n\n\n\n\nplt.figure(figsize=(12, 2))\nplt.title('Fraction of Best Convergences')\nplt.barh(['Random', 'K-Means++'], [random['ideals']/noof_trials, kmplusplus['ideals']/noof_trials], height=0.4);\nplt.xlim([0, 1]);\n\n\n\n\nThe SSE for the optimal cluster assignment for this particular dataset is around 175. We observe from the above charts that K-Means++ does indeed have a higher chance (0.8) of converging to this as compared to Vanilla K-Means (0.5).\n\n\nNumber of Iterations Comparison\n\nfig = plt.figure(figsize=(12, 5))\nfig.suptitle('Initialization Comparison | Number of Iterations')\n\nax1 = plt.subplot(121)\nax1.hist(random['iters'], bins=max(random['iters'])-min(random['iters']))\nax1.set_title('Random')\nax1.set_ylabel('Frequency')\nax1.set_xlabel('Number of Iterations')\n\nax2 = plt.subplot(122, sharex=ax1, sharey=ax1)\nax2.hist(kmplusplus['iters'], bins=max(kmplusplus['iters'])-min(kmplusplus['iters']))\nax2.set_title('K-Means++')\nax2.set_ylabel('Frequency')\nax2.set_xlabel('Number of Iterations')\n\nplt.show()\n\n\n\n\nFor Vanilla K-Means, we observe that the number of iterations has quite a bit of variation, with an average of 8.75 iterations to convergence.\nK-Means++ has a relatively smaller spread, with an average of 4.2 iterations to convergence (post intialization).\n\n\nTime Taken Comparison\n\nfig = plt.figure(figsize=(12, 5))\nfig.suptitle('Initialization Comparison | Time taken to converge')\n\nax1 = plt.subplot(121)\nax1.hist(random['time'], bins=20)\nax1.set_title('Random')\nax1.set_ylabel('Frequency')\nax1.set_xlabel('Time taken')\n\nax2 = plt.subplot(122, sharex=ax1, sharey=ax1)\nax2.hist(kmplusplus['time'], bins=20)\nax2.set_title('K-Means++')\nax2.set_ylabel('Frequency')\nax2.set_xlabel('Time taken (s)')\n\nplt.show()\n\n\n\n\n\nnp.array(kmplusplus['time']).mean()\n\n0.4248704458820016\n\n\nSimilar to the results for number of iterations, the time taken till convergence also has a wide spread for Random Initialization, with an average of 0.55s. K-Means++ has an average of 0.42s."
  },
  {
    "objectID": "notes/Not03.html#k-means",
    "href": "notes/Not03.html#k-means",
    "title": "Week 3: K-means",
    "section": "K-Means",
    "text": "K-Means\nK-Means with k = 3 for X = [-15, -10, 0, 5, 15, 20, 25] with mean clusters μ = [-15, 0, 5]\n\nplt.figure(figsize=(15, 7.5))\n# From the problem\nx = np.expand_dims(np.array([-15, -10, 0, 5, 15, 20, 25]), axis=1)\nclusters = np.expand_dims(np.array([-15, 0, 5]), axis=1)\nplt.subplot(2, 3, 1)\nplt.scatter(x[:,0], np.zeros(x.shape[0]));\nplt.scatter(clusters[:, 0], np.zeros(3), marker = 'x', s = 100, color = 'red', linewidth=1);\nplt.title('From the problem')\n\n# Initial Cluster Assignment\nz = cluster_assignment(x, clusters)\nplt.subplot(2, 3, 2)\nplt.scatter(x[:,0], np.zeros(x.shape[0]), c=z);\nplt.scatter(clusters[:, 0], np.zeros(3), marker = 'x', s = 100, color = 'red', linewidth=1);\nplt.title('Initial Cluster Assignment')\n\n# Recompute Cluster Centers\nclusters = recompute_clusters(x, z)\nplt.subplot(2, 3, 3)\nplt.scatter(x[:,0], np.zeros(x.shape[0]), c=z);\nplt.scatter(clusters[:, 0], np.zeros(3), marker = 'x', s = 100, color = 'red', linewidth=1);\nplt.title('Recompute Cluster Centers')\n\n# Next Cluster Assignment\nz = cluster_assignment(x, clusters)\nplt.subplot(2, 3, 4)\nplt.scatter(x[:,0], np.zeros(x.shape[0]), c=z);\nplt.scatter(clusters[:, 0], np.zeros(3), marker = 'x', s = 100, color = 'red', linewidth=1);\nplt.title('Next Cluster Assignment')\n\n# Again Recompute Cluster Centers\nclusters = recompute_clusters(x, z)\nplt.subplot(2, 3, 5)\nplt.scatter(x[:,0], np.zeros(x.shape[0]), c=z);\nplt.scatter(clusters[:, 0], np.zeros(3), marker = 'x', s = 100, color = 'red', linewidth=1);\nplt.title('Again Recompute Cluster Centers')\n\n# Cluster Assignment - No Change\n# Algorithm has converged\nz = cluster_assignment(x, clusters)\nplt.subplot(2, 3, 6)\nplt.scatter(x[:,0], np.zeros(x.shape[0]), c=z);\nplt.scatter(clusters[:, 0], np.zeros(3), marker = 'x', s = 100, color = 'red', linewidth=1);\nplt.title('Algorithm has converged');"
  },
  {
    "objectID": "notes/Not03.html#k-means-1",
    "href": "notes/Not03.html#k-means-1",
    "title": "Week 3: K-means",
    "section": "K-Means++",
    "text": "K-Means++\nFor the dataset below, k-means++ algorithm is run with k=2. Find the probability that \\mathbf{x}_2, \\mathbf{x}_1 are chosen as the initial clusters, in that order.\n\\left\\{\\mathbf{x_{1}} \\ =\\ \\begin{bmatrix}\n0\\\\\n2\n\\end{bmatrix} ,\\mathbf{\\ x_{2}} \\ =\\ \\begin{bmatrix}\n2\\\\\n0\n\\end{bmatrix} ,\\ \\mathbf{x_{3}} \\ =\\ \\begin{bmatrix}\n0\\\\\n0\n\\end{bmatrix} ,\\ \\mathbf{x_{4}} \\ =\\ \\begin{bmatrix}\n0\\\\\n-2\n\\end{bmatrix} ,\\ \\mathbf{x_{5}} \\ =\\ \\begin{bmatrix}\n-2\\\\\n0\n\\end{bmatrix}\\right\\}\n\ndataset = np.array([[0, 2], [2, 0], [0, 0], [0, -2], [-2, 0]])\nprobabilities = np.array([1/len(dataset) for _ in range(len(dataset))])\nprint('Initial Probablities:', probabilities)\nclusters = []\nanswer = 1\n\n# First we select x2 = [2,0]\nclusters.append(dataset[1])\nanswer *= probabilities[1]\n\n# Rescore based on selected clusters\nscores = np.array([min([np.linalg.norm(datapoint-cluster)**2 for cluster in clusters]) for datapoint in dataset])\n\n# Normalize scores to probability\nprobabilities = scores/scores.sum()\nprint('Probabilities after selecting x2: ', [round(i, 3) for i in probabilities])\n\n# Now we select x1 = [0,2]\nclusters.append(dataset[0])\nanswer *= probabilities[0]\n\nprint('Probability of selecting [x2 x1]:', round(answer, 3))\n\nInitial Probablities: [0.2 0.2 0.2 0.2 0.2]\nProbabilities after selecting x2:  [0.222, 0.0, 0.111, 0.222, 0.444]\nProbability of selecting [x2 x1]: 0.044"
  },
  {
    "objectID": "notes/Not01.html",
    "href": "notes/Not01.html",
    "title": "Week 1: Standard PCA",
    "section": "",
    "text": "Colab Link: Click here!"
  },
  {
    "objectID": "notes/Not01.html#steps-involved-in-pca",
    "href": "notes/Not01.html#steps-involved-in-pca",
    "title": "Week 1: Standard PCA",
    "section": "Steps involved in PCA",
    "text": "Steps involved in PCA\n\nStep 1: Center the dataset\nStep 2: Calculate the covariance matrix of the centered data\nStep 3: Compute the eigenvectors and eigenvalues\nStep 4: Sort the eigenvalues in descending order and choose the top k eigenvectors corresponding to the highest eigenvalues\nStep 5: Transform the original data by multiplying it with the selected eigenvectors(PCs) to obtain a lower-dimensional representation.\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "notes/Not01.html#observe-the-dataset",
    "href": "notes/Not01.html#observe-the-dataset",
    "title": "Week 1: Standard PCA",
    "section": "Observe the dataset",
    "text": "Observe the dataset\nLet’s take a dataset \\displaystyle \\mathbf{X} of shape (d,n) where\n\nd: no. of features\nn: no. of datapoints\n\n\nX = np.array([(4,1),(5,4),(6,3),(7,4),(2,-1),(-1,-2),(0,-3),(-1,-4)]).T\n\n\nplt.scatter(X[0,:],X[1,:])\nplt.axhline(0,color='k')\nplt.axvline(0,color='k')\n\nx_mean = X.mean(axis=1)\n\nplt.scatter(x_mean[0],x_mean[1],color='r')\nplt.grid()\nplt.show()"
  },
  {
    "objectID": "notes/Not01.html#center-the-dataset",
    "href": "notes/Not01.html#center-the-dataset",
    "title": "Week 1: Standard PCA",
    "section": "Center the dataset",
    "text": "Center the dataset\n\ndef center(X):\n    return X - X.mean(axis = 1).reshape(2,1)\n\nd, n = X.shape\nX_centered = center(X)\n\n\nX_centered\n\narray([[ 1.25,  2.25,  3.25,  4.25, -0.75, -3.75, -2.75, -3.75],\n       [ 0.75,  3.75,  2.75,  3.75, -1.25, -2.25, -3.25, -4.25]])\n\n\n\nimport matplotlib.pyplot as plt\n\nplt.scatter(X_centered[0,:],X_centered[1,:])\nplt.axhline(0,color='k')\nplt.axvline(0,color='k')\n\nc_mean = X_centered.mean(axis=1)\n\nplt.scatter(c_mean[0],c_mean[1],color='r')\nplt.grid()\nplt.show()\n\n\n\n\n\nX_centered.mean(axis=1)\n\n\n#Compare the two graphs\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.scatter(X[0,:],X[1,:])\nplt.axhline(0,color='k')\nplt.axvline(0,color='k')\n\nx_mean = X.mean(axis=1)\n\nplt.scatter(x_mean[0],x_mean[1],color='r')\nplt.grid()\nplt.title(\"Before Centering\")\n\n\n\nplt.subplot(1, 2, 2)\nplt.scatter(X_centered[0,:],X_centered[1,:])\nplt.axhline(0,color='k')\nplt.axvline(0,color='k')\n\nc_mean = X_centered.mean(axis=1)\n\nplt.scatter(c_mean[0],c_mean[1],color='r')\nplt.grid()\nplt.title(\"After Centering\")\n\nplt.show()"
  },
  {
    "objectID": "notes/Not01.html#find-the-covariance-matrix",
    "href": "notes/Not01.html#find-the-covariance-matrix",
    "title": "Week 1: Standard PCA",
    "section": "Find the covariance matrix",
    "text": "Find the covariance matrix\nThe covariance matrix is given by \\mathbf{C} \\ =\\ \\frac{1}{n}\\sum \\limits_{i\\ =\\ 1}^{n} \\mathbf {x}_{i}\\mathbf {x}_{i}^{T} \\ =\\ \\frac{1}{n}\\mathbf{XX}^{T}\n\ndef covariance(X):\n    return X @ X.T / X.shape[1]\n\nC = covariance(X_centered)\nd = C.shape[0]\nprint(C)\n\n[[8.9375 8.5625]\n [8.5625 8.9375]]"
  },
  {
    "objectID": "notes/Not01.html#compute-the-principal-components",
    "href": "notes/Not01.html#compute-the-principal-components",
    "title": "Week 1: Standard PCA",
    "section": "Compute the principal components",
    "text": "Compute the principal components\nThe k^{th} principal component is given by the eigenvector corresponding to the k^{th} largest eigenvalue\n\ndef compute_pc(C):\n    d = C.shape[0]\n    eigval, eigvec = np.linalg.eigh(C)\n    w_1, w_2 = eigvec[:, -1], eigvec[:, -2]\n    return w_1, w_2\n\nw_1, w_2 = compute_pc(C)\n\nw_1 = w_1.reshape(w_1.shape[0],1)\nw_2 = w_2.reshape(w_2.shape[0],1)\n\nprint(w_1)\nprint(w_2)\n\n[[0.70710678]\n [0.70710678]]\n[[-0.70710678]\n [ 0.70710678]]"
  },
  {
    "objectID": "notes/Not01.html#reconstruction-using-the-two-pcs",
    "href": "notes/Not01.html#reconstruction-using-the-two-pcs",
    "title": "Week 1: Standard PCA",
    "section": "Reconstruction using the two PCs",
    "text": "Reconstruction using the two PCs\nThe scalar projection of the dataset on k^{th} PC is given by \\mathbf{X}_{\\text{centered}}^{T} \\ .\\ \\mathbf{w_{k}}\nThe vector projection of the dataset on k^{th} PC is given by \\mathbf{w_{k} .(\\mathbf{X}_{\\text{centered}}^{T} \\ .\\ \\mathbf{w_{k}})^{T}}\n\n#Since the points are 2-dimensional, by combining the projection on the two PCs, we get back the centered dataset\nw_1 @ (X_centered.T @ w_1).reshape(1,n) + w_2 @ (X_centered.T @ w_2).reshape(1,n)\n\narray([[ 1.25,  2.25,  3.25,  4.25, -0.75, -3.75, -2.75, -3.75],\n       [ 0.75,  3.75,  2.75,  3.75, -1.25, -2.25, -3.25, -4.25]])\n\n\nLet us see the reconstruction error for a point along the first principal component\n\n#The reconstruction error by the first PC is given by\nX_1 = np.array((1.25,0.75))\np_1 = X_centered[:,0]\n\n#Let the reconstruction of the first point using first PC be given by\np_2 = w_1 @ (X_1 @ w_1)\nprint(\"The reconstruction error with first PC is \"+ str(np.sum(np.square(p_1 - p_2))))\n\nThe reconstruction error with first PC is 0.125\n\n\nThe reconstruction error for the entire dataset along the first principal component will be\n\n#Reconstruction error for each point when considering the first principal component\nrec_error_1 = np.square(np.linalg.norm(X_centered[:,] - (w_1 @ (X_centered.T @ w_1).reshape(1,n))[:,], axis=0))\nprint(rec_error_1)\n\n[0.125 1.125 0.125 0.125 0.125 1.125 0.125 0.125]\n\n\n\n#Total reconstruction error when considering first principal component\n\nprint(\"The reconstruction error along the first principal component is \"+str(np.round((rec_error_1).mean(),4)))\n\nThe reconstruction error along the first principal component is 0.375\n\n\nThe reconstruction error for the entire dataset along \\mathbf{w}_r will be\n\nw_r = np.array([0,1]).reshape(-1,1)\n\n\n#Reconstruction error for each point when considering the vector w_r\nrec_error_r = np.square(np.linalg.norm(X_centered[:,] - (w_r @ (X_centered.T @ w_r).reshape(1,n))[:,], axis=0))\nprint(rec_error_r)\n\n[ 1.5625  5.0625 10.5625 18.0625  0.5625 14.0625  7.5625 14.0625]\n\n\n\nprint(\"The reconstruction error along w_r is \"+str((rec_error_r).mean()))\n\nThe reconstruction error along w_r is 8.9375\n\n\nFor our dataset we can see that the reconstruction error is much lower along the first principal component as compared to the vector \\mathbf{w}_r"
  },
  {
    "objectID": "notes/Not01.html#finding-the-optimal-value-of-k",
    "href": "notes/Not01.html#finding-the-optimal-value-of-k",
    "title": "Week 1: Standard PCA",
    "section": "Finding the optimal value of K",
    "text": "Finding the optimal value of K\n\n#Sort the eigenvalues in descending order\neigval, eigvec = np.linalg.eigh(C)\neigval = eigval[::-1]\n\n\ndef var_thresh(k):\n  tot_var = 0\n  req_var = 0\n  for x in eigval:\n    tot_var += x\n  for y in range(k):\n    req_var += eigval[y]\n\n  return (req_var/tot_var)\n\nfor i in range(d+1):\n  print(\"The explained variance when K is \"+str(i)+\" is \"+str(np.round(var_thresh(i),4)))\n\nThe explained variance when K is 0 is 0.0\nThe explained variance when K is 1 is 0.979\nThe explained variance when K is 2 is 1.0"
  },
  {
    "objectID": "notes/Not01.html#pca-on-a-real-world-dataset",
    "href": "notes/Not01.html#pca-on-a-real-world-dataset",
    "title": "Week 1: Standard PCA",
    "section": "PCA on a real-world Dataset",
    "text": "PCA on a real-world Dataset\nWe will be working with a subset of the MNIST dataset. The cell below generates the data-matrix \\mathbf{X}, which is of shape (d, n), where n denotes the number of samples and d denotes the number of features.\n\n##### DATASET GENERATION #####\nimport numpy as np\nfrom keras.datasets import mnist\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\nX = X_train[y_train == 2][: 100].reshape(-1, 28 * 28).T\ntest_image = X_test[y_test == 2][0].reshape(28 * 28)\n\n\n# Observe the first image in the dataset\nimport matplotlib.pyplot as plt\nimg = X[:,0].reshape(28, 28)\nplt.imshow(img, cmap = 'gray');\n\n\n\n\nWe need to center the dataset \\mathbf{X} around its mean. Let us call this centered dataset \\mathbf{X}^{\\prime}.\n\ndef center(X):\n    return X - X.mean(axis = 1).reshape(-1,1)\n\nd, n = X.shape\nX_prime = center(X)\n\nCompute the covariance matrix \\mathbf{C} of the centered dataset.\n\ndef covariance(X):\n    return X @ X.T / X.shape[1]\n\nC = covariance(X_prime)\n\nCompute the first and second principal components of the dataset, \\mathbf{w}_1 and \\mathbf{w}_2.\n\ndef compute_pc(C):\n    d = C.shape[0]\n    eigval, eigvec = np.linalg.eigh(C)\n    w_1, w_2 = eigvec[:, -1], eigvec[:, -2]\n    return w_1, w_2\n\nw_1, w_2 = compute_pc(C)\n\nVisualize the first principal component as an image.\n\nw_1_image = w_1.reshape(28, 28)\nplt.imshow(w_1_image, cmap = 'gray')\n\n&lt;matplotlib.image.AxesImage at 0x7f6ea02093f0&gt;\n\n\n\n\n\nGiven a test_image, visualize the proxies by reconstructing it using the top k principal components. Consider four values of k; values of k for which the top-k principal components explain:\n\n20% of the variance\n50% of the variance\n80% of the variance\n95% of the variance\n\n\ndef reconstruct(C, test_image, thresh):\n    eigval, eigvec = np.linalg.eigh(C)\n    eigval = list(reversed(eigval))\n    tot = sum(eigval)\n    K = len(eigval)\n    for k in range(len(eigval)):\n        if sum(eigval[: k + 1]) / tot &gt;= thresh:\n            K = k + 1\n            break\n    W = eigvec[:, -K: ]\n    coeff = test_image @ W\n    return W @ coeff\n\nplt.figure(figsize=(20,20))\n# 0.20\nrecon_image = reconstruct(C, test_image, 0.20)\nplt.subplot(1, 5, 1)\nplt.imshow(recon_image.reshape(28, 28))\nplt.title(\"Variance covered = 20%\")\n# 0.5\nrecon_image = reconstruct(C, test_image, 0.50)\nplt.subplot(1, 5, 2)\nplt.imshow(recon_image.reshape(28, 28))\nplt.title(\"Variance covered = 50%\")\n# 0.80\nrecon_image = reconstruct(C, test_image, 0.80)\nplt.subplot(1, 5, 3)\nplt.imshow(recon_image.reshape(28, 28))\nplt.title(\"Variance covered = 80%\")\n# 0.95\nplt.subplot(1, 5, 4)\nrecon_image = reconstruct(C, test_image, 0.95)\nplt.imshow(recon_image.reshape(28, 28))\nplt.title(\"Variance covered = 95%\")\n# Original mean subtracted image\ntest_image = np.float64(test_image) - X.mean(axis = 1)\nplt.subplot(1, 5, 5)\nplt.imshow(test_image.reshape(28, 28))\nplt.title(\"Test Image\")\n\nText(0.5, 1.0, 'Test Image')"
  },
  {
    "objectID": "notes/Not05.html",
    "href": "notes/Not05.html",
    "title": "Week 5: Linear Regression: Least Squares and Kernel Regression",
    "section": "",
    "text": "Colab Link: Click here!\nimport numpy as np\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "notes/Not05.html#optimizing-the-error-function",
    "href": "notes/Not05.html#optimizing-the-error-function",
    "title": "Week 5: Linear Regression: Least Squares and Kernel Regression",
    "section": "Optimizing the Error Function",
    "text": "Optimizing the Error Function\nThe minimization equation can be rewritten in the vectorized form as, \\begin{equation*}\n\\min_{\\mathbf{w} \\in \\mathbb{R}^{d}}\\frac{1}{2} ||\\mathbf{X}^{T}\\mathbf{w} -\\mathbf{y} ||_{2}^{2}\n\\end{equation*} Let this be a function of \\mathbf{w} and as follows: \\begin{align*}\nf(\\mathbf{w} ) & =\\underset{\\mathbf{w} \\in \\mathbb{R}^{d}}{\\min} \\frac{1}{2} ||\\mathbf{X}^{T}\\mathbf{w} -\\mathbf{y} ||_{2}^{2}\\\\\nf(\\mathbf{w} ) & = \\frac{1}{2} (\\mathbf{X}^{T}\\mathbf{w} -\\mathbf{y} )^{T} (\\mathbf{X}^{T}\\mathbf{w} -\\mathbf{y} )\\\\\n\\therefore \\triangledown f(\\mathbf{w} ) & =(\\mathbf{XX}^{T} )\\mathbf{w} -(\\mathbf{Xy} )\n\\end{align*} Setting the above equation to zero, we get \\begin{align*}\n(\\mathbf{XX}^{T} )\\mathbf{w} -(\\mathbf{Xy} ) & =0\\\\\n(\\mathbf{XX}^{T} )\\mathbf{w}^{*} & =\\mathbf{Xy}\\\\\n\\therefore \\mathbf{w}^{*} & =(\\mathbf{XX}^{T} )^{+}\\mathbf{Xy}\n\\end{align*} where (\\mathbf{XX}^{T} )^{+} is the pseudo-inverse of \\mathbf{XX}^{T}.\n\nX = np.vstack((np.array([[1,1,1,1]]),X))\nX\n\narray([[ 1,  1,  1,  1],\n       [-2, -1,  1,  2]])\n\n\n\nw = np.linalg.pinv(X@X.T)@X@y\nw\n\narray([[0.  ],\n       [2.02]])"
  },
  {
    "objectID": "notes/Not05.html#using-gradient-descent",
    "href": "notes/Not05.html#using-gradient-descent",
    "title": "Week 5: Linear Regression: Least Squares and Kernel Regression",
    "section": "Using Gradient Descent",
    "text": "Using Gradient Descent\nAs we know w^* is the solution of an unconstrained optimization problem, we can solve it using gradient descent. It is given by, \\begin{align*}\nw^{t+1} &= w^t - \\eta^t \\bigtriangledown f(w^t) \\\\\n\\therefore w^{t+1} &= w^t - \\eta^t \\left [ (XX^T)w^t - (Xy) \\right ]\n\\end{align*} where \\eta is a scalar used to control the step-size of the descent and t is the current iteration.\n\neta = 1e-1\nw_grad = np.zeros((X.shape[0], 1))\nepochs = 1\n\nfor i in range(epochs):\n    w_grad = w_grad - eta*((X@X.T)@w_grad - X@y)\n    print(w_grad)\n\n[[0.  ]\n [2.02]]\n\n\n\ny_pred = w_grad.T@X\n\n\nplt.scatter(X[1], y)\nplt.plot(X[1], y_pred.T, c='r')\nplt.grid()\nplt.axhline(c='k')\nplt.axvline(c='k');"
  },
  {
    "objectID": "notes/Not05.html#kernel-regression-algorithm",
    "href": "notes/Not05.html#kernel-regression-algorithm",
    "title": "Week 5: Linear Regression: Least Squares and Kernel Regression",
    "section": "Kernel Regression Algorithm",
    "text": "Kernel Regression Algorithm\nGiven a dataset \\{x_{1} ,\\dotsc ,x_{n} \\} where x_{i} \\in \\mathbb{R}^{d}, let \\{y_{1} ,\\dotsc ,y_{n} \\} be the labels, where y_{i} \\in \\mathbb{R}. \\begin{equation*}\n\\mathbf{X} =\\begin{bmatrix}\n1 & 2 & -1 & -2\n\\end{bmatrix} \\quad \\mathbf{y} \\ =\\ \\begin{bmatrix}\n1\\\\\n3.9\\\\\n0.9\\\\\n4.1\n\\end{bmatrix}\n\\end{equation*}\n\nX = np.array([[1,2,-1,-2]])\ny = np.array([[1,3.9,0.9,4.1]]).T\nX = np.vstack((np.array([[1,1,1,1]]),X))\n\n\nplt.scatter(X[1], y)\nplt.grid()\nplt.axhline(c='k')\nplt.axvline(c='k');"
  },
  {
    "objectID": "notes/Not05.html#using-kernel-regression",
    "href": "notes/Not05.html#using-kernel-regression",
    "title": "Week 5: Linear Regression: Least Squares and Kernel Regression",
    "section": "Using Kernel Regression",
    "text": "Using Kernel Regression\nLet’s use the polynomial kernel of degree of two. By applying the kernel function to the dataset, we obtain,\n\\begin{aligned}\n\\mathbf{K} & =\\left(\\mathbf{X}^{T}\\mathbf{X} +1\\right)^{2}\n\\end{aligned}\n\nK = (X.T@X+1)**2\nK\n\narray([[ 9, 16,  1,  0],\n       [16, 36,  0,  4],\n       [ 1,  0,  9, 16],\n       [ 0,  4, 16, 36]])\n\n\nLet \\mathbf{w}^{*} =\\mathbf{X\\alpha }^{*} for some \\mathbf{\\alpha }^{*} \\in \\mathbb{R}^{n}. \\begin{align*}\n\\mathbf{\\alpha }^{*} =\\mathbf{K}^{-1}\\mathbf{y}\n\\end{align*}\n\nalp = np.linalg.pinv(K)@y\nalp\n\narray([[-0.18130556],\n       [ 0.17072222],\n       [-0.17980556],\n       [ 0.17372222]])\n\n\nLet X_{test} \\in R^{d \\times m} be the test dataset. We predict by, \\begin{align*}\nw^*\\phi(X_{test}) &=  \\sum _{i=1} ^n \\alpha_i^* k(x_i, x_{test_i})\n\\end{align*} where \\alpha_i^* gives the importance of the i^{th} datapoint towards w^* and k(x_i, x_{test_i}) shows how similar x_{test_i} is to x_i.\n\ny_pred = K.T@alp\ny_pred\n\narray([[0.92],\n       [3.94],\n       [0.98],\n       [4.06]])\n\n\n\nX_test = np.linspace(-2.5, 2.5).reshape((1,-1))\nX_test = np.vstack((np.ones((1,50)),X_test))\nK_test = (X.T@X_test+1)**2\ny_test = K_test.T@alp\n\nplt.scatter(X[1], y)\nplt.plot(np.linspace(-2.5, 2.5).reshape((-1,1)), y_test, c='r')\nplt.grid()\nplt.axhline(c='k')\nplt.axvline(c='k');"
  },
  {
    "objectID": "notes/Not02.html",
    "href": "notes/Not02.html",
    "title": "Week 2: Kernel PCA",
    "section": "",
    "text": "Colab Link: Click here!\nimport numpy as np\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "notes/Not02.html#kernel-pca",
    "href": "notes/Not02.html#kernel-pca",
    "title": "Week 2: Kernel PCA",
    "section": "Kernel PCA",
    "text": "Kernel PCA\nLet’s take a dataset \\mathbf{X} where\n\nd: no. of features\nn: no. of datapoints \nX=\\left [\n\\begin{array}{ccccc}\n  | & | & | & & | \\\\\n  x_1 & x_2 & x_3 & \\ldots & x_4 \\\\\n  | & | & | & & |\n\\end{array}\n\\right ]\n\n\n\nX = np.array([[1, 1],[2, 4],[-1, 1],[-2, 4]]).T\n\n\nX\n\narray([[ 1,  2, -1, -2],\n       [ 1,  4,  1,  4]])\n\n\n\nplt.scatter(X[0, :], X[1, :])\nplt.axhline(c='k')\nplt.axvline(c='k');\nplt.grid();\n\n\n\n\n\nStep 1: Calculate \\mathbf{K} \\in \\mathbb{R}^{n \\times n} using a kernel function where \\mathbf{K}_{ij}=k(x_i,x_j).\n\ndef pol_ker(A, B, k):\n    return (A.T@B + 1) ** k\n\nK_pol = pol_ker(X, X, 2)\n\n\nK_pol\n\narray([[  9,  49,   1,   9],\n       [ 49, 441,   9, 169],\n       [  1,   9,   9,  49],\n       [  9, 169,  49, 441]])\n\n\n\n\nStep 2: Center the kernel using the following formula.\n\n\\mathbf{K}^C=\\mathbf{K}-\\mathbf{I}\\mathbf{K}-\\mathbf{K}\\mathbf{I}+\\mathbf{I}\\mathbf{K}\\mathbf{I}\n where \\mathbf{K}^C is the centered kernel, and \\mathbf{I} \\in \\mathbb{R}^{n \\times n} where all the elements are \\frac{1}{n}.\n\ndef ker_cen(K):\n    n = K.shape[0]\n    I = np.ones((n,n)) * (1/n)\n    return K - I@K - K@I + I@K@I\n\nKC = ker_cen(K_pol)\n\n\nKC\n\narray([[ 67., -43.,  59., -83.],\n       [-43., 199., -83., -73.],\n       [ 59., -83.,  67., -43.],\n       [-83., -73., -43., 199.]])\n\n\n\n\nStep 3: Compute the eigenvectors \\{\\beta _1, \\beta _2, \\ldots, \\beta _n\\} and eigenvalues \\{n\\lambda _1, n\\lambda _2, \\ldots, n\\lambda _n\\} of K^C and normalize to get\n\n\\forall u \\hspace{2em} \\alpha _u = \\frac{\\beta _u}{\\sqrt{n \\lambda _u}}\n\n\n# Enter your solution here\nlam, bet = np.linalg.eigh(KC)\nlam, bet = lam[::-1][:-1], bet[:,::-1][:,:-1]\n\n\nlam, bet\n\n(array([277.9275172, 252.       ,   2.0724828]),\n array([[ 0.10365278, -0.5       , -0.69946844],\n        [ 0.69946844,  0.5       ,  0.10365278],\n        [-0.10365278, -0.5       ,  0.69946844],\n        [-0.69946844,  0.5       , -0.10365278]]))\n\n\n\n\nStep 3: Compute \\sum _{j=1}^{n}\\mathbf{\\alpha }_{kj}\\mathbf{K}_{ij}^{C} \\ \\ \\forall k\n\\begin{equation*}\n\\mathbf{x}_{i} \\in \\mathbb{R}^{d}\\rightarrow \\left[\\begin{array}{ c c c c }\n\\sum\\limits _{j=1}^{n}\\mathbf{\\alpha }_{1j}\\mathbf{K}_{ij}^{C} & \\sum\\limits _{j=1}^{n}\\mathbf{\\alpha }_{2j}\\mathbf{K}_{ij}^{C} & \\dotsc  & \\sum\\limits _{j=1}^{n}\\mathbf{\\alpha }_{nj}\\mathbf{K}_{ij}^{C}\n\\end{array}\\right]\n\\end{equation*}\n\nalp = bet / np.sqrt(lam.reshape((1,-1)))\n\n\nalp\n\narray([[ 0.00621749, -0.03149704, -0.48587288],\n       [ 0.0419568 ,  0.03149704,  0.0720005 ],\n       [-0.00621749, -0.03149704,  0.48587288],\n       [-0.0419568 ,  0.03149704, -0.0720005 ]])\n\n\n\nX_prime = KC@alp\n\n\nX_prime\n\narray([[  1.72801191,  -7.93725393,  -1.00696319],\n       [ 11.66094908,   7.93725393,   0.14921979],\n       [ -1.72801191,  -7.93725393,   1.00696319],\n       [-11.66094908,   7.93725393,  -0.14921979]])"
  },
  {
    "objectID": "pages/Wk03.html",
    "href": "pages/Wk03.html",
    "title": "Unsupervised Learning - Clustering - K-means/Kernel K-means",
    "section": "",
    "text": "Note\n\n\n\nFeedback/Correction: Click here!.\nPDF Link: Click here!"
  },
  {
    "objectID": "pages/Wk03.html#the-algorithm",
    "href": "pages/Wk03.html#the-algorithm",
    "title": "Unsupervised Learning - Clustering - K-means/Kernel K-means",
    "section": "The Algorithm",
    "text": "The Algorithm\nThe algorithm proceeds as follows:\nStep 1: Initialization: Randomly assign datapoints from the dataset as the initial cluster centers.\nStep 2: Reassignment Step: \\[\nz _i ^{t} = \\underset{k}{\\arg \\min} {|| \\mathbf{x}_i - \\boldsymbol{\\mu} _{k} ^t ||}_2 ^2 \\hspace{2em} \\forall i\n\\]\nStep 3: Compute Means: \\[\n\\boldsymbol{\\mu} _k ^{t+1} = \\frac{\\displaystyle \\sum _{i = 1} ^{n} {\\mathbf{x}_i \\cdot \\mathbb{1}(z_i^t=k)}}{\\displaystyle \\sum _{i = 1} ^{n} {\\mathbb{1}(z_i^t=k)}} \\hspace{2em} \\forall k\n\\]\nStep 4: Loop until Convergence: Repeat steps 2 and 3 until the cluster assignments do not change."
  },
  {
    "objectID": "pages/Wk05.html",
    "href": "pages/Wk05.html",
    "title": "Supervised Learning - Regression - Least Squares; Bayesian view",
    "section": "",
    "text": "Note\n\n\n\nFeedback/Correction: Click here!.\nPDF Link: Click here!"
  },
  {
    "objectID": "pages/Wk05.html#stochastic-gradient-descent",
    "href": "pages/Wk05.html#stochastic-gradient-descent",
    "title": "Supervised Learning - Regression - Least Squares; Bayesian view",
    "section": "Stochastic Gradient Descent",
    "text": "Stochastic Gradient Descent\nStochastic gradient descent (SGD) is an optimization algorithm widely employed in machine learning to minimize the loss function of a model by determining the optimal parameters. Unlike traditional gradient descent, which updates the model parameters based on the entire dataset, SGD updates the parameters using a randomly selected subset of the data, known as a batch. This approach leads to faster training times and makes SGD particularly suitable for handling large datasets.\nInstead of updating \\(\\mathbf{w}\\) using the entire dataset at each step \\(t\\), SGD leverages a small randomly selected subset of \\(k\\) data points to update \\(\\mathbf{w}\\). Consequently, the new gradient becomes \\(2(\\tilde{\\mathbf{X}}\\tilde{\\mathbf{X}}^T\\mathbf{w}^t - \\tilde{\\mathbf{X}}\\tilde{\\mathbf{y}})\\), where \\(\\tilde{\\mathbf{X}}\\) and \\(\\tilde{\\mathbf{y}}\\) represent small samples randomly chosen from the dataset. This strategy is feasible since \\(\\tilde{\\mathbf{X}} \\in \\mathbb{R}^{d \\times k}\\), which is considerably smaller compared to \\(\\mathbf{X}\\).\nAfter \\(T\\) rounds of training, the final estimate is obtained as follows:\n\\[\n\\mathbf{w}_{\\text{SGD}}^T = \\frac{1}{T} \\sum_{i=1}^T \\mathbf{w}^i\n\\]\nThe stochastic nature of SGD contributes to optimal convergence to a certain extent."
  },
  {
    "objectID": "pages/Wk07.html",
    "href": "pages/Wk07.html",
    "title": "Supervised Learning - Classification - K-NN, Decision tree",
    "section": "",
    "text": "Note\n\n\n\nFeedback/Correction: Click here!.\nPDF Link: Click here!"
  },
  {
    "objectID": "pages/Wk07.html#issues-with-k-nn",
    "href": "pages/Wk07.html#issues-with-k-nn",
    "title": "Supervised Learning - Classification - K-NN, Decision tree",
    "section": "Issues with K-NN",
    "text": "Issues with K-NN\nThe K-NN algorithm suffers from several limitations:\n\nThe choice of distance function can yield different results. The Euclidean distance, commonly used, might not always be the best fit for all scenarios.\nComputationally, the algorithm can be demanding. When making predictions for a single test data point, the distances between that data point and all training points must be calculated and sorted. Consequently, the algorithm has a complexity of \\(O(n \\log(n))\\), where \\(n\\) represents the size of the dataset.\nThe algorithm does not learn a model but instead relies on the training dataset for making predictions."
  },
  {
    "objectID": "pages/Wk07.html#goodness-of-a-question",
    "href": "pages/Wk07.html#goodness-of-a-question",
    "title": "Supervised Learning - Classification - K-NN, Decision tree",
    "section": "Goodness of a Question",
    "text": "Goodness of a Question\nTo evaluate the quality of a question, we need a measure of “impurity” for a set of labels \\(\\{y_1, \\ldots, y_n\\}\\). Various measures can be employed, but we will use the Entropy function.\nThe Entropy function is defined as:\n\\[\n\\text{Entropy}(\\{y_1, \\ldots, y_n\\}) = \\text{Entropy}(p) = -\\left( p\\log(p)+(1-p)\\log(1-p) \\right )\n\\]\nHere, \\(\\log(0)\\) is conventionally treated as \\(0\\).\nPictorial representation of the Entropy function:\n\nInformation Gain is then utilized to measure the quality of a split in the decision tree algorithm.\nInformation gain is a commonly used criterion in decision tree algorithms that quantifies the reduction in entropy or impurity of a dataset after splitting based on a given feature. High information gain signifies features that effectively differentiate between the different classes of data and lead to accurate predictions.\nInformation gain is calculated as:\n\\[\n\\text{Information Gain}(\\text{feature}, \\text{value}) = \\text{Entropy}(D) - \\left[\\gamma \\cdot \\text{Entropy}(D_{\\text{yes}}) + (1-\\gamma) \\cdot \\text{Entropy}(D_{\\text{no}}) \\right]\n\\]\nwhere \\(\\gamma\\) is defined as:\n\\[\n\\gamma = \\frac{|D_{\\text{yes}}|}{|D|}\n\\]"
  },
  {
    "objectID": "pages/Wk07.html#decision-tree-algorithm",
    "href": "pages/Wk07.html#decision-tree-algorithm",
    "title": "Supervised Learning - Classification - K-NN, Decision tree",
    "section": "Decision Tree Algorithm",
    "text": "Decision Tree Algorithm\nThe decision tree algorithm follows these steps:\n\nDiscretize each feature within the range [min, max].\nSelect the question that provides the highest information gain.\nRepeat the procedure for subsets \\(D_{\\text{yes}}\\) and \\(D_{\\text{no}}\\).\nStop growing the tree when a node becomes sufficiently “pure” according to a predefined criterion.\n\nDifferent measures, such as the Gini Index, can also be employed to evaluate the quality of a question.\nPictorial depiction of the decision boundary and its decision tree:"
  },
  {
    "objectID": "pages/Wk11.html",
    "href": "pages/Wk11.html",
    "title": "Ensemble methods - Bagging and Boosting (Adaboost)",
    "section": "",
    "text": "Note\n\n\n\nFeedback/Correction: Click here!.\nPDF Link: Click here!"
  },
  {
    "objectID": "pages/Wk11.html#dual-formulation",
    "href": "pages/Wk11.html#dual-formulation",
    "title": "Ensemble methods - Bagging and Boosting (Adaboost)",
    "section": "Dual Formulation",
    "text": "Dual Formulation\nMaximizing the Lagrangian function w.r.t. \\(\\alpha\\) and \\(\\beta\\), and minimizing it w.r.t. \\(w\\) and \\(\\epsilon\\), we get,\n\\[\n\\min _{w, \\epsilon}\\left [\\max _{\\alpha \\ge 0; \\beta \\ge 0}\\frac{1}{2}||w||^2_2 + C\\sum _{i=1} ^n\\epsilon_i + \\sum _{i=1} ^n \\alpha_i(1-(w^Tx_i)y_i - \\epsilon_i) + \\sum _{i=1} ^n \\beta(-\\epsilon_i) \\right ]\n\\]\nThe dual of this is given by,\n\\[\n\\max _{\\alpha \\ge 0; \\beta \\ge 0}\\left [\\min _{w, \\epsilon}\\frac{1}{2}||w||^2_2 + C\\sum _{i=1} ^n\\epsilon_i + \\sum _{i=1} ^n \\alpha_i(1-(w^Tx_i)y_i - \\epsilon_i) + \\sum _{i=1} ^n \\beta(-\\epsilon_i) \\right ]\n\\]\n\\[\n\\max _{\\alpha \\ge 0; \\beta \\ge 0}\\left [\\min _{w, \\epsilon}\\mathcal{L}(w, \\epsilon, \\alpha, \\beta) \\right ] \\quad \\ldots[1]\n\\]\nDifferentiating the above function\\([1]\\) w.r.t. \\(w\\) while fixing \\(\\alpha\\) and \\(\\beta\\), we get, \\[\n\\frac{d\\mathcal{L}}{dw}  = 0\n\\] \\[\n\\frac{d}{dw} \\frac{1}{2}||w||^2_2 + C\\sum _{i=1} ^n\\epsilon_i + \\sum _{i=1} ^n \\alpha_i(1-(w^Tx_i)y_i - \\epsilon_i) + \\sum _{i=1} ^n \\beta(-\\epsilon_i) = 0\\\\\n\\] \\[\nw_{\\alpha, \\beta}^* - \\alpha_ix_iy_i = 0\n\\] \\[\n\\therefore w_{\\alpha, \\beta}^* = \\alpha_ix_iy_i \\quad \\ldots [2]\n\\]\nDifferentiating the above function\\([1]\\) w.r.t. \\(\\epsilon_i \\forall i\\) while fixing \\(\\alpha\\) and \\(\\beta\\), we get,\n\\[\n\\frac{\\partial\\mathcal{L}}{\\partial\\epsilon_i}  = 0\n\\] \\[\n\\frac{\\partial}{\\partial\\epsilon_i} \\frac{1}{2}||w||^2_2 + C\\sum _{i=1} ^n\\epsilon_i + \\sum _{i=1} ^n \\alpha_i(1-(w^Tx_i)y_i - \\epsilon_i) + \\sum _{i=1} ^n \\beta(-\\epsilon_i) = 0\n\\] \\[\nC - \\alpha_i -\\beta_i = 0\n\\] \\[\n\\therefore C = \\alpha_i + \\beta_i \\quad \\ldots [3]\n\\]\nSubstituting the values of \\(w\\) and \\(\\beta\\) from \\([2]\\) and \\([3]\\) in \\([1]\\), we get,\n\\[\n\\max _{\\alpha \\ge 0; \\beta \\ge 0; C = \\alpha_i + \\beta_i}\\left [\\frac{1}{2}||\\alpha_ix_iy_i||^2_2 + C\\sum _{i=1} ^n\\epsilon_i + \\sum _{i=1} ^n \\alpha_i(1-((\\alpha_ix_iy_i)^Tx_i)y_i - \\epsilon_i) + \\sum _{i=1} ^n (C-\\alpha_i)(-\\epsilon_i) \\right ]\n\\] \\[\n\\max _{\\alpha \\ge 0; \\beta \\ge 0; C = \\alpha_i + \\beta_i}\\left [\\frac{1}{2}\\alpha_i^Tx_i^Ty_i^Ty_ix_i\\alpha_i + C\\sum _{i=1} ^n\\epsilon_i + \\sum _{i=1} ^n \\alpha_i-\\alpha_i^Tx_i^Ty_i^Ty_ix_i\\alpha_i - \\sum _{i=1} ^n \\alpha_i\\epsilon_i - C\\sum _{i=1} ^n\\epsilon_i + \\sum _{i=1} ^n \\alpha_i\\epsilon_i \\right ]\n\\] \\[\n\\max _{\\alpha \\ge 0; \\beta \\ge 0; C = \\alpha_i + \\beta_i}\\left [\\sum _{i=1} ^n \\alpha_i - \\frac{1}{2}\\alpha_i^Tx_i^Ty_i^Ty_ix_i\\alpha_i\\right ]\n\\] \\[\n\\therefore \\max _{0 \\le \\alpha \\le C}\\left [\\sum _{i=1} ^n \\alpha_i - \\frac{1}{2}\\alpha_i^Tx_i^Ty_i^Ty_ix_i\\alpha_i\\right ]\n\\]"
  },
  {
    "objectID": "pages/Wk09.html",
    "href": "pages/Wk09.html",
    "title": "Discriminative Models - Perceptron; Logistic Regression",
    "section": "",
    "text": "Note\n\n\n\nFeedback/Correction: Click here!.\nPDF Link: Click here!"
  },
  {
    "objectID": "pages/Wk09.html#analysis-of-the-update-rule",
    "href": "pages/Wk09.html#analysis-of-the-update-rule",
    "title": "Discriminative Models - Perceptron; Logistic Regression",
    "section": "Analysis of the Update Rule",
    "text": "Analysis of the Update Rule\nFor a given training example \\((\\mathbf{x}, y)\\), where \\(\\mathbf{x}\\) represents the input and \\(y\\) represents the correct output (either \\(1\\) or \\(-1\\)), the perceptron algorithm updates the weight vector \\(\\mathbf{w}\\) according to the following rules:\n\nIf the perceptron’s prediction on \\(\\mathbf{x}\\) is correct (i.e., \\(\\text{sign}(\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i)==y_i\\)), no update is performed.\nIf the perceptron’s prediction on \\(\\mathbf{x}\\) is incorrect (i.e., \\(\\text{sign}(\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i)\\neq y_i\\)), the weights are updated by adding the product of the input vector and the correct output to the current weight vector: \\(\\mathbf{w}^{(t+1)} = \\mathbf{w}^t + \\mathbf{x}_iy_i\\).\nIt is important to note that the update occurs solely in response to the current data point. Consequently, data points that were previously classified correctly may not be classified similarly in future iterations.\n\nThis update rule effectively adjusts the decision boundary in the direction of correct classification for the misclassified example. The algorithm is guaranteed to converge to a linearly separable solution if the data is indeed linearly separable. However, if the data is not linearly separable, the perceptron algorithm may not converge to a solution."
  },
  {
    "objectID": "pages/Wk09.html#further-assumptions",
    "href": "pages/Wk09.html#further-assumptions",
    "title": "Discriminative Models - Perceptron; Logistic Regression",
    "section": "Further Assumptions",
    "text": "Further Assumptions\nWe introduce three additional assumptions:\n\nLinear Separability with \\(\\gamma\\)-Margin: A dataset \\(D=\\{(\\mathbf{x}_1, y_1), \\ldots, (\\mathbf{x}_n,y_n)\\}\\) is considered linearly separable with a \\(\\gamma\\)-margin if there exists \\(\\mathbf{w}^* \\in \\mathbb{R}^d\\) such that \\((\\mathbf{w}^{*T}\\mathbf{x}_i)y_i\\geq\\gamma\\) holds for all \\(i\\), where \\(\\gamma&gt;0\\).\n\n\n\n\nLinear Separability with \\(\\gamma\\)-Margin\n\n\n\nRadius Assumption: Let \\(R&gt;0 \\in \\mathbb{R}\\) be a constant such that \\(\\forall i \\in D\\), \\(||\\mathbf{x}_i||\\leq R\\). In other words, \\(R\\) denotes the length of the data point farthest from the center.\nNormal Length for \\(\\mathbf{w}^*\\): Assume that \\(\\mathbf{w}^*\\) has unit length."
  },
  {
    "objectID": "pages/Wk09.html#sigmoid-function",
    "href": "pages/Wk09.html#sigmoid-function",
    "title": "Discriminative Models - Perceptron; Logistic Regression",
    "section": "Sigmoid Function",
    "text": "Sigmoid Function\nUntil now, we have utilized the \\(\\text{sign}\\) function to determine the class for the output. However, what if we also wish to obtain the probabilities associated with these outputs?\nLet \\(z=\\mathbf{w}^\\mathbf{T}\\mathbf{x}\\), where \\(z \\in \\mathbb{R}\\). How can we map \\([-\\infty, \\infty]\\rightarrow[0,1]\\)? To address this, we introduce the Sigmoid Function, defined as follows:\n\\[\ng(z) = \\frac{1}{1+e^{-z}}\n\\]\n\n\n\nSigmoid Function\n\n\nThe sigmoid function is commonly employed in machine learning as an activation function for neural networks. It exhibits an S-shaped curve, making it well-suited for modeling processes with a threshold or saturation point, such as logistic growth or binary classification problems.\nFor large positive input values, the sigmoid function approaches 1, while for large negative input values, it approaches 0. When the input value is 0, the sigmoid function output is exactly 0.5.\nThe term “sigmoid” is derived from the Greek word “sigmoides,” meaning “shaped like the letter sigma” (\\(\\Sigma\\)). The sigmoid function’s characteristic S-shaped curve resembles the shape of the letter sigma, which likely influenced the function’s name."
  },
  {
    "objectID": "pages/Wk09.html#logistic-regression-1",
    "href": "pages/Wk09.html#logistic-regression-1",
    "title": "Discriminative Models - Perceptron; Logistic Regression",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nLogistic regression is a statistical method used to analyze and model the relationship between a binary (two-valued) dependent variable and one or more independent variables. The independent variables can be either continuous or categorical. The main objective of logistic regression is to estimate the probability that the dependent variable belongs to one of the two possible values, given the independent variable values.\nIn logistic regression, the dependent variable is modeled as a function of the independent variables using a logistic (sigmoid) function. This function generates an S-shaped curve ranging between 0 and 1. By transforming the output of a linear combination of the independent variables using the logistic function, logistic regression provides a probability estimate that can be used for classifying new observations.\nLet \\(D=\\{(\\mathbf{x}_1, y_1), \\ldots, (\\mathbf{x}_n,y_n)\\}\\) denote the dataset, where \\(\\mathbf{x}_i \\in \\mathbb{R}^d\\) and \\(y_i \\in \\{0, 1\\}\\).\nWe know that:\n\\[\nP(y=1|\\mathbf{x}) = g(\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i) = \\frac{1}{1+e^{-\\mathbf{w}^\\mathbf{T}\\mathbf{x}}}\n\\]\nUsing the maximum likelihood approach, we can derive the following expression:\n\\[\\begin{align*}\n\\mathcal{L}(\\mathbf{w};\\text{Data}) &= \\prod _{i=1} ^{n} (g(\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i))^{y_i}(1- g(\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i))^{1-y_i} \\\\\n\\log(\\mathcal{L}(\\mathbf{w};\\text{Data})) &= \\sum _{i=1} ^{n} y_i\\log(g(\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i))+(1-y_i)\\log(1- g(\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i)) \\\\\n&= \\sum _{i=1} ^{n} y_i\\log\\left(\\frac{1}{1+e^{-\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i}}\\right)+(1-y_i)\\log\\left(\\frac{e^{-\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i}}{1+e^{-\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i}}\\right) \\\\\n&= \\sum _{i=1} ^{n} \\left [ (1-y_i)(-\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i) - \\log(1+e^{-\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i}) \\right ]\n\\end{align*}\\]\nTherefore, our objective, which involves maximizing the log-likelihood function, can be formulated as follows:\n\\[\n\\max _{\\mathbf{w}}\\sum _{i=1} ^{n} \\left [ (1-y_i)(-\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i) - \\log(1+e^{-\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i}) \\right ]\n\\]\nHowever, a closed-form solution for this problem does not exist. Therefore, we resort to using gradient descent for convergence.\nThe gradient of the log-likelihood function is computed as follows:\n\\[\\begin{align*}\n\\nabla \\log(\\mathcal{L}(\\mathbf{w};\\text{Data})) &= \\sum _{i=1} ^{n} \\left [ (1-y_i)(-\\mathbf{x}_i) - \\left( \\frac{e^{-\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i}}{1+e^{-\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i}} \\right ) (-\\mathbf{x}_i) \\right ] \\\\\n&= \\sum _{i=1} ^{n} \\left [ -\\mathbf{x}_i + \\mathbf{x}_iy_i + \\mathbf{x}_i \\left( \\frac{e^{-\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i}}{1+e^{-\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i}} \\right ) \\right ] \\\\\n&= \\sum _{i=1} ^{n} \\left [ \\mathbf{x}_iy_i - \\mathbf{x}_i \\left( \\frac{1}{1+e^{-\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i}} \\right ) \\right ] \\\\\n\\nabla \\log(\\mathcal{L}(\\mathbf{w};\\text{Data})) &= \\sum _{i=1} ^{n} \\left [ \\mathbf{x}_i \\left(y_i - \\frac{1}{1+e^{-\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i}} \\right ) \\right ]\n\\end{align*}\\]\nUtilizing the gradient descent update rule, we obtain:\n\\[\\begin{align*}\n\\mathbf{w}_{t+1} &= \\mathbf{w}_t + \\eta_t\\nabla \\log(\\mathcal{L}(\\mathbf{w};\\text{Data})) \\\\\n&= \\mathbf{w}_t + \\eta_t  \\left ( \\sum _{i=1} ^{n} \\mathbf{x}_i \\left(y_i - \\frac{1}{1+e^{-\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i}} \\right ) \\right )\n\\end{align*}\\]\n\nKernel and Regularized Versions\nIt is possible to argue that \\(\\mathbf{w}^*=\\displaystyle\\sum _{i=1} ^{n}\\alpha_i\\mathbf{x}_i\\), thereby allowing for kernelization. For additional information, please refer to this link.\nThe regularized version of logistic regression can be expressed as follows:\n\\[\n\\min _{\\mathbf{w}}\\sum _{i=1} ^{n} \\left [ \\log(1+e^{-\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i}) + \\mathbf{w}^\\mathbf{T}\\mathbf{x}_i(1-y_i) \\right ] + \\frac{\\lambda}{2}||\\mathbf{w}||^2\n\\]\nHere, \\(\\frac{\\lambda}{2}||\\mathbf{w}||^2\\) serves as the regularizer, and \\(\\lambda\\) is determined through cross-validation."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "To introduce the main methods and models used in machine learning problems of regression, classification and clustering. To study the properties of these models and methods and learn about their suitability for different problems."
  },
  {
    "objectID": "about.html#about-the-course",
    "href": "about.html#about-the-course",
    "title": "About",
    "section": "",
    "text": "To introduce the main methods and models used in machine learning problems of regression, classification and clustering. To study the properties of these models and methods and learn about their suitability for different problems."
  },
  {
    "objectID": "about.html#what-youll-learn",
    "href": "about.html#what-youll-learn",
    "title": "About",
    "section": "What you’ll learn",
    "text": "What you’ll learn\n\nDemonstrating In depth understanding of machine learning algorithms - model, objective or loss function, optimization algorithm and evaluation criteria.\nTweaking machine learning algorithms based on the outcome of experiments - what steps to take in case of underfitting and overfitting.\nBeing able to choose among multiple algorithms for a given task.\nDeveloping an understanding of unsupervised learning techniques."
  },
  {
    "objectID": "about.html#course-instructors",
    "href": "about.html#course-instructors",
    "title": "About",
    "section": "Course Instructors",
    "text": "Course Instructors\nProf. Arun Rajkumar\nAssistant Professor, Department of Computer Sciences & Engineering, IIT Madras"
  },
  {
    "objectID": "about.html#author",
    "href": "about.html#author",
    "title": "About",
    "section": "Author",
    "text": "Author\n\nSherry\n\n\nCo-Authors\n\nA Aniruddha\nVivek Sivaramakrishnan"
  },
  {
    "objectID": "about.html#study-material",
    "href": "about.html#study-material",
    "title": "About",
    "section": "Study Material",
    "text": "Study Material\nThe primary study material for this course is the set of videos and assignments posted on the course page. The prescribed textbook for this course is:\n\nPattern Classification by David G. Stork, Peter E. Hart, and Richard O. Duda\nPattern Recognition and Machine Learning by Christopher M. Bishop\nThe Elements of Statistical Learning: Data Mining, Inference, and Prediction by Trevor Hastie, Robert Tibshirani, and Jerome Friedman\n\nThe learners are advised to make best use of the interaction sessions with the course support members to clarify their doubts.\nTo know more about course syllabus, Instructors and course contents, please click on the below link\nhttps://ds.study.iitm.ac.in/course_pages/BSCCS2007.html\nPlease click on the below tab to view the Course Specific Calendar:\nhttps://calendar.google.com/calendar/u/2?cid=Y19vODg1amdtMTVrbjJzZW01dGlkM2szcjhnNEBncm91cC5jYWxlbmRhci5nb29nbGUuY29t"
  }
]