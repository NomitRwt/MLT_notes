[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CS2007: Machine Learning Techniques",
    "section": "",
    "text": "Note\n\n\n\nThis site is still under development.\nFeedback/Correction: Click here!.\n\n\n\n\n\n\nWeek\nTopic\nLecture Videos\nLecture Slides\nNotes PDF\nTutorial Video\nTutorial Slides\nTutorial Colab\n\n\n\n\n1\nIntroduction; Unsupervised Learning - Representation learning - PCA\n🖥️\n🎫\n📝\n🖥️\n🖥️\n🎫\n\n\n2\nUnsupervised Learning - Representation learning - Kernel PCA\n🖥️\n🎫\n📝\n🖥️\n🖥️\n🎫\n\n\n3\nUnsupervised Learning - Clustering - K-means/Kernel K-means\n🖥️\n🎫\n📝\n🖥️\n🖥️\n🎫\n\n\n4\nUnsupervised Learning - Estimation - Recap of MLE + Bayesian estimation, Gaussian Mixture Model - EM algorithm\n🖥️\n🎫\n📝\n🖥️\n🖥️\n🎫\n\n\n5\nSupervised Learning - Regression - Least Squares; Bayesian view\n🖥️\n🎫\n📝\n🖥️\n🖥️\n🎫\n\n\n6\nSupervised Learning - Regression - Ridge/LASSO\n🖥️\n🎫\n📝\n🖥️\n🖥️\n🎫\n\n\n7\nSupervised Learning - Classification - K-NN, Decision tree\n🖥️\n🎫\n📝\n🖥️\n🖥️\n🎫\n\n\n8\nSupervised Learning - Classification - Generative Models - Naive Bayes\n🖥️\n🎫\n📝\n\n🖥️\n🎫\n\n\n9\nDiscriminative Models - Perceptron; Logistic Regression\n🖥️\n🎫\n📝\n\n🖥️\n🎫\n\n\n10\nSupport Vector Machines\n🖥️\n🎫\n📝\n\n\n\n\n\n11\nEnsemble methods - Bagging and Boosting (Adaboost)\n🖥️\n🎫\n\n\n\n\n\n\n12\nArtificial Neural networks; Multiclass classification\n🖥️\n🎫"
  },
  {
    "objectID": "pages/Not09.html",
    "href": "pages/Not09.html",
    "title": "Week 9: Discriminative Models - Perceptron; Logistic Regression",
    "section": "",
    "text": "Colab Link: Click here!"
  },
  {
    "objectID": "pages/Not09.html#perceptron-algorithm",
    "href": "pages/Not09.html#perceptron-algorithm",
    "title": "Week 9: Discriminative Models - Perceptron; Logistic Regression",
    "section": "Perceptron Algorithm:",
    "text": "Perceptron Algorithm:\n\n\\mathbf{w}^0 = 0\nIn step t, if ∃ some \\mathbf{x}_i \\ni \\text{sign}(\\mathbf{w}^{t}\\mathbf{x}_i^T) != y_i (misclassified):\n\nUpdate \\mathbf{w}^{t+1} = \\mathbf{w}^{t} + \\mathbf{x}_iy_i\n\nElse, stop (converged).\n\n\nfrom IPython.display import HTML, Markdown, display\n\nfrom matplotlib import animation\n\n# Initialize w to zero\nw = np.array([0, 0])\n\nfig, (ax, ax1) = plt.subplots(1, 2, figsize=(16*2/3, 7*2/3), dpi=150)\n\nartists = []\nframe = []\n\nconverged = False\nn_iter = 0\n\nwhile not converged and n_iter&lt;=10:\n  frame1 = []\n\n  y_pred = np.array(list(map(sign, w@X.T)))\n  true_negatives = np.logical_and(y_pred==-1, y==-1)\n  true_positives = np.logical_and(y_pred==1, y==1)\n  mistakes = np.logical_or(np.logical_and(y_pred==1, y==-1), np.logical_and(y_pred==-1, y==1))\n\n  frame1.append(ax1.text(0.5, 1.05, f'({n_iter}) Check - w = [{w[0]:.2f} {w[1]:.2f}]', transform=ax1.transAxes, ha=\"center\"))\n\n  if np.linalg.norm(w):\n    frame1.append(ax1.axline([0, 0], slope=perp(w), c='black'))\n    frame1.append(ax1.scatter(X[:, 0][true_positives], X[:, 1][true_positives], color='green'))\n    frame1.append(ax1.scatter(X[:, 0][true_negatives], X[:, 1][true_negatives], color='red'))\n    frame1.append(ax1.scatter(X[:, 0][mistakes], X[:, 1][mistakes], color='blue'))\n\n  else:\n    frame1.append(ax1.scatter(X[:, 0][true_positives], X[:, 1][true_positives], color='green', label='Positive'))\n    frame1.append(ax1.scatter(X[:, 0][true_negatives], X[:, 1][true_negatives], color='red', label='Negative'))\n    frame1.append(ax1.scatter(X[:, 0][mistakes], X[:, 1][mistakes], color='blue', label='Mistakes'))\n    frame1.append(ax.scatter([], [], color='cornflowerblue', s=[50], label='Update; False Negative'))\n    frame1.append(ax.scatter([], [], color='lightcoral', s=[50], label='Update; False Positive'))\n    ax1.legend(loc='lower right')\n    ax.legend(loc='lower right')\n\n  frame1.append(ax1.axvline(x=0, c='black'))\n  frame1.append(ax1.axhline(y=0, c='black'))\n\n  frame1.append(ax1.arrow(0, 0, w[0], w[1], length_includes_head=True, head_width=0.1))\n  artists.append(frame1+frame)\n\n  n_iter += 1\n\n  for i in range(n):\n\n    if sign(w@X[i].T) != y[i]: # if mistake\n\n      frame = []\n      frame.append(ax.text(0.5, 1.05, f'({n_iter}) Arbitrary mistake on [{X[i][0]:.2f} {X[i][1]:.2f}] ' + ['(False Positive)', '(False Negative)'][int(y[i]==1)], transform=ax.transAxes, ha=\"center\"))\n      frame.append(ax.axvline(x=0, c='black'))\n      frame.append(ax.axhline(y=0, c='black'))\n      frame.append(ax.arrow(0, 0, w[0], w[1], length_includes_head=True, head_width=0.1))\n      if y[i] == 1:\n        frame.append(ax.scatter([X[i][0]], [X[i][1]], color='cornflowerblue', s=[50]))\n      else:\n        frame.append(ax.scatter([X[i][0]], [X[i][1]], color='lightcoral', s=[50]))\n      if np.linalg.norm(w):\n        frame.append(ax.axline([0, 0], slope=perp(w), c='black'))\n      artists.append(frame+frame1)\n\n      frame = []\n      frame.append(ax.text(0.5, 1.05, f'({n_iter}) Update - w = [{w[0]:.2f} {w[1]:.2f}] + ({y[i]}) * [{X[i][0]:.2f} {X[i][1]:.2f}]', transform=ax.transAxes, ha=\"center\"))\n      frame.append(ax.axvline(x=0, c='black'))\n      frame.append(ax.axhline(y=0, c='black'))\n      frame.append(ax.arrow(0, 0, w[0], w[1], length_includes_head=True, head_width=0.1))\n      if y[i] == 1:\n        frame.append(ax.arrow(w[0], w[1], y[i]*X[i][0], y[i]*X[i][1], length_includes_head=True, head_width=0.1, color='cornflowerblue'))\n        frame.append(ax.arrow(0, 0, y[i]*X[i][0], y[i]*X[i][1], length_includes_head=True, head_width=0.1, color='cornflowerblue', linestyle='--'))\n        frame.append(ax.scatter([X[i][0]], [X[i][1]], color='cornflowerblue', s=[50]))\n      else:\n        frame.append(ax.arrow(w[0], w[1], y[i]*X[i][0], y[i]*X[i][1], length_includes_head=True, head_width=0.1, color='lightcoral'))\n        frame.append(ax.arrow(0, 0, y[i]*X[i][0], y[i]*X[i][1], length_includes_head=True, head_width=0.1, color='lightcoral', linestyle='--'))\n        frame.append(ax.scatter([X[i][0]], [X[i][1]], color='lightcoral', s=[50]))\n      if np.linalg.norm(w):\n        frame.append(ax.axline([0, 0], slope=perp(w), c='black'))\n      artists.append(frame+frame1)\n\n      # update w\n      w = w + X[i]*y[i]\n\n      frame = []\n      frame.append(ax.text(0.5, 1.05, f'({n_iter}) Updated w = [{w[0]:.2f} {w[1]:.2f}]', transform=ax.transAxes, ha=\"center\"))\n      frame.append(ax.axvline(x=0, c='black'))\n      frame.append(ax.axhline(y=0, c='black'))\n      frame.append(ax.arrow(0, 0, w[0], w[1], length_includes_head=True, head_width=0.1))\n      if y[i] == 1:\n        frame.append(ax.scatter([X[i][0]], [X[i][1]], color='cornflowerblue', s=[50]))\n      else:\n        frame.append(ax.scatter([X[i][0]], [X[i][1]], color='lightcoral', s=[50]))\n      if np.linalg.norm(w):\n        frame.append(ax.axline([0, 0], slope=perp(w), c='black'))\n      artists.append(frame+frame1)\n\n      break\n  else:\n    # if no mistakes, perceptron has converged\n    converged = True\n\nplt.close()\n\nanim = animation.ArtistAnimation(fig, artists, interval=500, repeat=False, blit=False);\ndisplay(HTML(anim.to_jshtml()))\n\nMarkdown(r\"Perceptron converges to $$\\mathbf{w} =\\begin{pmatrix}%.2f\\\\%.2f\\end{pmatrix}$$ after %i updates.\"%(w[0], w[1], n_iter))\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\nPerceptron converges to \\mathbf{w} =\\begin{pmatrix}7.06\\\\10.34\\end{pmatrix} after 9 updates."
  },
  {
    "objectID": "pages/Not09.html#logistic-regression",
    "href": "pages/Not09.html#logistic-regression",
    "title": "Week 9: Discriminative Models - Perceptron; Logistic Regression",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nWe consider our prediction probabilty to be proportional to the distance of a point from the decision boundary. With this formulation, we use MLE to learn the best decision boundary for a given dataset.\n\nP(y=1|\\mathbf{x}) = σ(\\mathbf{w}^T\\mathbf{x}) = \\frac{1}{1+e^{-\\mathbf{w}^T\\mathbf{x}}}\n\n\nw = np.array([1, 1])/np.sqrt(2)\ny = np.array([int(w@x.T &gt;= 0) for x in X])\n\nsigmoid = lambda w, x: 1/(1+np.exp(-1*w@x.T))\ny_proba = np.array([sigmoid(w, x) for x in X])\n\nplt.scatter(X[:, 0], X[:, 1], c=y_proba, cmap='bwr')\nplt.arrow(0, 0, w[0], w[1], length_includes_head=True, head_width=0.1)\nplt.axline([0, 0], slope=perp(w), c='black')\n\nplt.axvline(x=0, c='black')\nplt.axhline(y=0, c='black')\n\nplt.show()\n\n\n\n\nGoal: Minimize error (Negative Log-Likelihood) function below \n\\min_{\\mathbf{w}} -\\ln L(\\mathbf{w}) =\\min_{\\mathbf{w}} -\\sum_{i=1}^{n} y_i \\ln (σ(\\mathbf{w}^T\\mathbf{x})) + (1-y_i) \\ln (1-σ(\\mathbf{w}^T\\mathbf{x}))\n\nThe gradient of the log-lokelihood is given as follows:\n\n\\nabla \\ln L(\\mathbf{w}) = \\sum_{i=1}^{n} \\left(σ(\\mathbf{w}^T\\mathbf{x})- y_i \\right) \\mathbf{x}_i\n\nWe perform gradient descent (till convergence) as follows: \n\\mathbf{w}^{t+1} = \\mathbf{w}^t - \\eta \\nabla \\log L(\\mathbf{w}^t)\n\n\nw = np.array([0, 0])\nstep = 0.001\n\ndef grad(w):\n  y_proba = np.array([sigmoid(w, x) for x in X])\n  return X.T@(y_proba-y)\n\ndef error(w):\n  l = 0\n  for i in range(n):\n    l += y[i]*np.log(sigmoid(w, X[i])) + (1-y[i])*np.log(1-sigmoid(w, X[i]))\n  return -l\n\nw_next = w - grad(w) * step\n\nn_iter = 0\nresults = r\"Iteration Number $i$ | $\\mathbf{w}$ | Error\"+\"\\n--|--|--\\n\"\n\nwhile np.linalg.norm(w-w_next)&gt;0.001 and n_iter&lt;41:\n\n  if not n_iter%10 or n_iter &lt;= 3:\n    results += str(n_iter)+r\" | $$\\begin{pmatrix}{\" + str(round(w[0], 2)) + \",\\quad}{\" + str(round(w[0], 2)) + r\"}\\end{pmatrix}$$ | \" + f\"{error(w):.2f}\\n\"\n\n  w = w_next\n  y_proba = np.array([sigmoid(w, x) for x in X])\n  grad_step = grad(w_next) * step\n\n  w_next = w_next - grad_step\n  n_iter+=1\n\nMarkdown(results)\n\n\n\n\nIteration Number i\n\\mathbf{w}\nError\n\n\n\n\n0\n\\begin{pmatrix}{0,\\quad}{0}\\end{pmatrix}\n59.61\n\n\n1\n\\begin{pmatrix}{0.09,\\quad}{0.09}\\end{pmatrix}\n44.56\n\n\n2\n\\begin{pmatrix}{0.16,\\quad}{0.16}\\end{pmatrix}\n35.93\n\n\n3\n\\begin{pmatrix}{0.21,\\quad}{0.21}\\end{pmatrix}\n30.52\n\n\n10\n\\begin{pmatrix}{0.42,\\quad}{0.42}\\end{pmatrix}\n17.04\n\n\n20\n\\begin{pmatrix}{0.57,\\quad}{0.57}\\end{pmatrix}\n11.83\n\n\n30\n\\begin{pmatrix}{0.67,\\quad}{0.67}\\end{pmatrix}\n9.49\n\n\n40\n\\begin{pmatrix}{0.74,\\quad}{0.74}\\end{pmatrix}\n8.09\n\n\n\n\n\nIt’s worth noting that Logistic Regression can exhibit severe over-fitting on datasets that are linearly seperable. This happens since \\mathbf{w} can be made arbitrarily large along this direction to minimize error. This phenomenon can be observed above, since the dataset in question is linearly seperable.\n\nperceptron_w = np.array([7.06, 10.34])\nerror(perceptron_w/3)\n\n# Scaling along this direction also yields -inf error\n\n1.3071990315434519\n\n\nOne can get away with this singularity by introducing a regularization term (quantity that grows proportional to ||\\mathbf{w}||) into the error function.\nWe illustrate this in our example again, using the following modified loss and gradient functions:\n\n\\begin{aligned}\n\\min_{\\mathbf{w}} -\\ln L(\\mathbf{w}) &= \\min_{\\mathbf{w}} -\\sum_{i=1}^{n} y_i \\ln (σ(\\mathbf{w}^T\\mathbf{x})) + (1-y_i) \\ln (1-σ(\\mathbf{w}^T\\mathbf{x})) + \\frac{λ}{2}||\\mathbf{w}||^2_2 \\\\\n\\nabla \\ln L(\\mathbf{w}) &= \\sum_{i=1}^{n} \\left(σ(\\mathbf{w}^T\\mathbf{x})- y_i \\right) \\mathbf{x}_i + \\lambda \\mathbf{w}\n\\end{aligned}\n\n\nw = np.array([0, 0])\nstep = 0.001\nlmda = 200\n\ndef reg_grad(w):\n  y_proba = np.array([sigmoid(w, x) for x in X])\n  return X.T@(y_proba-y) + lmda*w\n\ndef reg_error(w):\n  l = 0\n  for i in range(n):\n    l += y[i]*np.log(sigmoid(w, X[i])) + (1-y[i])*np.log(1-sigmoid(w, X[i]))\n  return -l + (lmda/2)*w@w.T\n\nw_next = w - reg_grad(w) * step\n\nn_iter = 0\nresults = r\"Iteration Number $i$ | $\\mathbf{w}$ | Regularized Error\"+\"\\n--|--|--\\n\"\n\nwhile np.linalg.norm(w-w_next)&gt;0.001 and n_iter&lt;101:\n\n  if not n_iter%10 or n_iter &lt;= 3:\n    results += str(n_iter)+r\" | $$\\begin{pmatrix}{\" + str(round(w[0], 2)) + \",\\quad}{\" + str(round(w[0], 2)) + r\"}\\end{pmatrix}$$ | \" + f\"{reg_error(w):.2f}\\n\"\n\n  w = w_next\n  y_proba = np.array([sigmoid(w, x) for x in X])\n  reg_grad_step = reg_grad(w_next) * step\n\n  w_next = w_next - reg_grad_step\n  n_iter+=1\nelse:\n  results += str(n_iter)+r\" | $$\\begin{pmatrix}{\" + str(round(w[0], 2)) + \",\\quad}{\" + str(round(w[0], 2)) + r\"}\\end{pmatrix}$$ | \" + f\"{reg_error(w):.2f}\\n\"\n\nMarkdown(results)\n\n\n\n\nIteration Number i\n\\mathbf{w}\nRegularized Error\n\n\n\n\n0\n\\begin{pmatrix}{0,\\quad}{0}\\end{pmatrix}\n59.61\n\n\n1\n\\begin{pmatrix}{0.09,\\quad}{0.09}\\end{pmatrix}\n46.28\n\n\n2\n\\begin{pmatrix}{0.14,\\quad}{0.14}\\end{pmatrix}\n42.17\n\n\n3\n\\begin{pmatrix}{0.17,\\quad}{0.17}\\end{pmatrix}\n40.77\n\n\n10\n\\begin{pmatrix}{0.21,\\quad}{0.21}\\end{pmatrix}\n39.94"
  },
  {
    "objectID": "pages/Not07.html",
    "href": "pages/Not07.html",
    "title": "Week 7: Classification - K-NN, Decision tree",
    "section": "",
    "text": "Colab Link: Click here!\nimport numpy as np\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "pages/Not07.html#real-world-dataset-wine-dataset",
    "href": "pages/Not07.html#real-world-dataset-wine-dataset",
    "title": "Week 7: Classification - K-NN, Decision tree",
    "section": "Real World Dataset: Wine Dataset",
    "text": "Real World Dataset: Wine Dataset\nIn this section, we will explore the Wine dataset, which is a real-world dataset used for classification tasks. The dataset contains various attributes related to wine samples, and our goal is to classify these samples into two categories: Class 0 and Class 1.\n\nDataset Description\n\nFeatures: The dataset includes several features, but for this analysis, we will focus on two specific attributes: ‘proline’ and ‘hue.’ These attributes represent different characteristics of the wine samples.\nLabels: The target variable, denoted as ‘y,’ assigns each sample to one of two classes, Class 0 or Class 1.\n\n\nfrom sklearn.datasets import load_wine\n\nX, y = load_wine(return_X_y=True, as_frame=True)\nX, y = X[y &lt; 2], y[y &lt; 2]\nX = X[['proline', 'hue']]\nX = X.to_numpy()\n\n\n\nData Preprocessing\nBefore using the dataset, we perform some data preprocessing steps to standardize the features. Standardization is a common practice in machine learning to ensure that all features have the same scale. This can improve the performance of our models.\n\nfrom sklearn.preprocessing import StandardScaler\n\nstd_scaler = StandardScaler()\nX = std_scaler.fit_transform(X)\n\nc = np.array(['red', 'green'])\nplt.scatter(X[:, 0], X[:, 1], c = c[y]);\n\n\n\n\n\n\nPredict the class for a test point\n\nc[predict(X, y, np.array([-0.2, 0]), 5)]\n\n'green'\n\n\n\n\nDecision boundary for various values of k\n\nplt.figure(figsize=(10, 7))\nfor ind, k in enumerate([1, 5, 10, 30, 50, 100]):\n    plt.subplot(2, 3, ind + 1)\n    boundary(k)"
  },
  {
    "objectID": "pages/Not07.html#predict-label-for-a-test-point",
    "href": "pages/Not07.html#predict-label-for-a-test-point",
    "title": "Week 7: Classification - K-NN, Decision tree",
    "section": "Predict label for a test point",
    "text": "Predict label for a test point\n\ndef predict(tree, x, ind):\n    if tree[ind]['state'] == 'leaf':\n        return tree[ind]['label']\n    feat, val = tree[ind]['question']\n    if x[feat] &lt; val:\n        ind = 2 * ind + 1\n        return predict(tree, x, ind)\n    else:\n        ind = 2 * ind + 2\n        return predict(tree, x, ind)\n\nc[predict(tree, [3, 4], 0)]\n\n'red'"
  },
  {
    "objectID": "pages/Not07.html#visualize-the-decision-boundary",
    "href": "pages/Not07.html#visualize-the-decision-boundary",
    "title": "Week 7: Classification - K-NN, Decision tree",
    "section": "Visualize the decision boundary",
    "text": "Visualize the decision boundary\n\nx = np.linspace(0, 8, 100)\nfloor = [ ]\ncolor = [ ]\nfor i in range(x.shape[0]):\n    for j in range(x.shape[0]):\n        floor.append([x[i], x[j]])\n        color.append(c[predict(tree, [x[i], x[j]], 0)])\nfloor = np.array(floor)\nplt.scatter(floor[:, 0], floor[:, 1], c = color);"
  },
  {
    "objectID": "pages/Not04.html",
    "href": "pages/Not04.html",
    "title": "Week 4: EM Algorithm",
    "section": "",
    "text": "Colab Link: Click here!\n\nimport numpy as np\nimport pandas as pd\n\n\npd.options.display.float_format = '{:.5f}'.format\n\n%precision 5\n\n'%.5f'\n\n\nConsider the following one-dimensional dataset: \nX\\ =\\ \\begin{bmatrix}\n-1.5 & -1 & -0.5 & 0.5 & 1 & 1.5\n\\end{bmatrix}\n\nX = np.array([(-1.5),(-1),(-0.5),(0.5),(1),(1.5)])\n\nE.M Algorithm  Initialize \\displaystyle \\theta ^{0} = \\displaystyle \\left\\{\\begin{matrix} \\mu _{1}^{0} , & \\dotsc & ,\\mu _{k}^{0}\\\\ \\sigma ^{2}{}_{1}^{0} , & \\dotsc & ,\\sigma ^{2}{}_{k}^{0}\\\\ \\pi _{1}^{0} , & \\dotsc & ,\\pi _{k}^{0} \\end{matrix}\\right\\}   Until Convergence (||\\theta ^{t+1} -\\theta ^{t} ||\\leq \\epsilon ), where \\epsilon is the tolerance parameter, do the following:\n\\begin{align*}\n\\lambda ^{t+1} &=\\underset{\\lambda }{\\arg\\max}\\text{ modified\\_log} (\\theta ^{t} ,\\lambda ) \\quad \\rightarrow \\text{Expectation Step  } \\\\\n\\theta ^{t+1} &=\\underset{\\theta }{\\arg\\max}\\text{ modified\\_log} (\\theta ,\\lambda ^{t+1}) \\rightarrow \\text{Maximization Step }\n\\end{align*}\nWe can initialize \\displaystyle \\theta ^{0} as, \\displaystyle \\begin{aligned}\n\\mu _{1}^{0} \\  & =-0.667\\\\\n\\mu _{2}^{0} \\  & =\\ 0.667\\\\\n\\sigma ^{2}{}_{1}^{0} & =\\ 0.722\\\\\n\\sigma ^{2}{}_{2}^{0} & =\\ 0.722\\ \\\\\n\\pi _{1}^{0} & =\\ 0.5\\\\\n\\pi _{2}^{0} & =\\ 0.5\n\\end{aligned}\n\ndef init():\n    return np.array([-0.667,0.667, 0.722, 0.722, 0.5, 0.5])\n\ntheta_0 = init()\n\nIn the E-step we calculate the values of λ_{k}^{i}  \\begin{align*}\n\\hat{\\lambda }{_{k}^{i}}^{MML} & =\\frac{\\left(\\frac{1}{\\sqrt{2\\pi } \\sigma _{k}} e^{\\frac{-(x_{i} -\\mu _{k} )^{2}}{2\\sigma _{k}^{2}}}\\right) *\\pi _{k}}{{\\displaystyle \\sum _{k=1}^{K}\\left(\\frac{1}{\\sqrt{2\\pi } \\sigma _{k}} e^{\\frac{-(x_{i} -\\mu _{k} )^{2}}{2\\sigma _{k}^{2}}} *\\pi _{k}\\right)}}\n\\end{align*}\n\ndef gaussian(x, mu, sigma):\n    den = np.sqrt(2 * np.pi) * sigma\n    num = np.exp(-(x - mu) ** 2 / (2 * sigma ** 2))\n    return num / den\n\ndef estep(theta, X):\n    n = X.shape[0]\n    K = theta.shape[0] // 3\n    mu, sigma, pi = theta[: K],\\\n                       np.sqrt(theta[K: 2 * K]),\\\n                       theta[2 * K: ]\n    lamb = np.zeros((n, K))\n    for i in range(n):\n        x = X[i]\n        evidence = sum([pi[k] * gaussian(x, mu[k], sigma[k])\n                        for k in range(K)])\n        for k in range(K):\n            prior = pi[k]\n            likelihood = gaussian(x, mu[k], sigma[k])\n            lamb[i][k] = prior * likelihood / evidence\n    return lamb\n\nThe closed form expressions for the parameters is given by,\n\\begin{aligned}\n\\hat{\\mu }_{k}^{MML} & =\\frac{{\\displaystyle \\sum _{i=1}^{n} \\lambda _{k}^{i} x_{i}}}{{\\displaystyle \\sum _{i=1}^{n} \\lambda _{k}^{i}}}\\\\\n\\widehat{\\sigma ^{2}}_{k}^{MML} & =\\frac{{\\displaystyle \\sum _{i=1}^{n} \\lambda _{k}^{i} (x_{i} -\\hat{\\mu }_{k}^{MML} )^{2}}}{{\\displaystyle \\sum _{i=1}^{n} \\lambda _{k}^{i}}}\\\\\n\\hat{\\pi }_{k}^{MML} & =\\frac{{\\displaystyle \\sum _{i=1}^{n} \\lambda _{k}^{i}}}{n}\n\\end{aligned}\n\ndef mstep(lamb, X):\n    n, K = lamb.shape\n    mu = np.zeros(K)\n    var = np.zeros(K)\n    pi = np.zeros(K)\n    for k in range(K):\n        mu[k] = (X * lamb[:, k]).sum() / lamb[:, k].sum()\n        var[k] = (((X - mu[k]) ** 2) * lamb[:, k]).sum() / lamb[:, k].sum()\n        pi[k] = lamb[:, k].sum() / n\n    return np.concatenate([mu, var, pi])\n\nWe will repeat the above two steps until the given convergence criteria is satisfied, (||\\theta ^{t+1} -\\theta ^{t} ||\\leq \\epsilon )  For the given example we shall perform these steps for 8 iterations. After 8 iterations the change in values in negligible.\n\ntheta_k = theta_0\ntheta_k_1 = np.zeros(6)\n\nfor k in range(8):\n  lamb_k = estep(theta_k, X)\n  theta_k_1 = theta_k\n  theta_k = mstep(lamb_k, X)\n  print(\"\\nlambda-\"+str(k+1)+\":\")\n  print(pd.DataFrame(data=lamb_k.T,columns=[1,2,3,4,5,6],index=[1,2]))\n  print(\"\\ntheta-\"+str(k+1)+\": \"+str(theta_k))\n  print(\"\\nnorm(theta_k - theta_k-1): \"+str(np.round(np.linalg.norm(theta_k - theta_k_1),5)))\n  print('-' * 70)\n\n\nlambda-1:\n        1       2       3       4       5       6\n1 0.94111 0.86385 0.71582 0.28418 0.13615 0.05889\n2 0.05889 0.13615 0.28418 0.71582 0.86385 0.94111\n\ntheta-1: [-0.75562  0.75562  0.5957   0.5957   0.5      0.5    ]\n\nnorm(theta_k - theta_k-1): 0.2182\n----------------------------------------------------------------------\n\nlambda-2:\n        1       2       3       4       5       6\n1 0.97823 0.92669 0.78048 0.21952 0.07331 0.02177\n2 0.02177 0.07331 0.21952 0.78048 0.92669 0.97823\n\ntheta-2: [-0.85619  0.85619  0.43361  0.43361  0.5      0.5    ]\n\nnorm(theta_k - theta_k-1): 0.26976\n----------------------------------------------------------------------\n\nlambda-3:\n        1       2       3       4       5       6\n1 0.99733 0.98109 0.87810 0.12190 0.01891 0.00267\n2 0.00267 0.01891 0.12190 0.87810 0.98109 0.99733\n\ntheta-3: [-0.94409  0.94409  0.27536  0.27536  0.5      0.5    ]\n\nnorm(theta_k - theta_k-1): 0.25602\n----------------------------------------------------------------------\n\nlambda-4:\n        1       2       3       4       5       6\n1 0.99997 0.99895 0.96859 0.03141 0.00105 0.00003\n2 0.00003 0.00105 0.03141 0.96859 0.99895 0.99997\n\ntheta-4: [-0.98879  0.98879  0.18895  0.18895  0.5      0.5    ]\n\nnorm(theta_k - theta_k-1): 0.13758\n----------------------------------------------------------------------\n\nlambda-5:\n        1       2       3       4       5       6\n1 1.00000 0.99997 0.99469 0.00531 0.00003 0.00000\n2 0.00000 0.00003 0.00531 0.99469 0.99997 1.00000\n\ntheta-5: [-0.99821  0.99821  0.17024  0.17024  0.5      0.5    ]\n\nnorm(theta_k - theta_k-1): 0.02962\n----------------------------------------------------------------------\n\nlambda-6:\n        1       2       3       4       5       6\n1 1.00000 0.99999 0.99717 0.00283 0.00001 0.00000\n2 0.00000 0.00001 0.00283 0.99717 0.99999 1.00000\n\ntheta-6: [-0.99905  0.99905  0.16857  0.16857  0.5      0.5    ]\n\nnorm(theta_k - theta_k-1): 0.00265\n----------------------------------------------------------------------\n\nlambda-7:\n        1       2       3       4       5       6\n1 1.00000 0.99999 0.99734 0.00266 0.00001 0.00000\n2 0.00000 0.00001 0.00266 0.99734 0.99999 1.00000\n\ntheta-7: [-0.99911  0.99911  0.16845  0.16845  0.5      0.5    ]\n\nnorm(theta_k - theta_k-1): 0.00018\n----------------------------------------------------------------------\n\nlambda-8:\n        1       2       3       4       5       6\n1 1.00000 0.99999 0.99735 0.00265 0.00001 0.00000\n2 0.00000 0.00001 0.00265 0.99735 0.99999 1.00000\n\ntheta-8: [-0.99911  0.99911  0.16844  0.16844  0.5      0.5    ]\n\nnorm(theta_k - theta_k-1): 1e-05\n----------------------------------------------------------------------"
  },
  {
    "objectID": "pages/Not05.html",
    "href": "pages/Not05.html",
    "title": "Week 5: Linear Regression: Least Squares and Kernel Regression",
    "section": "",
    "text": "Colab Link: Click here!\nimport numpy as np\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "pages/Not05.html#optimizing-the-error-function",
    "href": "pages/Not05.html#optimizing-the-error-function",
    "title": "Week 5: Linear Regression: Least Squares and Kernel Regression",
    "section": "Optimizing the Error Function",
    "text": "Optimizing the Error Function\nThe minimization equation can be rewritten in the vectorized form as, \\begin{equation*}\n\\min_{\\mathbf{w} \\in \\mathbb{R}^{d}}\\frac{1}{2} ||\\mathbf{X}^{T}\\mathbf{w} -\\mathbf{y} ||_{2}^{2}\n\\end{equation*} Let this be a function of \\mathbf{w} and as follows: \\begin{align*}\nf(\\mathbf{w} ) & =\\underset{\\mathbf{w} \\in \\mathbb{R}^{d}}{\\min} \\frac{1}{2} ||\\mathbf{X}^{T}\\mathbf{w} -\\mathbf{y} ||_{2}^{2}\\\\\nf(\\mathbf{w} ) & = \\frac{1}{2} (\\mathbf{X}^{T}\\mathbf{w} -\\mathbf{y} )^{T} (\\mathbf{X}^{T}\\mathbf{w} -\\mathbf{y} )\\\\\n\\therefore \\triangledown f(\\mathbf{w} ) & =(\\mathbf{XX}^{T} )\\mathbf{w} -(\\mathbf{Xy} )\n\\end{align*} Setting the above equation to zero, we get \\begin{align*}\n(\\mathbf{XX}^{T} )\\mathbf{w} -(\\mathbf{Xy} ) & =0\\\\\n(\\mathbf{XX}^{T} )\\mathbf{w}^{*} & =\\mathbf{Xy}\\\\\n\\therefore \\mathbf{w}^{*} & =(\\mathbf{XX}^{T} )^{+}\\mathbf{Xy}\n\\end{align*} where (\\mathbf{XX}^{T} )^{+} is the pseudo-inverse of \\mathbf{XX}^{T}.\n\nX = np.vstack((np.array([[1,1,1,1]]),X))\nX\n\narray([[ 1,  1,  1,  1],\n       [-2, -1,  1,  2]])\n\n\n\nw = np.linalg.pinv(X@X.T)@X@y\nw\n\narray([[0.  ],\n       [2.02]])"
  },
  {
    "objectID": "pages/Not05.html#using-gradient-descent",
    "href": "pages/Not05.html#using-gradient-descent",
    "title": "Week 5: Linear Regression: Least Squares and Kernel Regression",
    "section": "Using Gradient Descent",
    "text": "Using Gradient Descent\nAs we know w^* is the solution of an unconstrained optimization problem, we can solve it using gradient descent. It is given by, \\begin{align*}\nw^{t+1} &= w^t - \\eta^t \\bigtriangledown f(w^t) \\\\\n\\therefore w^{t+1} &= w^t - \\eta^t \\left [ (XX^T)w^t - (Xy) \\right ]\n\\end{align*} where \\eta is a scalar used to control the step-size of the descent and t is the current iteration.\n\neta = 1e-1\nw_grad = np.zeros((X.shape[0], 1))\nepochs = 1\n\nfor i in range(epochs):\n    w_grad = w_grad - eta*((X@X.T)@w_grad - X@y)\n    print(w_grad)\n\n[[0.  ]\n [2.02]]\n\n\n\ny_pred = w_grad.T@X\n\n\nplt.scatter(X[1], y)\nplt.plot(X[1], y_pred.T, c='r')\nplt.grid()\nplt.axhline(c='k')\nplt.axvline(c='k');"
  },
  {
    "objectID": "pages/Not05.html#kernel-regression-algorithm",
    "href": "pages/Not05.html#kernel-regression-algorithm",
    "title": "Week 5: Linear Regression: Least Squares and Kernel Regression",
    "section": "Kernel Regression Algorithm",
    "text": "Kernel Regression Algorithm\nGiven a dataset \\{x_{1} ,\\dotsc ,x_{n} \\} where x_{i} \\in \\mathbb{R}^{d}, let \\{y_{1} ,\\dotsc ,y_{n} \\} be the labels, where y_{i} \\in \\mathbb{R}. \\begin{equation*}\n\\mathbf{X} =\\begin{bmatrix}\n1 & 2 & -1 & -2\n\\end{bmatrix} \\quad \\mathbf{y} \\ =\\ \\begin{bmatrix}\n1\\\\\n3.9\\\\\n0.9\\\\\n4.1\n\\end{bmatrix}\n\\end{equation*}\n\nX = np.array([[1,2,-1,-2]])\ny = np.array([[1,3.9,0.9,4.1]]).T\nX = np.vstack((np.array([[1,1,1,1]]),X))\n\n\nplt.scatter(X[1], y)\nplt.grid()\nplt.axhline(c='k')\nplt.axvline(c='k');"
  },
  {
    "objectID": "pages/Not05.html#using-kernel-regression",
    "href": "pages/Not05.html#using-kernel-regression",
    "title": "Week 5: Linear Regression: Least Squares and Kernel Regression",
    "section": "Using Kernel Regression",
    "text": "Using Kernel Regression\nLet’s use the polynomial kernel of degree of two. By applying the kernel function to the dataset, we obtain,\n\\begin{aligned}\n\\mathbf{K} & =\\left(\\mathbf{X}^{T}\\mathbf{X} +1\\right)^{2}\n\\end{aligned}\n\nK = (X.T@X+1)**2\nK\n\narray([[ 9, 16,  1,  0],\n       [16, 36,  0,  4],\n       [ 1,  0,  9, 16],\n       [ 0,  4, 16, 36]])\n\n\nLet \\mathbf{w}^{*} =\\mathbf{X\\alpha }^{*} for some \\mathbf{\\alpha }^{*} \\in \\mathbb{R}^{n}. \\begin{align*}\n\\mathbf{\\alpha }^{*} =\\mathbf{K}^{-1}\\mathbf{y}\n\\end{align*}\n\nalp = np.linalg.pinv(K)@y\nalp\n\narray([[-0.18130556],\n       [ 0.17072222],\n       [-0.17980556],\n       [ 0.17372222]])\n\n\nLet X_{test} \\in R^{d \\times m} be the test dataset. We predict by, \\begin{align*}\nw^*\\phi(X_{test}) &=  \\sum _{i=1} ^n \\alpha_i^* k(x_i, x_{test_i})\n\\end{align*} where \\alpha_i^* gives the importance of the i^{th} datapoint towards w^* and k(x_i, x_{test_i}) shows how similar x_{test_i} is to x_i.\n\ny_pred = K.T@alp\ny_pred\n\narray([[0.92],\n       [3.94],\n       [0.98],\n       [4.06]])\n\n\n\nX_test = np.linspace(-2.5, 2.5).reshape((1,-1))\nX_test = np.vstack((np.ones((1,50)),X_test))\nK_test = (X.T@X_test+1)**2\ny_test = K_test.T@alp\n\nplt.scatter(X[1], y)\nplt.plot(np.linspace(-2.5, 2.5).reshape((-1,1)), y_test, c='r')\nplt.grid()\nplt.axhline(c='k')\nplt.axvline(c='k');"
  },
  {
    "objectID": "pages/Not02.html",
    "href": "pages/Not02.html",
    "title": "Week 2: Kernel PCA",
    "section": "",
    "text": "Colab Link: Click here!\nimport numpy as np\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "pages/Not02.html#kernel-pca",
    "href": "pages/Not02.html#kernel-pca",
    "title": "Week 2: Kernel PCA",
    "section": "Kernel PCA",
    "text": "Kernel PCA\nLet’s take a dataset \\mathbf{X} where\n\nd: no. of features\nn: no. of datapoints \nX=\\left [\n\\begin{array}{ccccc}\n  | & | & | & & | \\\\\n  x_1 & x_2 & x_3 & \\ldots & x_4 \\\\\n  | & | & | & & |\n\\end{array}\n\\right ]\n\n\n\nX = np.array([[1, 1],[2, 4],[-1, 1],[-2, 4]]).T\n\n\nX\n\narray([[ 1,  2, -1, -2],\n       [ 1,  4,  1,  4]])\n\n\n\nplt.scatter(X[0, :], X[1, :])\nplt.axhline(c='k')\nplt.axvline(c='k');\nplt.grid();\n\n\n\n\n\nStep 1: Calculate \\mathbf{K} \\in \\mathbb{R}^{n \\times n} using a kernel function where \\mathbf{K}_{ij}=k(x_i,x_j).\n\ndef pol_ker(A, B, k):\n    return (A.T@B + 1) ** k\n\nK_pol = pol_ker(X, X, 2)\n\n\nK_pol\n\narray([[  9,  49,   1,   9],\n       [ 49, 441,   9, 169],\n       [  1,   9,   9,  49],\n       [  9, 169,  49, 441]])\n\n\n\n\nStep 2: Center the kernel using the following formula.\n\n\\mathbf{K}^C=\\mathbf{K}-\\mathbf{I}\\mathbf{K}-\\mathbf{K}\\mathbf{I}+\\mathbf{I}\\mathbf{K}\\mathbf{I}\n where \\mathbf{K}^C is the centered kernel, and \\mathbf{I} \\in \\mathbb{R}^{n \\times n} where all the elements are \\frac{1}{n}.\n\ndef ker_cen(K):\n    n = K.shape[0]\n    I = np.ones((n,n)) * (1/n)\n    return K - I@K - K@I + I@K@I\n\nKC = ker_cen(K_pol)\n\n\nKC\n\narray([[ 67., -43.,  59., -83.],\n       [-43., 199., -83., -73.],\n       [ 59., -83.,  67., -43.],\n       [-83., -73., -43., 199.]])\n\n\n\n\nStep 3: Compute the eigenvectors \\{\\beta _1, \\beta _2, \\ldots, \\beta _n\\} and eigenvalues \\{n\\lambda _1, n\\lambda _2, \\ldots, n\\lambda _n\\} of K^C and normalize to get\n\n\\forall u \\hspace{2em} \\alpha _u = \\frac{\\beta _u}{\\sqrt{n \\lambda _u}}\n\n\n# Enter your solution here\nlam, bet = np.linalg.eigh(KC)\nlam, bet = lam[::-1][:-1], bet[:,::-1][:,:-1]\n\n\nlam, bet\n\n(array([277.9275172, 252.       ,   2.0724828]),\n array([[ 0.10365278, -0.5       , -0.69946844],\n        [ 0.69946844,  0.5       ,  0.10365278],\n        [-0.10365278, -0.5       ,  0.69946844],\n        [-0.69946844,  0.5       , -0.10365278]]))\n\n\n\n\nStep 3: Compute \\sum _{j=1}^{n}\\mathbf{\\alpha }_{kj}\\mathbf{K}_{ij}^{C} \\ \\ \\forall k\n\\begin{equation*}\n\\mathbf{x}_{i} \\in \\mathbb{R}^{d}\\rightarrow \\left[\\begin{array}{ c c c c }\n\\sum\\limits _{j=1}^{n}\\mathbf{\\alpha }_{1j}\\mathbf{K}_{ij}^{C} & \\sum\\limits _{j=1}^{n}\\mathbf{\\alpha }_{2j}\\mathbf{K}_{ij}^{C} & \\dotsc  & \\sum\\limits _{j=1}^{n}\\mathbf{\\alpha }_{nj}\\mathbf{K}_{ij}^{C}\n\\end{array}\\right]\n\\end{equation*}\n\nalp = bet / np.sqrt(lam.reshape((1,-1)))\n\n\nalp\n\narray([[ 0.00621749, -0.03149704, -0.48587288],\n       [ 0.0419568 ,  0.03149704,  0.0720005 ],\n       [-0.00621749, -0.03149704,  0.48587288],\n       [-0.0419568 ,  0.03149704, -0.0720005 ]])\n\n\n\nX_prime = KC@alp\n\n\nX_prime\n\narray([[  1.72801191,  -7.93725393,  -1.00696319],\n       [ 11.66094908,   7.93725393,   0.14921979],\n       [ -1.72801191,  -7.93725393,   1.00696319],\n       [-11.66094908,   7.93725393,  -0.14921979]])"
  },
  {
    "objectID": "pages/Wk09.html",
    "href": "pages/Wk09.html",
    "title": "Discriminative Models - Perceptron; Logistic Regression",
    "section": "",
    "text": "Note\n\n\n\nFeedback/Correction: Click here!.\nPDF Link: Click here!"
  },
  {
    "objectID": "pages/Wk09.html#analysis-of-the-update-rule",
    "href": "pages/Wk09.html#analysis-of-the-update-rule",
    "title": "Discriminative Models - Perceptron; Logistic Regression",
    "section": "Analysis of the Update Rule",
    "text": "Analysis of the Update Rule\nFor a given training example \\((\\mathbf{x}, y)\\), where \\(\\mathbf{x}\\) represents the input and \\(y\\) represents the correct output (either \\(1\\) or \\(-1\\)), the perceptron algorithm updates the weight vector \\(\\mathbf{w}\\) according to the following rules:\n\nIf the perceptron’s prediction on \\(\\mathbf{x}\\) is correct (i.e., \\(\\text{sign}(\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i)==y_i\\)), no update is performed.\nIf the perceptron’s prediction on \\(\\mathbf{x}\\) is incorrect (i.e., \\(\\text{sign}(\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i)\\neq y_i\\)), the weights are updated by adding the product of the input vector and the correct output to the current weight vector: \\(\\mathbf{w}^{(t+1)} = \\mathbf{w}^t + \\mathbf{x}_iy_i\\).\nIt is important to note that the update occurs solely in response to the current data point. Consequently, data points that were previously classified correctly may not be classified similarly in future iterations.\n\nThis update rule effectively adjusts the decision boundary in the direction of correct classification for the misclassified example. The algorithm is guaranteed to converge to a linearly separable solution if the data is indeed linearly separable. However, if the data is not linearly separable, the perceptron algorithm may not converge to a solution."
  },
  {
    "objectID": "pages/Wk09.html#further-assumptions",
    "href": "pages/Wk09.html#further-assumptions",
    "title": "Discriminative Models - Perceptron; Logistic Regression",
    "section": "Further Assumptions",
    "text": "Further Assumptions\nWe introduce three additional assumptions:\n\nLinear Separability with \\(\\gamma\\)-Margin: A dataset \\(D=\\{(\\mathbf{x}_1, y_1), \\ldots, (\\mathbf{x}_n,y_n)\\}\\) is considered linearly separable with a \\(\\gamma\\)-margin if there exists \\(\\mathbf{w}^* \\in \\mathbb{R}^d\\) such that \\((\\mathbf{w}^{*T}\\mathbf{x}_i)y_i\\geq\\gamma\\) holds for all \\(i\\), where \\(\\gamma&gt;0\\).\n\n\n\n\nLinear Separability with \\(\\gamma\\)-Margin\n\n\n\nRadius Assumption: Let \\(R&gt;0 \\in \\mathbb{R}\\) be a constant such that \\(\\forall i \\in D\\), \\(||\\mathbf{x}_i||\\leq R\\). In other words, \\(R\\) denotes the length of the data point farthest from the center.\nNormal Length for \\(\\mathbf{w}^*\\): Assume that \\(\\mathbf{w}^*\\) has unit length."
  },
  {
    "objectID": "pages/Wk09.html#sigmoid-function",
    "href": "pages/Wk09.html#sigmoid-function",
    "title": "Discriminative Models - Perceptron; Logistic Regression",
    "section": "Sigmoid Function",
    "text": "Sigmoid Function\nUntil now, we have utilized the \\(\\text{sign}\\) function to determine the class for the output. However, what if we also wish to obtain the probabilities associated with these outputs?\nLet \\(z=\\mathbf{w}^\\mathbf{T}\\mathbf{x}\\), where \\(z \\in \\mathbb{R}\\). How can we map \\([-\\infty, \\infty]\\rightarrow[0,1]\\)? To address this, we introduce the Sigmoid Function, defined as follows:\n\\[\ng(z) = \\frac{1}{1+e^{-z}}\n\\]\n\n\n\nSigmoid Function\n\n\nThe sigmoid function is commonly employed in machine learning as an activation function for neural networks. It exhibits an S-shaped curve, making it well-suited for modeling processes with a threshold or saturation point, such as logistic growth or binary classification problems.\nFor large positive input values, the sigmoid function approaches 1, while for large negative input values, it approaches 0. When the input value is 0, the sigmoid function output is exactly 0.5.\nThe term “sigmoid” is derived from the Greek word “sigmoides,” meaning “shaped like the letter sigma” (\\(\\Sigma\\)). The sigmoid function’s characteristic S-shaped curve resembles the shape of the letter sigma, which likely influenced the function’s name."
  },
  {
    "objectID": "pages/Wk09.html#logistic-regression-1",
    "href": "pages/Wk09.html#logistic-regression-1",
    "title": "Discriminative Models - Perceptron; Logistic Regression",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nLogistic regression is a statistical method used to analyze and model the relationship between a binary (two-valued) dependent variable and one or more independent variables. The independent variables can be either continuous or categorical. The main objective of logistic regression is to estimate the probability that the dependent variable belongs to one of the two possible values, given the independent variable values.\nIn logistic regression, the dependent variable is modeled as a function of the independent variables using a logistic (sigmoid) function. This function generates an S-shaped curve ranging between 0 and 1. By transforming the output of a linear combination of the independent variables using the logistic function, logistic regression provides a probability estimate that can be used for classifying new observations.\nLet \\(D=\\{(\\mathbf{x}_1, y_1), \\ldots, (\\mathbf{x}_n,y_n)\\}\\) denote the dataset, where \\(\\mathbf{x}_i \\in \\mathbb{R}^d\\) and \\(y_i \\in \\{0, 1\\}\\).\nWe know that:\n\\[\nP(y=1|\\mathbf{x}) = g(\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i) = \\frac{1}{1+e^{-\\mathbf{w}^\\mathbf{T}\\mathbf{x}}}\n\\]\nUsing the maximum likelihood approach, we can derive the following expression:\n\\[\\begin{align*}\n\\mathcal{L}(\\mathbf{w};\\text{Data}) &= \\prod _{i=1} ^{n} (g(\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i))^{y_i}(1- g(\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i))^{1-y_i} \\\\\n\\log(\\mathcal{L}(\\mathbf{w};\\text{Data})) &= \\sum _{i=1} ^{n} y_i\\log(g(\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i))+(1-y_i)\\log(1- g(\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i)) \\\\\n&= \\sum _{i=1} ^{n} y_i\\log\\left(\\frac{1}{1+e^{-\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i}}\\right)+(1-y_i)\\log\\left(\\frac{e^{-\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i}}{1+e^{-\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i}}\\right) \\\\\n&= \\sum _{i=1} ^{n} \\left [ (1-y_i)(-\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i) - \\log(1+e^{-\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i}) \\right ]\n\\end{align*}\\]\nTherefore, our objective, which involves maximizing the log-likelihood function, can be formulated as follows:\n\\[\n\\max _{\\mathbf{w}}\\sum _{i=1} ^{n} \\left [ (1-y_i)(-\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i) - \\log(1+e^{-\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i}) \\right ]\n\\]\nHowever, a closed-form solution for this problem does not exist. Therefore, we resort to using gradient descent for convergence.\nThe gradient of the log-likelihood function is computed as follows:\n\\[\\begin{align*}\n\\nabla \\log(\\mathcal{L}(\\mathbf{w};\\text{Data})) &= \\sum _{i=1} ^{n} \\left [ (1-y_i)(-\\mathbf{x}_i) - \\left( \\frac{e^{-\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i}}{1+e^{-\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i}} \\right ) (-\\mathbf{x}_i) \\right ] \\\\\n&= \\sum _{i=1} ^{n} \\left [ -\\mathbf{x}_i + \\mathbf{x}_iy_i + \\mathbf{x}_i \\left( \\frac{e^{-\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i}}{1+e^{-\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i}} \\right ) \\right ] \\\\\n&= \\sum _{i=1} ^{n} \\left [ \\mathbf{x}_iy_i - \\mathbf{x}_i \\left( \\frac{1}{1+e^{-\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i}} \\right ) \\right ] \\\\\n\\nabla \\log(\\mathcal{L}(\\mathbf{w};\\text{Data})) &= \\sum _{i=1} ^{n} \\left [ \\mathbf{x}_i \\left(y_i - \\frac{1}{1+e^{-\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i}} \\right ) \\right ]\n\\end{align*}\\]\nUtilizing the gradient descent update rule, we obtain:\n\\[\\begin{align*}\n\\mathbf{w}_{t+1} &= \\mathbf{w}_t + \\eta_t\\nabla \\log(\\mathcal{L}(\\mathbf{w};\\text{Data})) \\\\\n&= \\mathbf{w}_t + \\eta_t  \\left ( \\sum _{i=1} ^{n} \\mathbf{x}_i \\left(y_i - \\frac{1}{1+e^{-\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i}} \\right ) \\right )\n\\end{align*}\\]\n\nKernel and Regularized Versions\nIt is possible to argue that \\(\\mathbf{w}^*=\\displaystyle\\sum _{i=1} ^{n}\\alpha_i\\mathbf{x}_i\\), thereby allowing for kernelization. For additional information, please refer to this link.\nThe regularized version of logistic regression can be expressed as follows:\n\\[\n\\min _{\\mathbf{w}}\\sum _{i=1} ^{n} \\left [ \\log(1+e^{-\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i}) + \\mathbf{w}^\\mathbf{T}\\mathbf{x}_i(1-y_i) \\right ] + \\frac{\\lambda}{2}||\\mathbf{w}||^2\n\\]\nHere, \\(\\frac{\\lambda}{2}||\\mathbf{w}||^2\\) serves as the regularizer, and \\(\\lambda\\) is determined through cross-validation."
  },
  {
    "objectID": "pages/Wk11.html",
    "href": "pages/Wk11.html",
    "title": "Ensemble methods - Bagging and Boosting (Adaboost)",
    "section": "",
    "text": "Note\n\n\n\nFeedback/Correction: Click here!.\nPDF Link: Click here!"
  },
  {
    "objectID": "pages/Wk11.html#dual-formulation",
    "href": "pages/Wk11.html#dual-formulation",
    "title": "Ensemble methods - Bagging and Boosting (Adaboost)",
    "section": "Dual Formulation",
    "text": "Dual Formulation\nMaximizing the Lagrangian function w.r.t. \\(\\alpha\\) and \\(\\beta\\), and minimizing it w.r.t. \\(w\\) and \\(\\epsilon\\), we get,\n\\[\n\\min _{w, \\epsilon}\\left [\\max _{\\alpha \\ge 0; \\beta \\ge 0}\\frac{1}{2}||w||^2_2 + C\\sum _{i=1} ^n\\epsilon_i + \\sum _{i=1} ^n \\alpha_i(1-(w^Tx_i)y_i - \\epsilon_i) + \\sum _{i=1} ^n \\beta(-\\epsilon_i) \\right ]\n\\]\nThe dual of this is given by,\n\\[\n\\max _{\\alpha \\ge 0; \\beta \\ge 0}\\left [\\min _{w, \\epsilon}\\frac{1}{2}||w||^2_2 + C\\sum _{i=1} ^n\\epsilon_i + \\sum _{i=1} ^n \\alpha_i(1-(w^Tx_i)y_i - \\epsilon_i) + \\sum _{i=1} ^n \\beta(-\\epsilon_i) \\right ]\n\\]\n\\[\n\\max _{\\alpha \\ge 0; \\beta \\ge 0}\\left [\\min _{w, \\epsilon}\\mathcal{L}(w, \\epsilon, \\alpha, \\beta) \\right ] \\quad \\ldots[1]\n\\]\nDifferentiating the above function\\([1]\\) w.r.t. \\(w\\) while fixing \\(\\alpha\\) and \\(\\beta\\), we get, \\[\n\\frac{d\\mathcal{L}}{dw}  = 0\n\\] \\[\n\\frac{d}{dw} \\frac{1}{2}||w||^2_2 + C\\sum _{i=1} ^n\\epsilon_i + \\sum _{i=1} ^n \\alpha_i(1-(w^Tx_i)y_i - \\epsilon_i) + \\sum _{i=1} ^n \\beta(-\\epsilon_i) = 0\\\\\n\\] \\[\nw_{\\alpha, \\beta}^* - \\alpha_ix_iy_i = 0\n\\] \\[\n\\therefore w_{\\alpha, \\beta}^* = \\alpha_ix_iy_i \\quad \\ldots [2]\n\\]\nDifferentiating the above function\\([1]\\) w.r.t. \\(\\epsilon_i \\forall i\\) while fixing \\(\\alpha\\) and \\(\\beta\\), we get,\n\\[\n\\frac{\\partial\\mathcal{L}}{\\partial\\epsilon_i}  = 0\n\\] \\[\n\\frac{\\partial}{\\partial\\epsilon_i} \\frac{1}{2}||w||^2_2 + C\\sum _{i=1} ^n\\epsilon_i + \\sum _{i=1} ^n \\alpha_i(1-(w^Tx_i)y_i - \\epsilon_i) + \\sum _{i=1} ^n \\beta(-\\epsilon_i) = 0\n\\] \\[\nC - \\alpha_i -\\beta_i = 0\n\\] \\[\n\\therefore C = \\alpha_i + \\beta_i \\quad \\ldots [3]\n\\]\nSubstituting the values of \\(w\\) and \\(\\beta\\) from \\([2]\\) and \\([3]\\) in \\([1]\\), we get,\n\\[\n\\max _{\\alpha \\ge 0; \\beta \\ge 0; C = \\alpha_i + \\beta_i}\\left [\\frac{1}{2}||\\alpha_ix_iy_i||^2_2 + C\\sum _{i=1} ^n\\epsilon_i + \\sum _{i=1} ^n \\alpha_i(1-((\\alpha_ix_iy_i)^Tx_i)y_i - \\epsilon_i) + \\sum _{i=1} ^n (C-\\alpha_i)(-\\epsilon_i) \\right ]\n\\] \\[\n\\max _{\\alpha \\ge 0; \\beta \\ge 0; C = \\alpha_i + \\beta_i}\\left [\\frac{1}{2}\\alpha_i^Tx_i^Ty_i^Ty_ix_i\\alpha_i + C\\sum _{i=1} ^n\\epsilon_i + \\sum _{i=1} ^n \\alpha_i-\\alpha_i^Tx_i^Ty_i^Ty_ix_i\\alpha_i - \\sum _{i=1} ^n \\alpha_i\\epsilon_i - C\\sum _{i=1} ^n\\epsilon_i + \\sum _{i=1} ^n \\alpha_i\\epsilon_i \\right ]\n\\] \\[\n\\max _{\\alpha \\ge 0; \\beta \\ge 0; C = \\alpha_i + \\beta_i}\\left [\\sum _{i=1} ^n \\alpha_i - \\frac{1}{2}\\alpha_i^Tx_i^Ty_i^Ty_ix_i\\alpha_i\\right ]\n\\] \\[\n\\therefore \\max _{0 \\le \\alpha \\le C}\\left [\\sum _{i=1} ^n \\alpha_i - \\frac{1}{2}\\alpha_i^Tx_i^Ty_i^Ty_ix_i\\alpha_i\\right ]\n\\]"
  },
  {
    "objectID": "pages/Wk07.html",
    "href": "pages/Wk07.html",
    "title": "Supervised Learning - Classification - K-NN, Decision tree",
    "section": "",
    "text": "Note\n\n\n\nFeedback/Correction: Click here!.\nPDF Link: Click here!"
  },
  {
    "objectID": "pages/Wk07.html#issues-with-k-nn",
    "href": "pages/Wk07.html#issues-with-k-nn",
    "title": "Supervised Learning - Classification - K-NN, Decision tree",
    "section": "Issues with K-NN",
    "text": "Issues with K-NN\nThe K-NN algorithm suffers from several limitations:\n\nThe choice of distance function can yield different results. The Euclidean distance, commonly used, might not always be the best fit for all scenarios.\nComputationally, the algorithm can be demanding. When making predictions for a single test data point, the distances between that data point and all training points must be calculated and sorted. Consequently, the algorithm has a complexity of \\(O(n \\log(n))\\), where \\(n\\) represents the size of the dataset.\nThe algorithm does not learn a model but instead relies on the training dataset for making predictions."
  },
  {
    "objectID": "pages/Wk07.html#goodness-of-a-question",
    "href": "pages/Wk07.html#goodness-of-a-question",
    "title": "Supervised Learning - Classification - K-NN, Decision tree",
    "section": "Goodness of a Question",
    "text": "Goodness of a Question\nTo evaluate the quality of a question, we need a measure of “impurity” for a set of labels \\(\\{y_1, \\ldots, y_n\\}\\). Various measures can be employed, but we will use the Entropy function.\nThe Entropy function is defined as:\n\\[\n\\text{Entropy}(\\{y_1, \\ldots, y_n\\}) = \\text{Entropy}(p) = -\\left( p\\log(p)+(1-p)\\log(1-p) \\right )\n\\]\nHere, \\(\\log(0)\\) is conventionally treated as \\(0\\).\nPictorial representation of the Entropy function:\n\nInformation Gain is then utilized to measure the quality of a split in the decision tree algorithm.\nInformation gain is a commonly used criterion in decision tree algorithms that quantifies the reduction in entropy or impurity of a dataset after splitting based on a given feature. High information gain signifies features that effectively differentiate between the different classes of data and lead to accurate predictions.\nInformation gain is calculated as:\n\\[\n\\text{Information Gain}(\\text{feature}, \\text{value}) = \\text{Entropy}(D) - \\left[\\gamma \\cdot \\text{Entropy}(D_{\\text{yes}}) + (1-\\gamma) \\cdot \\text{Entropy}(D_{\\text{no}}) \\right]\n\\]\nwhere \\(\\gamma\\) is defined as:\n\\[\n\\gamma = \\frac{|D_{\\text{yes}}|}{|D|}\n\\]"
  },
  {
    "objectID": "pages/Wk07.html#decision-tree-algorithm",
    "href": "pages/Wk07.html#decision-tree-algorithm",
    "title": "Supervised Learning - Classification - K-NN, Decision tree",
    "section": "Decision Tree Algorithm",
    "text": "Decision Tree Algorithm\nThe decision tree algorithm follows these steps:\n\nDiscretize each feature within the range [min, max].\nSelect the question that provides the highest information gain.\nRepeat the procedure for subsets \\(D_{\\text{yes}}\\) and \\(D_{\\text{no}}\\).\nStop growing the tree when a node becomes sufficiently “pure” according to a predefined criterion.\n\nDifferent measures, such as the Gini Index, can also be employed to evaluate the quality of a question.\nPictorial depiction of the decision boundary and its decision tree:"
  },
  {
    "objectID": "pages/Wk05.html",
    "href": "pages/Wk05.html",
    "title": "Supervised Learning - Regression - Least Squares; Bayesian view",
    "section": "",
    "text": "Note\n\n\n\nFeedback/Correction: Click here!.\nPDF Link: Click here!"
  },
  {
    "objectID": "pages/Wk05.html#stochastic-gradient-descent",
    "href": "pages/Wk05.html#stochastic-gradient-descent",
    "title": "Supervised Learning - Regression - Least Squares; Bayesian view",
    "section": "Stochastic Gradient Descent",
    "text": "Stochastic Gradient Descent\nStochastic gradient descent (SGD) is an optimization algorithm widely employed in machine learning to minimize the loss function of a model by determining the optimal parameters. Unlike traditional gradient descent, which updates the model parameters based on the entire dataset, SGD updates the parameters using a randomly selected subset of the data, known as a batch. This approach leads to faster training times and makes SGD particularly suitable for handling large datasets.\nInstead of updating \\(\\mathbf{w}\\) using the entire dataset at each step \\(t\\), SGD leverages a small randomly selected subset of \\(k\\) data points to update \\(\\mathbf{w}\\). Consequently, the new gradient becomes \\(2(\\tilde{\\mathbf{X}}\\tilde{\\mathbf{X}}^T\\mathbf{w}^t - \\tilde{\\mathbf{X}}\\tilde{\\mathbf{y}})\\), where \\(\\tilde{\\mathbf{X}}\\) and \\(\\tilde{\\mathbf{y}}\\) represent small samples randomly chosen from the dataset. This strategy is feasible since \\(\\tilde{\\mathbf{X}} \\in \\mathbb{R}^{d \\times k}\\), which is considerably smaller compared to \\(\\mathbf{X}\\).\nAfter \\(T\\) rounds of training, the final estimate is obtained as follows:\n\\[\n\\mathbf{w}_{\\text{SGD}}^T = \\frac{1}{T} \\sum_{i=1}^T \\mathbf{w}^i\n\\]\nThe stochastic nature of SGD contributes to optimal convergence to a certain extent."
  },
  {
    "objectID": "pages/Wk03.html",
    "href": "pages/Wk03.html",
    "title": "Unsupervised Learning - Clustering - K-means/Kernel K-means",
    "section": "",
    "text": "Note\n\n\n\nFeedback/Correction: Click here!.\nPDF Link: Click here!"
  },
  {
    "objectID": "pages/Wk03.html#the-algorithm",
    "href": "pages/Wk03.html#the-algorithm",
    "title": "Unsupervised Learning - Clustering - K-means/Kernel K-means",
    "section": "The Algorithm",
    "text": "The Algorithm\nThe algorithm proceeds as follows:\nStep 1: Initialization: Randomly assign datapoints from the dataset as the initial cluster centers.\nStep 2: Reassignment Step: \\[\nz _i ^{t} = \\underset{k}{\\arg \\min} {|| \\mathbf{x}_i - \\boldsymbol{\\mu} _{k} ^t ||}_2 ^2 \\hspace{2em} \\forall i\n\\]\nStep 3: Compute Means: \\[\n\\boldsymbol{\\mu} _k ^{t+1} = \\frac{\\displaystyle \\sum _{i = 1} ^{n} {\\mathbf{x}_i \\cdot \\mathbb{1}(z_i^t=k)}}{\\displaystyle \\sum _{i = 1} ^{n} {\\mathbb{1}(z_i^t=k)}} \\hspace{2em} \\forall k\n\\]\nStep 4: Loop until Convergence: Repeat steps 2 and 3 until the cluster assignments do not change."
  },
  {
    "objectID": "pages/Wk01.html",
    "href": "pages/Wk01.html",
    "title": "Introduction; Unsupervised Learning - Representation learning - PCA",
    "section": "",
    "text": "Note\n\n\n\nFeedback/Correction: Click here!.\nPDF Link: Click here!"
  },
  {
    "objectID": "pages/Wk01.html#broad-paradigms-of-machine-learning",
    "href": "pages/Wk01.html#broad-paradigms-of-machine-learning",
    "title": "Introduction; Unsupervised Learning - Representation learning - PCA",
    "section": "Broad Paradigms of Machine Learning",
    "text": "Broad Paradigms of Machine Learning\n\nSupervised Learning:Supervised Machine Learning is a type of machine learning where the algorithm is trained on a labeled dataset, meaning that the data includes both inputs and their corresponding outputs. The goal of supervised learning is to build a model that can accurately predict the output for new, unseen input data. Few examples:\n\n\nLinear regression for predicting a continuous output\nLogistic regression for binary classification problems\nDecision trees for non-linear classification and regression problems\nSupport Vector Machines for binary and multi-class classification problems\nNeural Networks for complex non-linear problems in various domains such as computer vision, natural language processing, and speech recognition\n\n\nUnsupervised Learning: Unsupervised Machine Learning is a type of machine learning where the algorithm is trained on an unlabeled dataset, meaning that only the inputs are provided and no corresponding outputs. The goal of unsupervised learning is to uncover patterns or relationships within the data without any prior knowledge or guidance. Few examples:\n\n\nClustering algorithms such as K-means, hierarchical clustering, and density-based clustering, used to group similar data points together into clusters\nDimensionality reduction techniques such as Principal Component Analysis (PCA), used to reduce the number of features in a dataset while preserving the maximum amount of information\nAnomaly detection algorithms used to identify unusual data points that deviate from the normal patterns in the data\n\n\nSequential learning: Sequential Machine Learning (also known as time-series prediction) is a type of machine learning that is focused on making predictions based on sequences of data. It involves training the model on a sequence of inputs, such that the predictions for each time step depend on the previous time steps. Few examples:\n\n\nTime series forecasting, used to predict future values based on past trends and patterns in data such as stock prices, weather patterns, and energy consumption\nSpeech recognition, used to transcribe speech into text by recognizing patterns in audio signals\nNatural language processing, used to analyze and make predictions about sequences of text data"
  },
  {
    "objectID": "pages/Wk01.html#potential-algorithm",
    "href": "pages/Wk01.html#potential-algorithm",
    "title": "Introduction; Unsupervised Learning - Representation learning - PCA",
    "section": "Potential Algorithm",
    "text": "Potential Algorithm\nBased on the above concepts, we can outline the following algorithm for representation learning:\nGiven a dataset \\(\\{\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n\\}\\) where \\(\\mathbf{x}_i \\in \\mathbb{R}^{d}\\),\n\nCenter the dataset: \\[\n\\mathbf{\\mu} = \\frac{1}{n} \\sum _{i=1} ^{n} \\mathbf{x}_i\n\\] \\[\n\\mathbf{x}_i = \\mathbf{x}_i - \\mathbf{\\mu}  \\hspace{2em} \\forall i\n\\]\nFind the best representation \\(\\mathbf{w} \\in \\mathbb{R}^d\\) with \\(||\\mathbf{w}|| = 1\\).\nUpdate the dataset with the representation: \\[\n\\mathbf{x}_i = \\mathbf{x}_i - (\\mathbf{x}_i^T\\mathbf{w})\\mathbf{w}  \\hspace{1em} \\forall i\n\\]\nRepeat steps 2 and 3 until the residues become zero, resulting in \\(\\mathbf{w}_2, \\mathbf{w}_3, \\ldots, \\mathbf{w}_d\\).\n\nThe question arises: Is this the most effective approach, and how many \\(\\mathbf{w}\\) do we need to achieve optimal compression?"
  },
  {
    "objectID": "pages/Wk01.html#approximate-representation",
    "href": "pages/Wk01.html#approximate-representation",
    "title": "Introduction; Unsupervised Learning - Representation learning - PCA",
    "section": "Approximate Representation",
    "text": "Approximate Representation\nThe question arises: If the data can be approximately represented by a lower-dimensional subspace, would it suffice to use only those \\(k\\) projections? Additionally, how much variance should be covered?\nLet us consider a centered dataset \\(\\{\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n\\}\\) where \\(\\mathbf{x}_i \\in \\mathbb{R}^{d}\\). Let \\(\\mathbf{C}\\) represent its covariance matrix, and \\(\\{\\lambda_1, \\lambda_2, \\ldots, \\lambda_d \\}\\) be the corresponding eigenvalues, which are non-negative due to the positive semi-definiteness of the covariance matrix. These eigenvalues are arranged in descending order, with \\(\\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_d \\}\\) as their corresponding eigenvectors of unit length.\nThe eigen equation for the covariance matrix can be expressed as follows: \\[\\begin{align*}\n    \\mathbf{C}\\mathbf{w} &= \\lambda \\mathbf{w} \\\\\n    \\mathbf{w}^T\\mathbf{C}\\mathbf{w} &= \\mathbf{w}^T\\lambda \\mathbf{w}\\\\\n    \\therefore \\lambda &= \\mathbf{w}^T\\mathbf{C}\\mathbf{w} \\hspace{2em} \\{\\mathbf{w}^T\\mathbf{w} = 1\\} \\\\\n    \\lambda &= \\frac{1}{n} \\sum _{i=1} ^{n} (\\mathbf{x}_i^T\\mathbf{w})^2 \\\\\n\\end{align*}\\]\nHence, the mean of the dataset being zero, \\(\\lambda\\) represents the variance captured by the eigenvector \\(\\mathbf{w}\\).\nA commonly accepted heuristic suggests that PCA should capture at least 95% of the variance. If the first \\(k\\) eigenvectors capture the desired variance, it can be stated as: \\[\n\\frac{\\displaystyle \\sum _{j=1} ^{k} \\lambda_j}{\\displaystyle \\sum _{i=1} ^{d} \\lambda_i} \\ge 0.95\n\\]\nThus, the higher the variance captured, the lower the error incurred."
  },
  {
    "objectID": "pages/Wk01.html#p.c.a.-algorithm",
    "href": "pages/Wk01.html#p.c.a.-algorithm",
    "title": "Introduction; Unsupervised Learning - Representation learning - PCA",
    "section": "P.C.A. Algorithm",
    "text": "P.C.A. Algorithm\nThe Principal Component Analysis algorithm can be summarized as follows for a centered dataset \\(\\{\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n\\}\\) where \\(\\mathbf{x}_i \\in \\mathbb{R}^{d}\\), and \\(\\mathbf{C}\\) represents its covariance matrix:\n\nStep 1: Find the eigenvalues and eigenvectors of \\(\\mathbf{C}\\). Let \\(\\{\\lambda_1, \\lambda_2, \\ldots, \\lambda_d \\}\\) be the eigenvalues arranged in descending order, and \\(\\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_d \\}\\) be their corresponding eigenvectors of unit length.\nStep 2: Calculate \\(k\\), the number of top eigenvalues and eigenvectors required, based on the desired variance to be covered.\nStep 3: Project the data onto the eigenvectors and obtain the desired representation as a linear combination of these projections.\n\n\n\n\nThe dataset depicted in the diagram has two principal components: the green vector represents the first PC, whereas the red vector corresponds to the second PC.\n\n\nIn essence, PCA is a dimensionality reduction technique that identifies feature combinations that are de-correlated (independent of each other)."
  },
  {
    "objectID": "pages/Wk04.html",
    "href": "pages/Wk04.html",
    "title": "Unsupervised Learning - Estimation - Recap of MLE + Bayesian estimation, Gaussian Mixture Model - EM algorithm",
    "section": "",
    "text": "Note\n\n\n\nFeedback/Correction: Click here!.\nPDF Link: Click here!"
  },
  {
    "objectID": "pages/Wk04.html#fishers-principle-of-maximum-likelihood",
    "href": "pages/Wk04.html#fishers-principle-of-maximum-likelihood",
    "title": "Unsupervised Learning - Estimation - Recap of MLE + Bayesian estimation, Gaussian Mixture Model - EM algorithm",
    "section": "Fisher’s Principle of Maximum Likelihood",
    "text": "Fisher’s Principle of Maximum Likelihood\nFisher’s principle of maximum likelihood is a statistical method used to estimate parameters of a statistical model by selecting values that maximize the likelihood function. This function quantifies how well the model fits the observed data."
  },
  {
    "objectID": "pages/Wk04.html#likelihood-estimation-for-bernoulli-distributions",
    "href": "pages/Wk04.html#likelihood-estimation-for-bernoulli-distributions",
    "title": "Unsupervised Learning - Estimation - Recap of MLE + Bayesian estimation, Gaussian Mixture Model - EM algorithm",
    "section": "Likelihood Estimation for Bernoulli Distributions",
    "text": "Likelihood Estimation for Bernoulli Distributions\nApplying the likelihood function on the aforementioned dataset, we obtain: \\[\\begin{align*}\n\\mathcal{L}(p;\\{\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n\\}) &= P(\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n;p)\\\\\n&= p(\\mathbf{x}_1;p)p(\\mathbf{x}_2;p)\\ldots p(\\mathbf{x}_n;p) \\\\\n&=\\prod _{i=1} ^n {p^{\\mathbf{x}_i}(1-p)^{1-\\mathbf{x}_i}}\n\\end{align*}\\] \\[\\begin{align*}\n\\therefore \\log(\\mathcal{L}(p;\\{\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n\\})) &=\\underset{p} {\\arg \\max}\\log \\left ( \\prod _{i=1} ^n {p^{\\mathbf{x}_i}(1-p)^{1-\\mathbf{x}_i}} \\right ) \\\\\n\\text{Differentiating wrt $p$, we get}\\\\\n\\therefore \\hat{p}_{\\text{ML}} &= \\frac{1}{n}\\sum _{i=1} ^n \\mathbf{x}_i\n\\end{align*}\\]"
  },
  {
    "objectID": "pages/Wk04.html#likelihood-estimation-for-gaussian-distributions",
    "href": "pages/Wk04.html#likelihood-estimation-for-gaussian-distributions",
    "title": "Unsupervised Learning - Estimation - Recap of MLE + Bayesian estimation, Gaussian Mixture Model - EM algorithm",
    "section": "Likelihood Estimation for Gaussian Distributions",
    "text": "Likelihood Estimation for Gaussian Distributions\nLet \\(\\{\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n\\}\\) be a dataset where \\(\\mathbf{x}_i \\sim \\mathcal{N}(\\boldsymbol{\\mu},\\boldsymbol{\\sigma}^2)\\). We assume that the data points are independent and identically distributed.\n\\[\\begin{align*}\n\\mathcal{L}(\\boldsymbol{\\mu}, \\boldsymbol{\\sigma}^2;\\{\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n\\}) &= f_{\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n}(\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n;\\boldsymbol{\\mu}, \\boldsymbol{\\sigma}^2) \\\\\n&=\\prod _{i=1} ^n  f_{\\mathbf{x}_i}(\\mathbf{x}_i;\\boldsymbol{\\mu}, \\boldsymbol{\\sigma}^2) \\\\\n&=\\prod _{i=1} ^n \\left [ \\frac{1}{\\sqrt{2\\pi}\\boldsymbol{\\sigma}} e^{\\frac{-(\\mathbf{x}_i-\\boldsymbol{\\mu})^2}{2\\boldsymbol{\\sigma}^2}} \\right ] \\\\\n\\therefore \\log(\\mathcal{L}(p;\\{\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n\\})) &= \\sum _{i=1} ^n \\left[ \\log \\left (\\frac{1}{\\sqrt{2\\pi}\\boldsymbol{\\sigma}}  \\right ) - \\frac{(\\mathbf{x}_i-\\boldsymbol{\\mu})^2}{2\\boldsymbol{\\sigma}^2} \\right] \\\\\n\\end{align*}\\] \\[\n\\text{By differentiating with respect to $\\boldsymbol{\\mu}$ and $\\boldsymbol{\\sigma}$, we get}\n\\] \\[\\begin{align*}\n\\hat{\\boldsymbol{\\mu}}_{\\text{ML}} &= \\frac{1}{n}\\sum _{i=1} ^n \\mathbf{x}_i \\\\\n\\hat{\\boldsymbol{\\sigma}^2}_{\\text{ML}} &= \\frac{1}{n}\\sum _{i=1} ^n (\\mathbf{x}_i-\\boldsymbol{\\mu})^T(\\mathbf{x}_i-\\boldsymbol{\\mu})\n\\end{align*}\\]"
  },
  {
    "objectID": "pages/Wk04.html#bayesian-estimation-for-a-bernoulli-distribution",
    "href": "pages/Wk04.html#bayesian-estimation-for-a-bernoulli-distribution",
    "title": "Unsupervised Learning - Estimation - Recap of MLE + Bayesian estimation, Gaussian Mixture Model - EM algorithm",
    "section": "Bayesian Estimation for a Bernoulli Distribution",
    "text": "Bayesian Estimation for a Bernoulli Distribution\nLet \\(\\{x_1, x_2, \\ldots, x_n\\}\\) be a dataset where \\(x_i \\in \\{0,1\\}\\) with parameter \\(\\theta\\). What distribution can be suitable for \\(P(\\theta)\\)?\nA commonly used distribution for priors is the Beta Distribution. \\[\nf(p;\\alpha,\\beta) = \\frac{p^{\\alpha-1}(1-p)^{\\beta-1}}{z} \\hspace{2em} \\forall p \\in [0,1] \\\\\n\\] \\[\n\\text{where $z$ is a normalizing factor}\n\\]\nHence, utilizing the Beta Distribution as the Prior, we obtain, \\[\\begin{align*}\nP(\\theta|\\{x_1, x_2, \\ldots, x_n\\}) &\\propto P(\\theta|\\{x_1, x_2, \\ldots, x_n\\})*P(\\theta) \\\\\nf_{\\theta|\\{x_1, x_2, \\ldots, x_n\\}}(p) &\\propto \\left [ \\prod _{i=1} ^n {p^{x_i}(1-p)^{1-x_i}} \\right ]*\\left [ p^{\\alpha-1}(1-p)^{\\beta-1} \\right ] \\\\\nf_{\\theta|\\{x_1, x_2, \\ldots, x_n\\}}(p) &\\propto p^{\\sum _{i=1} ^n x_i + \\alpha - 1}(1-p)^{\\sum _{i=1} ^n(1-x_i) + \\beta - 1}\n\\end{align*}\\] i.e. we obtain, \\[\n\\text{BETA PRIOR }(\\alpha, \\beta) \\xrightarrow[Bernoulli]{\\{x_1, x_2, \\ldots, x_n\\}} \\text{BETA POSTERIOR }(\\alpha + n_h, \\beta + n_t)\n\\] \\[\n\\therefore \\hat{p_{\\text{ML}}} = \\mathbb{E}[\\text{Posterior}]=\\mathbb{E}[\\text{Beta}(\\alpha +n_h, \\beta + n_t)]= \\frac{\\alpha + n_h}{\\alpha + n_h + \\beta + n_t}\n\\]"
  },
  {
    "objectID": "pages/Wk04.html#convexity-and-jensens-inequality",
    "href": "pages/Wk04.html#convexity-and-jensens-inequality",
    "title": "Unsupervised Learning - Estimation - Recap of MLE + Bayesian estimation, Gaussian Mixture Model - EM algorithm",
    "section": "Convexity and Jensen’s Inequality",
    "text": "Convexity and Jensen’s Inequality\nConvexity is a property of a function or set that implies a unique line segment can be drawn between any two points within the function or set. For a concave function, this property can be expressed as, \\[\nf \\left (\\sum _{k=1} ^K \\lambda_k a_k \\right ) \\ge \\sum _{k=1} ^K \\lambda_k f(a_k)\n\\] where \\[\n\\sum _{k=1} ^K \\lambda _k = 1\n\\] \\[\na_k \\text{ are points of the function}\n\\] This is also known as Jensen’s Inequality."
  },
  {
    "objectID": "pages/Wk06.html",
    "href": "pages/Wk06.html",
    "title": "Supervised Learning - Regression - Ridge/LASSO",
    "section": "",
    "text": "Note\n\n\n\nFeedback/Correction: Click here!.\nPDF Link: Click here!"
  },
  {
    "objectID": "pages/Wk06.html#introduction",
    "href": "pages/Wk06.html#introduction",
    "title": "Supervised Learning - Regression - Ridge/LASSO",
    "section": "Introduction",
    "text": "Introduction\nLinear regression is a widely used technique for modeling the relationship between a dependent variable and one or more independent variables. The maximum likelihood estimator (MLE) is commonly employed to estimate the parameters of a linear regression model. Here, we discuss the goodness of the MLE for linear regression, explore cross-validation techniques to minimize mean squared error (MSE), examine Bayesian modeling as an alternative approach, and finally, delve into ridge and lasso regression as methods to mitigate overfitting."
  },
  {
    "objectID": "pages/Wk08.html",
    "href": "pages/Wk08.html",
    "title": "Supervised Learning - Classification - Generative Models - Naive Bayes",
    "section": "",
    "text": "Note\n\n\n\nFeedback/Correction: Click here!.\nPDF Link: Click here!"
  },
  {
    "objectID": "pages/Wk08.html#alternate-generative-model",
    "href": "pages/Wk08.html#alternate-generative-model",
    "title": "Supervised Learning - Classification - Generative Models - Naive Bayes",
    "section": "Alternate Generative Model",
    "text": "Alternate Generative Model\nAn alternative generative model starts with the class conditional independence assumption, which is a common assumption in various machine learning algorithms. This assumption states that the features of an object are conditionally independent given its class label.\nLet us again consider the dataset \\(D=\\{(\\mathbf{x}_1, y_1), \\ldots, (\\mathbf{x}_n,y_n)\\}\\), with \\(\\mathbf{x}_i \\in \\{0, 1\\}^d\\) and \\(y_i \\in \\{0, 1\\}\\).\nThe general steps of the algorithm under this alternative model are as follows:\n\nDecide the labels by tossing a coin with \\(P(y_i=1)=p\\).\nDetermine the features for \\(\\mathbf{x}\\) given \\(y\\) using the following conditional probability: \\[\nP(\\mathbf{x} = [f_1, f_2, \\ldots, f_d]|y) = \\prod_{i=1}^d(p^{y_i}_i)^{f_i}(1-p^{y_i}_i)^{1-f_i}\n\\]\n\nThe parameters in this alternative model are as follows:\n\nParameter \\(\\hat{p}\\) to decide the label: 1\nParameters for \\(P(\\mathbf{x}|y=1)\\): \\(d\\)\nParameters for \\(P(\\mathbf{x}|y=0)\\): \\(d\\)\n\nThus, the total number of parameters is given by: \\[\\begin{align*}\n    & = 1 + d + d \\\\\n    & = 2d + 1\n\\end{align*}\\]\nThe parameters are estimated using Maximum Likelihood Estimation."
  },
  {
    "objectID": "pages/Wk08.html#prediction-using-the-parameters",
    "href": "pages/Wk08.html#prediction-using-the-parameters",
    "title": "Supervised Learning - Classification - Generative Models - Naive Bayes",
    "section": "Prediction using the parameters",
    "text": "Prediction using the parameters\nGiven \\(\\mathbf{x}^{test}\\in\\{0,1\\}^d\\), the prediction for \\(\\hat{y}^{test}\\) is done using the following criterion:\n\\[\nP(\\hat{y}^{test}=1|\\mathbf{x}^{test}) \\ge P(\\hat{y}^{test}=0|\\mathbf{x}^{test})\n\\]\nIf the above inequality holds, then \\(\\hat{y}^{test}=1\\), otherwise \\(\\hat{y}^{test}=0\\).\nUsing Bayes’ rule, we can express \\(P(\\hat{y}^{test}=1|\\mathbf{x}^{test})\\) and \\(P(\\hat{y}^{test}=0|\\mathbf{x}^{test})\\) as follows:\n\\[\\begin{align*}\nP(\\hat{y}^{test}=1|\\mathbf{x}^{test}) & = \\frac{P(\\mathbf{x}^{test}|\\hat{y}^{test}=1)*P(\\hat{y}^{test}=1)}{P(\\mathbf{x}^{test})} \\\\\nP(\\hat{y}^{test}=0|\\mathbf{x}^{test}) & = \\frac{P(\\mathbf{x}^{test}|\\hat{y}^{test}=0)*P(\\hat{y}^{test}=0)}{P(\\mathbf{x}^{test})}\n\\end{align*}\\]\nHowever, since we are only interested in the comparison of these probabilities, we can avoid calculating \\(P(\\mathbf{x}^{test})\\).\nBy solving for \\(P(\\mathbf{x}^{test}|\\hat{y}^{test}=1)*P(\\hat{y}^{test}=1)\\), we find:\n\\[\\begin{align*}\n&=P(\\mathbf{x}^{test} = [f_1, f_2, \\ldots, f_d]|y^{test}=1)*P(\\hat{y}^{test}=1) \\\\\n&=\\left(\\prod_{i=1}^d(\\hat{p}^1_i)^{f_i}(1-\\hat{p}^1_i)^{1-f_i}\\right)*\\hat{p}\n\\end{align*}\\]\nSimilarly, we can obtain \\(P(\\mathbf{x}^{test}|\\hat{y}^{test}=0)*P(\\hat{y}^{test}=0)\\).\nTherefore, we predict \\(\\hat{y}^{test}=1\\) if:\n\\[\n\\left(\\prod_{i=1}^d(\\hat{p}^1_i)^{f_i}(1-\\hat{p}^1_i)^{1-f_i}\\right)*\\hat{p} \\ge \\left(\\prod_{i=1}^d(\\hat{p}^0_i)^{f_i}(1-\\hat{p}^0_i)^{1-f_i}\\right)*(1-\\hat{p})\n\\]\nOtherwise, we predict \\(\\hat{y}^{test}=0\\).\nThe Naive Bayes algorithm employs two main techniques:\n\nThe Class Conditional Independence Assumption.\nUtilizing Bayes’ Rule.\n\nAs a result, this algorithm is commonly referred to as Naive Bayes.\nIn summary, Naive Bayes is a classification algorithm based on Bayes’ theorem, which assumes that the features are independent of each other given the class label. It estimates the conditional probabilities of features given the class and uses these probabilities to make predictions for new data. Despite its naive assumption, Naive Bayes has demonstrated good performance across various applications, particularly when dealing with high-dimensional data and limited training examples."
  },
  {
    "objectID": "pages/Wk08.html#pitfalls-of-naive-bayes",
    "href": "pages/Wk08.html#pitfalls-of-naive-bayes",
    "title": "Supervised Learning - Classification - Generative Models - Naive Bayes",
    "section": "Pitfalls of Naive Bayes",
    "text": "Pitfalls of Naive Bayes\nOne prominent issue with Naive Bayes is that if a feature is not observed in the training set but present in the testing set, the prediction probabilities for both classes become zero.\n\\[\\begin{align*}\nP(\\hat{y}^{test}=1|\\mathbf{x}^{test} = [f_1, f_2, \\ldots, f_d]) & \\propto \\left(\\prod_{i=1}^d(\\hat{p}^1_i)^{f_i}(1-\\hat{p}^1_i)^{1-f_i}\\right)*\\hat{p} \\\\\nP(\\hat{y}^{test}=0|\\mathbf{x}^{test} = [f_1, f_2, \\ldots, f_d]) & \\propto \\left(\\prod_{i=1}^d(\\hat{p}^0_i)^{f_i}(1-\\hat{p}^0_i)^{1-f_i}\\right)*(1-\\hat{p})\n\\end{align*}\\]\nIf any feature \\(f_i\\) was absent in the training set, it results in \\(\\hat{p}^1_i=\\hat{p}^0_i=0\\), leading to \\(P(\\hat{y}^{test}=0|\\mathbf{x}^{test})=P(\\hat{y}^{test}=1|\\mathbf{x}^{test})=0\\).\nA popular remedy for this issue is to introduce two “pseudo” data points with labels 1 and 0, respectively, into the dataset, where all their features are set to 1. This technique is also known as Laplace smoothing.\nIn brief, Laplace smoothing is a technique employed to address the zero-frequency problem in probabilistic models, particularly in text classification. It involves adding a small constant value to the count of each feature and the number of unique classes to avoid zero probability estimates, which can cause problems during model training and prediction. By incorporating this smoothing term, the model becomes more robust and better suited for handling unseen data."
  },
  {
    "objectID": "pages/Wk08.html#prediction-using-bayes-rule",
    "href": "pages/Wk08.html#prediction-using-bayes-rule",
    "title": "Supervised Learning - Classification - Generative Models - Naive Bayes",
    "section": "Prediction using Bayes’ Rule",
    "text": "Prediction using Bayes’ Rule\nPrediction is based on the following equation: \\[\nP(y_{test}=1|\\mathbf{x}_{test})\\propto P(\\mathbf{x}_{test}|y_{test})*P(y_{test})\n\\]\nWhere \\(P(\\mathbf{x}_{test}|y_{test})\\equiv f(\\mathbf{x}_{test};\\hat{\\boldsymbol{\\mu}}_{y_{test}}, \\hat{\\boldsymbol{\\Sigma}})\\) and \\(P(y_{test})\\equiv \\hat{p}\\).\nTo predict \\(y_{test}=1\\), we compare the probabilities:\n\\[\\begin{align*}\nf(\\mathbf{x}_{i} ;\\hat{\\boldsymbol{\\mu} }_{1} ,\\hat{\\boldsymbol{\\Sigma} }_{1} )\\hat{p} & \\geq f(\\mathbf{x}_{i} ;\\hat{\\boldsymbol{\\mu} }_{0} ,\\hat{\\boldsymbol{\\Sigma} }_{0} )(1-\\hat{p} )\\\\\ne^{-(\\mathbf{x}_{i} -\\hat{\\boldsymbol{\\mu} }_{1} )^{T}\\hat{\\boldsymbol{\\Sigma} }_{1}^{-1} (\\mathbf{x}_{i} -\\hat{\\boldsymbol{\\mu} }_{1} )}\\hat{p} & \\geq e^{-(\\mathbf{x}_{i} -\\hat{\\boldsymbol{\\mu} }_{0} )^{T}\\hat{\\boldsymbol{\\Sigma} }_{0}^{-1} (\\mathbf{x}_{i} -\\hat{\\boldsymbol{\\mu} }_{0} )} (1-\\hat{p} )\\\\\n-(\\mathbf{x}_{i} -\\hat{\\boldsymbol{\\mu} }_{1} )^{T}\\hat{\\boldsymbol{\\Sigma} }_{1}^{-1} (\\mathbf{x}_{i} -\\hat{\\boldsymbol{\\mu} }_{1} )+\\log (\\hat{p} ) & \\geq -(\\mathbf{x}_{i} -\\hat{\\boldsymbol{\\mu} }_{0} )^{T}\\hat{\\boldsymbol{\\Sigma} }_{0}^{-1} (\\mathbf{x}_{i} -\\hat{\\boldsymbol{\\mu} }_{0} )+\\log (1-\\hat{p} )\n\\end{align*}\\]\nThis inequality can be expressed as a linear decision function:\n\\[\n\\left( (\\hat{\\boldsymbol{\\mu}}_1-\\hat{\\boldsymbol{\\mu}}_0)^T\\hat{\\boldsymbol{\\Sigma}}^{-1} \\right)\\mathbf{x}_{test} + \\hat{\\boldsymbol{\\mu}}_0^T\\hat{\\boldsymbol{\\Sigma}}^{-1}\\hat{\\boldsymbol{\\mu}}_0 - \\hat{\\boldsymbol{\\mu}}_1^T\\hat{\\boldsymbol{\\Sigma}}^{-1}\\hat{\\boldsymbol{\\mu}}_1 + \\log(\\frac{1-\\hat{p}}{\\hat{p}}) \\ge 0\n\\]\nThus, the decision function of Gaussian Naive Bayes is linear."
  },
  {
    "objectID": "pages/Wk08.html#decision-boundaries-for-different-covariances",
    "href": "pages/Wk08.html#decision-boundaries-for-different-covariances",
    "title": "Supervised Learning - Classification - Generative Models - Naive Bayes",
    "section": "Decision Boundaries for Different Covariances",
    "text": "Decision Boundaries for Different Covariances\n\nWhen the covariance matrices are equal for both classes: As previously discussed, the decision boundary is linear.\n\n\n\n\nWhen the covariance matrices are equal for both classes\n\n\n\nWhen the covariance matrices are Identity matrices for both classes: The decision boundary is both linear and the perpendicular bisector of the line drawn from \\(\\hat{\\boldsymbol{\\mu}}_1\\) to \\(\\hat{\\boldsymbol{\\mu}}_0\\).\n\n\n\n\nWhen the covariance matrices are Identity matrices for both classes\n\n\n\nWhen the covariance matrices are not equal for both classes: Let \\(\\hat{\\boldsymbol{\\Sigma}}_1\\) and \\(\\hat{\\boldsymbol{\\Sigma}}_0\\) be the covariance matrices for classes 1 and 0, respectively. They are given by, \\[\\begin{align*}\n\\hat{\\boldsymbol{\\Sigma}}_1 &= \\frac{\\displaystyle \\sum_{i=1}^n(\\mathbb{1}(y_i=1)*\\mathbf{x}_i-\\hat{\\boldsymbol{\\mu}}_1)(\\mathbb{1}(y_i=1)*\\mathbf{x}_i-\\hat{\\boldsymbol{\\mu}}_1)^T}{\\displaystyle \\sum_{i=1}^n\\mathbb{1}(y_i=1)} \\\\\n\\hat{\\boldsymbol{\\Sigma}}_0 &= \\frac{\\displaystyle \\sum_{i=1}^n(\\mathbb{1}(y_i=0)*\\mathbf{x}_i-\\hat{\\boldsymbol{\\mu}}_0)(\\mathbb{1}(y_i=0)*\\mathbf{x}_i-\\hat{\\boldsymbol{\\mu}}_0)^T}{\\displaystyle \\sum_{i=1}^n\\mathbb{1}(y_i=0)}\n\\end{align*}\\] To predict \\(y_{test}=1\\), we compare the probabilities: \\[\\begin{align*}\nf(\\mathbf{x}_{test};\\hat{\\boldsymbol{\\mu}}_1, \\hat{\\boldsymbol{\\Sigma}}_1)*\\hat{p}&\\ge f(\\mathbf{x}_{test};\\hat{\\boldsymbol{\\mu}}_0, \\hat{\\boldsymbol{\\Sigma}}_0)*(1-\\hat{p}) \\\\\ne^{-(\\mathbf{x}_{test}-\\hat{\\boldsymbol{\\mu}}_1)^T\\hat{\\boldsymbol{\\Sigma}}_1(\\mathbf{x}_{test}-\\hat{\\boldsymbol{\\mu}}_1)}*\\hat{p}&\\ge e^{-(\\mathbf{x}_{test}-\\hat{\\boldsymbol{\\mu}}_0)^T\\hat{\\boldsymbol{\\Sigma}}_1(\\mathbf{x}_{test}-\\hat{\\boldsymbol{\\mu}}_0)}*(1-\\hat{p}) \\\\\n-(\\mathbf{x}_{test}-\\hat{\\boldsymbol{\\mu}}_1)^T\\hat{\\boldsymbol{\\Sigma}}_1(\\mathbf{x}_{test}-\\hat{\\boldsymbol{\\mu}}_1)+\\log(\\hat{p})&\\ge -(\\mathbf{x}_{test}-\\hat{\\boldsymbol{\\mu}}_0)^T\\hat{\\boldsymbol{\\Sigma}}_0(\\mathbf{x}_{test}-\\hat{\\boldsymbol{\\mu}}_0) + \\log(1-\\hat{p}) \\\\\n\\end{align*}\\] This inequality leads to a quadratic decision function: \\[\n\\mathbf{x}_{test}^T(\\hat{\\boldsymbol{\\Sigma}}_1^{-1}-\\hat{\\boldsymbol{\\Sigma}}_0^{-1})\\mathbf{x}_{test}-2(\\hat{\\boldsymbol{\\mu}}_1^T\\hat{\\boldsymbol{\\Sigma}}_1^{-1}-\\hat{\\boldsymbol{\\mu}}_0^T\\hat{\\boldsymbol{\\Sigma}}_0^{-1})\\mathbf{x}_{test}+(\\hat{\\boldsymbol{\\mu}}_0^T\\hat{\\boldsymbol{\\Sigma}}_0^{-1}\\hat{\\boldsymbol{\\mu}}_0-\\hat{\\boldsymbol{\\mu}}_1^T\\hat{\\boldsymbol{\\Sigma}}_1^{-1}\\hat{\\boldsymbol{\\mu}}_1) + \\log(\\frac{1-\\hat{p}}{\\hat{p}}) \\ge 0\n\\] Hence, the decision boundary is a quadratic function when the covariance matrices are not equal for both classes.\n\n\n\n\nWhen the covariance matrices are not equal for both classes"
  },
  {
    "objectID": "pages/Wk02.html",
    "href": "pages/Wk02.html",
    "title": "Unsupervised Learning - Representation learning - Kernel PCA",
    "section": "",
    "text": "Note\n\n\n\nFeedback/Correction: Click here!.\nPDF Link: Click here!"
  },
  {
    "objectID": "pages/Wk02.html#transforming-features",
    "href": "pages/Wk02.html#transforming-features",
    "title": "Unsupervised Learning - Representation learning - Kernel PCA",
    "section": "Transforming Features",
    "text": "Transforming Features\nTo address non-linear relationships, we propose mapping the dataset to higher dimensions as follows: \\[\n\\mathbf{x} \\to \\phi(\\mathbf{x}) \\quad \\mathbb{R}^d \\to \\mathbb{R}^D \\quad \\text{where } [D &gt;&gt; d]\n\\]\nTo compute \\(D\\), let \\(\\mathbf{x}=\\left [ \\begin{array} {cc}  f_1 & f_2 \\end{array} \\right ]\\) represent features of a dataset containing datapoints lying on a second-degree curve in a two-dimensional space.\nTo convert it from quadratic to linear, we map the features to: \\[\n\\phi(\\mathbf{x})=\\left [\n\\begin{array} {cccccc}\n    1 & f_1^2 & f_2^2 & f_1f_2 & f_1 & f_2\n\\end{array}\n\\right ]\n\\]\nMapping \\(d\\) features to the polynomial power \\(p\\) results in \\(^{d+p} C_d\\) new features.\nHowever, it is essential to note that finding \\(\\phi(\\mathbf{x})\\) may be computationally demanding.\nTo overcome this issue, we present the solution in the subsequent section."
  },
  {
    "objectID": "pages/Wk02.html#kernel-functions",
    "href": "pages/Wk02.html#kernel-functions",
    "title": "Unsupervised Learning - Representation learning - Kernel PCA",
    "section": "Kernel Functions",
    "text": "Kernel Functions\nA function \\(k: \\mathbb{R}^d \\times \\mathbb{R}^d \\to \\mathbb{R}\\) is considered a “valid” Kernel Function if it maps data points to the real numbers.\nProof of a “Valid” Kernel: There are two methods to establish the validity of a kernel:\n\nMethod 1: Explicitly exhibit the mapping to \\(\\phi\\), which may be challenging in certain cases.\nMethod 2: Utilize Mercer’s Theorem, which states that \\(k: \\mathbb{R}^d \\times \\mathbb{R}^d \\to \\mathbb{R}\\) is a valid kernel if and only if:\n\n\\(k\\) is symmetric, i.e., \\(k(\\mathbf{x},\\mathbf{x}') = k(\\mathbf{x}',\\mathbf{x})\\)\nFor any dataset \\(\\{\\mathbf{x}_1,\\mathbf{x}_2,\\ldots,\\mathbf{x}_n\\}\\), the matrix \\(\\mathbf{K} \\in \\mathbb{R}^{n \\times n}\\), where \\(\\mathbf{K}_{ij} = k(\\mathbf{x}_i,\\mathbf{x}_j)\\), is Positive Semi-Definite.\n\n\nTwo popular kernel functions are:\n\nPolynomial Kernel: \\(k(\\mathbf{x},\\mathbf{x}') = (\\mathbf{x}^T\\mathbf{x} + 1)^p\\)\nRadial Basis Function Kernel or Gaussian Kernel: \\(k(\\mathbf{x},\\mathbf{x}') = \\exp\\left(\\displaystyle-\\frac{\\|\\mathbf{x}-\\mathbf{x}'\\|^2}{2\\sigma^2}\\right)\\)"
  },
  {
    "objectID": "pages/Not01.html",
    "href": "pages/Not01.html",
    "title": "Week 1: Standard PCA",
    "section": "",
    "text": "Colab Link: Click here!"
  },
  {
    "objectID": "pages/Not01.html#steps-involved-in-pca",
    "href": "pages/Not01.html#steps-involved-in-pca",
    "title": "Week 1: Standard PCA",
    "section": "Steps involved in PCA",
    "text": "Steps involved in PCA\n\nStep 1: Center the dataset\nStep 2: Calculate the covariance matrix of the centered data\nStep 3: Compute the eigenvectors and eigenvalues\nStep 4: Sort the eigenvalues in descending order and choose the top k eigenvectors corresponding to the highest eigenvalues\nStep 5: Transform the original data by multiplying it with the selected eigenvectors(PCs) to obtain a lower-dimensional representation.\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "pages/Not01.html#observe-the-dataset",
    "href": "pages/Not01.html#observe-the-dataset",
    "title": "Week 1: Standard PCA",
    "section": "Observe the dataset",
    "text": "Observe the dataset\nLet’s take a dataset \\displaystyle \\mathbf{X} of shape (d,n) where\n\nd: no. of features\nn: no. of datapoints\n\n\nX = np.array([(4,1),(5,4),(6,3),(7,4),(2,-1),(-1,-2),(0,-3),(-1,-4)]).T\n\n\nplt.scatter(X[0,:],X[1,:])\nplt.axhline(0,color='k')\nplt.axvline(0,color='k')\n\nx_mean = X.mean(axis=1)\n\nplt.scatter(x_mean[0],x_mean[1],color='r')\nplt.grid()\nplt.show()"
  },
  {
    "objectID": "pages/Not01.html#center-the-dataset",
    "href": "pages/Not01.html#center-the-dataset",
    "title": "Week 1: Standard PCA",
    "section": "Center the dataset",
    "text": "Center the dataset\n\ndef center(X):\n    return X - X.mean(axis = 1).reshape(2,1)\n\nd, n = X.shape\nX_centered = center(X)\n\n\nX_centered\n\narray([[ 1.25,  2.25,  3.25,  4.25, -0.75, -3.75, -2.75, -3.75],\n       [ 0.75,  3.75,  2.75,  3.75, -1.25, -2.25, -3.25, -4.25]])\n\n\n\nimport matplotlib.pyplot as plt\n\nplt.scatter(X_centered[0,:],X_centered[1,:])\nplt.axhline(0,color='k')\nplt.axvline(0,color='k')\n\nc_mean = X_centered.mean(axis=1)\n\nplt.scatter(c_mean[0],c_mean[1],color='r')\nplt.grid()\nplt.show()\n\n\n\n\n\nX_centered.mean(axis=1)\n\n\n#Compare the two graphs\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.scatter(X[0,:],X[1,:])\nplt.axhline(0,color='k')\nplt.axvline(0,color='k')\n\nx_mean = X.mean(axis=1)\n\nplt.scatter(x_mean[0],x_mean[1],color='r')\nplt.grid()\nplt.title(\"Before Centering\")\n\n\n\nplt.subplot(1, 2, 2)\nplt.scatter(X_centered[0,:],X_centered[1,:])\nplt.axhline(0,color='k')\nplt.axvline(0,color='k')\n\nc_mean = X_centered.mean(axis=1)\n\nplt.scatter(c_mean[0],c_mean[1],color='r')\nplt.grid()\nplt.title(\"After Centering\")\n\nplt.show()"
  },
  {
    "objectID": "pages/Not01.html#find-the-covariance-matrix",
    "href": "pages/Not01.html#find-the-covariance-matrix",
    "title": "Week 1: Standard PCA",
    "section": "Find the covariance matrix",
    "text": "Find the covariance matrix\nThe covariance matrix is given by \\mathbf{C} \\ =\\ \\frac{1}{n}\\sum \\limits_{i\\ =\\ 1}^{n} \\mathbf {x}_{i}\\mathbf {x}_{i}^{T} \\ =\\ \\frac{1}{n}\\mathbf{XX}^{T}\n\ndef covariance(X):\n    return X @ X.T / X.shape[1]\n\nC = covariance(X_centered)\nd = C.shape[0]\nprint(C)\n\n[[8.9375 8.5625]\n [8.5625 8.9375]]"
  },
  {
    "objectID": "pages/Not01.html#compute-the-principal-components",
    "href": "pages/Not01.html#compute-the-principal-components",
    "title": "Week 1: Standard PCA",
    "section": "Compute the principal components",
    "text": "Compute the principal components\nThe k^{th} principal component is given by the eigenvector corresponding to the k^{th} largest eigenvalue\n\ndef compute_pc(C):\n    d = C.shape[0]\n    eigval, eigvec = np.linalg.eigh(C)\n    w_1, w_2 = eigvec[:, -1], eigvec[:, -2]\n    return w_1, w_2\n\nw_1, w_2 = compute_pc(C)\n\nw_1 = w_1.reshape(w_1.shape[0],1)\nw_2 = w_2.reshape(w_2.shape[0],1)\n\nprint(w_1)\nprint(w_2)\n\n[[0.70710678]\n [0.70710678]]\n[[-0.70710678]\n [ 0.70710678]]"
  },
  {
    "objectID": "pages/Not01.html#reconstruction-using-the-two-pcs",
    "href": "pages/Not01.html#reconstruction-using-the-two-pcs",
    "title": "Week 1: Standard PCA",
    "section": "Reconstruction using the two PCs",
    "text": "Reconstruction using the two PCs\nThe scalar projection of the dataset on k^{th} PC is given by \\mathbf{X}_{\\text{centered}}^{T} \\ .\\ \\mathbf{w_{k}}\nThe vector projection of the dataset on k^{th} PC is given by \\mathbf{w_{k} .(\\mathbf{X}_{\\text{centered}}^{T} \\ .\\ \\mathbf{w_{k}})^{T}}\n\n#Since the points are 2-dimensional, by combining the projection on the two PCs, we get back the centered dataset\nw_1 @ (X_centered.T @ w_1).reshape(1,n) + w_2 @ (X_centered.T @ w_2).reshape(1,n)\n\narray([[ 1.25,  2.25,  3.25,  4.25, -0.75, -3.75, -2.75, -3.75],\n       [ 0.75,  3.75,  2.75,  3.75, -1.25, -2.25, -3.25, -4.25]])\n\n\nLet us see the reconstruction error for a point along the first principal component\n\n#The reconstruction error by the first PC is given by\nX_1 = np.array((1.25,0.75))\np_1 = X_centered[:,0]\n\n#Let the reconstruction of the first point using first PC be given by\np_2 = w_1 @ (X_1 @ w_1)\nprint(\"The reconstruction error with first PC is \"+ str(np.sum(np.square(p_1 - p_2))))\n\nThe reconstruction error with first PC is 0.125\n\n\nThe reconstruction error for the entire dataset along the first principal component will be\n\n#Reconstruction error for each point when considering the first principal component\nrec_error_1 = np.square(np.linalg.norm(X_centered[:,] - (w_1 @ (X_centered.T @ w_1).reshape(1,n))[:,], axis=0))\nprint(rec_error_1)\n\n[0.125 1.125 0.125 0.125 0.125 1.125 0.125 0.125]\n\n\n\n#Total reconstruction error when considering first principal component\n\nprint(\"The reconstruction error along the first principal component is \"+str(np.round((rec_error_1).mean(),4)))\n\nThe reconstruction error along the first principal component is 0.375\n\n\nThe reconstruction error for the entire dataset along \\mathbf{w}_r will be\n\nw_r = np.array([0,1]).reshape(-1,1)\n\n\n#Reconstruction error for each point when considering the vector w_r\nrec_error_r = np.square(np.linalg.norm(X_centered[:,] - (w_r @ (X_centered.T @ w_r).reshape(1,n))[:,], axis=0))\nprint(rec_error_r)\n\n[ 1.5625  5.0625 10.5625 18.0625  0.5625 14.0625  7.5625 14.0625]\n\n\n\nprint(\"The reconstruction error along w_r is \"+str((rec_error_r).mean()))\n\nThe reconstruction error along w_r is 8.9375\n\n\nFor our dataset we can see that the reconstruction error is much lower along the first principal component as compared to the vector \\mathbf{w}_r"
  },
  {
    "objectID": "pages/Not01.html#finding-the-optimal-value-of-k",
    "href": "pages/Not01.html#finding-the-optimal-value-of-k",
    "title": "Week 1: Standard PCA",
    "section": "Finding the optimal value of K",
    "text": "Finding the optimal value of K\n\n#Sort the eigenvalues in descending order\neigval, eigvec = np.linalg.eigh(C)\neigval = eigval[::-1]\n\n\ndef var_thresh(k):\n  tot_var = 0\n  req_var = 0\n  for x in eigval:\n    tot_var += x\n  for y in range(k):\n    req_var += eigval[y]\n\n  return (req_var/tot_var)\n\nfor i in range(d+1):\n  print(\"The explained variance when K is \"+str(i)+\" is \"+str(np.round(var_thresh(i),4)))\n\nThe explained variance when K is 0 is 0.0\nThe explained variance when K is 1 is 0.979\nThe explained variance when K is 2 is 1.0"
  },
  {
    "objectID": "pages/Not01.html#pca-on-a-real-world-dataset",
    "href": "pages/Not01.html#pca-on-a-real-world-dataset",
    "title": "Week 1: Standard PCA",
    "section": "PCA on a real-world Dataset",
    "text": "PCA on a real-world Dataset\nWe will be working with a subset of the MNIST dataset. The cell below generates the data-matrix \\mathbf{X}, which is of shape (d, n), where n denotes the number of samples and d denotes the number of features.\n\n##### DATASET GENERATION #####\nimport numpy as np\nfrom keras.datasets import mnist\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\nX = X_train[y_train == 2][: 100].reshape(-1, 28 * 28).T\ntest_image = X_test[y_test == 2][0].reshape(28 * 28)\n\n\n# Observe the first image in the dataset\nimport matplotlib.pyplot as plt\nimg = X[:,0].reshape(28, 28)\nplt.imshow(img, cmap = 'gray');\n\n\n\n\nWe need to center the dataset \\mathbf{X} around its mean. Let us call this centered dataset \\mathbf{X}^{\\prime}.\n\ndef center(X):\n    return X - X.mean(axis = 1).reshape(-1,1)\n\nd, n = X.shape\nX_prime = center(X)\n\nCompute the covariance matrix \\mathbf{C} of the centered dataset.\n\ndef covariance(X):\n    return X @ X.T / X.shape[1]\n\nC = covariance(X_prime)\n\nCompute the first and second principal components of the dataset, \\mathbf{w}_1 and \\mathbf{w}_2.\n\ndef compute_pc(C):\n    d = C.shape[0]\n    eigval, eigvec = np.linalg.eigh(C)\n    w_1, w_2 = eigvec[:, -1], eigvec[:, -2]\n    return w_1, w_2\n\nw_1, w_2 = compute_pc(C)\n\nVisualize the first principal component as an image.\n\nw_1_image = w_1.reshape(28, 28)\nplt.imshow(w_1_image, cmap = 'gray')\n\n&lt;matplotlib.image.AxesImage at 0x7f6ea02093f0&gt;\n\n\n\n\n\nGiven a test_image, visualize the proxies by reconstructing it using the top k principal components. Consider four values of k; values of k for which the top-k principal components explain:\n\n20% of the variance\n50% of the variance\n80% of the variance\n95% of the variance\n\n\ndef reconstruct(C, test_image, thresh):\n    eigval, eigvec = np.linalg.eigh(C)\n    eigval = list(reversed(eigval))\n    tot = sum(eigval)\n    K = len(eigval)\n    for k in range(len(eigval)):\n        if sum(eigval[: k + 1]) / tot &gt;= thresh:\n            K = k + 1\n            break\n    W = eigvec[:, -K: ]\n    coeff = test_image @ W\n    return W @ coeff\n\nplt.figure(figsize=(20,20))\n# 0.20\nrecon_image = reconstruct(C, test_image, 0.20)\nplt.subplot(1, 5, 1)\nplt.imshow(recon_image.reshape(28, 28))\nplt.title(\"Variance covered = 20%\")\n# 0.5\nrecon_image = reconstruct(C, test_image, 0.50)\nplt.subplot(1, 5, 2)\nplt.imshow(recon_image.reshape(28, 28))\nplt.title(\"Variance covered = 50%\")\n# 0.80\nrecon_image = reconstruct(C, test_image, 0.80)\nplt.subplot(1, 5, 3)\nplt.imshow(recon_image.reshape(28, 28))\nplt.title(\"Variance covered = 80%\")\n# 0.95\nplt.subplot(1, 5, 4)\nrecon_image = reconstruct(C, test_image, 0.95)\nplt.imshow(recon_image.reshape(28, 28))\nplt.title(\"Variance covered = 95%\")\n# Original mean subtracted image\ntest_image = np.float64(test_image) - X.mean(axis = 1)\nplt.subplot(1, 5, 5)\nplt.imshow(test_image.reshape(28, 28))\nplt.title(\"Test Image\")\n\nText(0.5, 1.0, 'Test Image')"
  },
  {
    "objectID": "pages/Not03.html",
    "href": "pages/Not03.html",
    "title": "Week 3: K-means",
    "section": "",
    "text": "Colab Link: Click here!"
  },
  {
    "objectID": "pages/Not03.html#problem-definition",
    "href": "pages/Not03.html#problem-definition",
    "title": "Week 3: K-means",
    "section": "Problem Definition",
    "text": "Problem Definition\nIn this context, the objective becomes the following:\n\n\\underset{z \\in S}{\\text{min }} \\sum_{i=1}^{n}||x_i - μ_{z_i}||^2_2\n\nWhere: - x_i denotes the i’th datapoint - z_i denotes the cluster indicator of x_i - μ_{z_i} denotes the mean of the cluster with indicator z_i - S denotes the set of all possible cluster assignments. Note that S is finite\n\ndef obj(X, cluster_centers):\n  return sum([np.min([np.linalg.norm(x_i - cluster_center)**2 for cluster_center in cluster_centers]) for x_i in X])"
  },
  {
    "objectID": "pages/Not03.html#algorithm-strategy",
    "href": "pages/Not03.html#algorithm-strategy",
    "title": "Week 3: K-means",
    "section": "Algorithm Strategy",
    "text": "Algorithm Strategy\n\nInitialization Step : Assign random datapoints from the dataset as the cluster centers\nCluster Assignment Step : For every datapoint x_i, assign a cluster indicator z_i = \\underset{j \\in [1, 2, ..., n]}{\\text{min }} ||x_i - μ_j||^2_2\nRecompute cluster centers : For every cluster indicator j \\in [1, 2, ..., n] recompute μ_j = \\frac{\\sum_{i=1}^{n}x_i \\cdot \\mathbf{1}(z_i=j)}{\\sum_{i=1}^{n} \\mathbf{1}(z_i=j)}\nRepeat steps 2 and 3 until convergence.\n\nConvergence in accordance to the objective is established, since the following can be shown: - The set of all possible cluster assignments S is finite. - The objective function value strictly decreases after every iteration of Lloyd’s.\nThe initialization for K-Means can be done in smarter ways than a random initialization - which improve the chance of lloyd’s converging to a good cluster assignment; with a lesser number of iterations.\nIt is important to note that the final assignment need not necessarily be the best answer (global optima) to the objective function, but it is good enough in practice."
  },
  {
    "objectID": "pages/Not03.html#initialization-step",
    "href": "pages/Not03.html#initialization-step",
    "title": "Week 3: K-means",
    "section": "Initialization Step",
    "text": "Initialization Step\nn points from the dataset are randomly chosen as cluster centers, where n is the number of clusters - a hyperparameter\n\nn = 3\n# cluster_centers = X[np.random.choice(len(X), 3)]\ncluster_centers = X[[70, 85, 80]]"
  },
  {
    "objectID": "pages/Not03.html#cluster-assignment-step",
    "href": "pages/Not03.html#cluster-assignment-step",
    "title": "Week 3: K-means",
    "section": "Cluster Assignment Step",
    "text": "Cluster Assignment Step\nFor every datapoint, the cluster indicator whose center is closest to the datapoint is assigned as its cluster.\n\nfrom scipy.spatial import Voronoi, voronoi_plot_2d\n\ndef cluster_assignment(X, cluster_centers):\n  z = np.zeros(X.shape[0])\n  for i in range(X.shape[0]):\n    z[i] = np.argmin([np.linalg.norm(X[i] - cluster_center) for cluster_center in cluster_centers])\n  return z\n\nz = cluster_assignment(X, cluster_centers)\n\nfig, (ax) = plt.subplots(1, 1)\nfig.set_size_inches(5, 5)\n\nax.scatter(X[:, 0], X[:, 1], c=z, s=10);\nax.scatter(cluster_centers[:, 0], cluster_centers[:, 1], marker = 'x', s = 100, color = 'red', linewidth=1)\n\nvor = Voronoi(cluster_centers)\nvoronoi_plot_2d(vor, ax=ax, show_points=False, show_vertices=False);\n\nax.axis('equal');\n\n\n\n\nFor every cluster, there is a corresponding interesction of half-spaces - called Voronoi regions. The K-Means algorithm, equivalently, is trying to find the most optimal Voronoi partition of the space, that minimizes the objective function."
  },
  {
    "objectID": "pages/Not03.html#recompute-meanscluster-centers",
    "href": "pages/Not03.html#recompute-meanscluster-centers",
    "title": "Week 3: K-means",
    "section": "Recompute Means/Cluster Centers",
    "text": "Recompute Means/Cluster Centers\nFor every cluster, the cluster center is updated to the mean of the points in the cluster.\n\ndef recompute_clusters(X, z):\n  cluster_centers = np.array([np.mean(X[z == i], axis = 0) for i in range(n)])\n  return cluster_centers"
  },
  {
    "objectID": "pages/Not03.html#generate-a-complex-dataset-with-6-clusters",
    "href": "pages/Not03.html#generate-a-complex-dataset-with-6-clusters",
    "title": "Week 3: K-means",
    "section": "Generate a complex dataset with 6 clusters",
    "text": "Generate a complex dataset with 6 clusters\nWe make use of the convienient make_blobs data generator from the scikit-learn ibrary.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.datasets import make_blobs\n\ncen = [(-3, 3), (-2, 1), (0, 0), (1, 2), (0, -2), (3, -1)]\nX, ideal_z = make_blobs(n_samples=1000, centers=cen, n_features=2, cluster_std=0.3, random_state=13, center_box=(-3, 3))\n\nfig, ax = plt.subplots()\nax.scatter(X[:, 0], X[:, 1], s=10, c=ideal_z)\nvor = Voronoi(cen)\nvoronoi_plot_2d(vor, ax=ax, show_points=False, show_vertices=False);\nfig.set_size_inches(5, 4.5)\nax.axis([-5, 5, -4, 5])\n\n(-5.0, 5.0, -4.0, 5.0)\n\n\n\n\n\n\n# the make_blobs generator returns the optimal/good cluster assignment along with the dataset\n# this function checks whether the result from lloyd's is equivalent to the optimal assignment\n\ndef ideal_check(ideal, obtained):\n  mapping = dict([(i, -1) for i in range(n)])\n\n  for i in range(len(ideal)):\n    if mapping[ideal[i]] == -1:\n      mapping[ideal[i]] = obtained[i]\n    elif mapping[ideal[i]] != obtained[i]:\n      return False\n\n  return True"
  },
  {
    "objectID": "pages/Not03.html#lloyds-algorithm-definition",
    "href": "pages/Not03.html#lloyds-algorithm-definition",
    "title": "Week 3: K-means",
    "section": "Lloyd’s Algorithm Definition",
    "text": "Lloyd’s Algorithm Definition\n\nfrom IPython.display import HTML\nfrom matplotlib import animation\n\ndef lloyds(cluster_centers, X, z, artists = [], animate=True, fig=None, ax=None, ax1=None, n_iter=0, n = len(cen)):\n  loss = []\n  if fig is None and animate:\n    fig, (ax, ax1) = plt.subplots(1, 2, figsize=(10, 4.15))\n    title = ax.set_title('')\n    artists = []\n\n  if animate:\n      frame = []\n      frame.append(ax.scatter(X[:, 0], X[:, 1], c=z, s=10))\n      frame.append(ax.scatter(cluster_centers[:, 0], cluster_centers[:, 1], marker = 'x', s = 100, color = 'red', linewidth=1))\n      frame.append(ax.text(0.5, 1.05, f'Iteration {n_iter} | Cluster Assignment', transform=ax.transAxes, ha=\"center\"))\n\n      vor = Voronoi(cluster_centers)\n      d = voronoi_plot_2d(vor, ax=ax, show_points=False, show_vertices=False);\n      frame += list(d.axes[0].lines[-1:] + d.axes[0].collections[-2:])\n      ax.axis([-5, 5, -4, 5])\n\n      loss.append(obj(X, cluster_centers))\n      m = 1\n      frame.append(ax1.scatter([0], [loss[0]], color='red', marker='x', s=30))\n      frame.append(ax1.text(0.5, 1.05, 'Objective Function', transform=ax1.transAxes, ha=\"center\"))\n      frame.append(ax1.text(0.5, -0.1, 'Iterations', transform=ax1.transAxes, ha=\"center\"))\n\n      artists.append(frame)\n\n  converged = False\n  while not converged:\n\n    # cluster_centers = recompute_clusters(X, z)\n    for i in range(n):\n      cluster_points = X[z==i]\n      if len(cluster_points)&gt;0:\n        cluster_centers[i] = np.mean(cluster_points, axis=0)\n\n    n_iter += 1\n\n    # Modified cluster assignment step\n    converged = True\n\n    if animate:\n      frame = []\n      frame.append(ax.scatter(X[:, 0], X[:, 1], c=z, s=10))\n      frame.append(ax.scatter(cluster_centers[:, 0], cluster_centers[:, 1], marker = 'x', s = 100, color = 'red', linewidth=1))\n      frame.append(ax.text(0.5, 1.05, f'Iteration {n_iter} | Cluster Recomputation', transform=ax.transAxes, ha=\"center\"))\n\n      vor = Voronoi(cluster_centers)\n      d = voronoi_plot_2d(vor, ax=ax, show_points=False, show_vertices=False);\n      frame += list(d.axes[0].lines[-1:] + d.axes[0].collections[-2:])\n      ax.axis([-5, 5, -4, 5])\n\n      for i in range(0, len(loss)-1, 2):\n        frame.append(list(ax1.plot([i/2, (i+2)/2], [loss[i], loss[i+1]], color='red', marker='x', markersize=5))[0])\n\n      loss.append(obj(X, cluster_centers))\n      frame.append(list(ax1.plot([(len(loss)-2)/2, (len(loss)-1)/2], [loss[-2], loss[-1]], color='red', linestyle=':'))[0])\n      # lines = list(ax1.plot(np.arange(len(loss))/2, loss, color='red', marker='xo', markersize=6))\n      # frame += lines\n      frame.append(ax1.text(0.5, 1.05, 'Objective Function', transform=ax1.transAxes, ha=\"center\"))\n      frame.append(ax1.text(0.5, -0.1, 'Iterations', transform=ax1.transAxes, ha=\"center\"))\n\n      artists.append(frame)\n\n    for i in range(len(X)):\n      z_i = np.argmin([np.linalg.norm(X[i] - cluster_center) for cluster_center in cluster_centers])\n\n      if z_i != z[i]:\n        z[i] = z_i\n        converged = False\n\n    if animate and not converged:\n      frame = []\n      frame.append(ax.scatter(X[:, 0], X[:, 1], c=z, s=10))\n      frame.append(ax.scatter(cluster_centers[:, 0], cluster_centers[:, 1], marker = 'x', s = 100, color = 'red', linewidth=1))\n      frame.append(ax.text(0.5, 1.05, f'Iteration {n_iter} | Cluster Re-assignment', transform=ax.transAxes, ha=\"center\"))\n\n      vor = Voronoi(cluster_centers)\n      d = voronoi_plot_2d(vor, ax=ax, show_points=False, show_vertices=False);\n      frame += list(d.axes[0].lines[-1:] + d.axes[0].collections[-2:])\n      ax.axis([-5, 5, -4, 5])\n\n      loss.append(obj(X, cluster_centers))\n      m = 1\n      for i in range(0, len(loss)-1, 2):\n        frame.append(list(ax1.plot([i/2, (i+2)/2], [loss[i], loss[i+1]], color='red', marker='x', markersize=5))[0])\n\n      frame.append(ax1.text(0.5, 1.05, 'Objective Function', transform=ax1.transAxes, ha=\"center\"))\n      frame.append(ax1.text(0.5, -0.1, 'Iterations', transform=ax1.transAxes, ha=\"center\"))\n\n      artists.append(frame)\n\n  if animate:\n    plt.close()\n    return fig, (ax, ax1), cluster_centers, artists\n  else:\n    return cluster_centers, n_iter"
  },
  {
    "objectID": "pages/Not03.html#random-initialization",
    "href": "pages/Not03.html#random-initialization",
    "title": "Week 3: K-means",
    "section": "Random Initialization",
    "text": "Random Initialization\nWe now run Lloyd’s algorithm on the dataset with random initialization. We use the ideal_check function to see whether the obtained cluster from lloyd’s is optimal; else we re-run it again.\nAn animation using Matplotlib’s ArtistAnimation is shown to illustrate the working of this strategy.\n\ncluster_centers = X[np.random.choice(len(X), n)]\nz = cluster_assignment(X, cluster_centers)\n\nfig, ax, final_clusters, artists = lloyds(cluster_centers, X, z)\n\nwhile not ideal_check(cluster_assignment(X, final_clusters), ideal_z):\n\n  cluster_centers = X[np.random.choice(len(X), n)]\n  z = cluster_assignment(X, cluster_centers)\n  fig, ax, final_clusters, artists = lloyds(cluster_centers, X, z)\n\nanim = animation.ArtistAnimation(fig, artists, interval=500, repeat=False, blit=False);\nHTML(anim.to_jshtml())\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\nSome initializations do not give a good clustering\n\ncluster_centers = X[np.random.choice(len(X), n)]\nz = cluster_assignment(X, cluster_centers)\nfig, ax, final_clusters, artists = lloyds(cluster_centers, X, z)\n\nwhile ideal_check(cluster_assignment(X, final_clusters), ideal_z):\n  cluster_centers = X[np.random.choice(len(X), n)]\n  z = cluster_assignment(X, cluster_centers)\n  fig, ax, final_clusters, artists = lloyds(cluster_centers, X, z)\n\nanim = animation.ArtistAnimation(fig, artists, interval=500, repeat=False, blit=False);\nHTML(anim.to_jshtml())\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "pages/Not03.html#experiments",
    "href": "pages/Not03.html#experiments",
    "title": "Week 3: K-means",
    "section": "Experiments",
    "text": "Experiments\n\nRandom Initialization\n\nfrom timeit import default_timer as timer\n\nnoof_trials = 1000\n\nrandom = {'ideals' : 0,\n          'SSE' : [],\n          'iters' : [],\n          'time' : []}\n\nexp_start = timer()\nfor _ in range(noof_trials):\n\n  start = timer()\n  cluster_centers = X[np.random.choice(len(X), n)]\n  final_clusters, n_iter = lloyds(cluster_centers, X, cluster_assignment(X, cluster_centers), animate=False)\n\n  random['time'].append(timer()-start)\n  random['ideals'] += int(ideal_check(ideal_z, cluster_assignment(X, final_clusters)))\n  random['SSE'].append(obj(X, final_clusters))\n  random['iters'].append(n_iter)\n\nprint(f'Experiment done in {timer()-exp_start:.2f} seconds')\n\nExperiment done in 670.24 seconds\n\n\n\n\nK-Means++ Initialization\n\nkmplusplus = {'ideals' : 0,\n              'SSE' : [],\n              'iters' : [],\n              'time' : []}\n\nexp_start = timer()\nfor _ in range(noof_trials):\n\n  start = timer()\n  cluster_centers = plusplus(animate=False)\n  final_clusters, n_iter = lloyds(cluster_centers, X, cluster_assignment(X, cluster_centers), animate=False)\n\n  kmplusplus['time'].append(timer()-start)\n  kmplusplus['ideals'] += int(ideal_check(ideal_z, cluster_assignment(X, final_clusters)))\n  kmplusplus['SSE'].append(obj(X, final_clusters))\n  kmplusplus['iters'].append(n_iter)\n\nprint(f'Experiment done in {timer()-exp_start:.2f} seconds')\n\nExperiment done in 539.66 seconds"
  },
  {
    "objectID": "pages/Not03.html#results",
    "href": "pages/Not03.html#results",
    "title": "Week 3: K-means",
    "section": "Results",
    "text": "Results\n\nSSE Comparison\n\nplt.style.use('seaborn-v0_8')\n\nfig = plt.figure(figsize=(12, 5))\nfig.suptitle('Initialization Comparison | SSE')\n\nax1 = plt.subplot(121)\nax1.hist(random['SSE'], bins=30)\nax1.set_title('Random')\nax1.set_ylabel('Frequency')\nax1.set_xlabel('SSE')\n\nax2 = plt.subplot(122, sharex=ax1, sharey=ax1)\nax2.hist(kmplusplus['SSE'], bins=30)\nax2.set_title('K-Means++')\nax2.set_ylabel('Frequency')\nax2.set_xlabel('SSE')\n\nplt.show()\n\n\n\n\n\nplt.figure(figsize=(12, 2))\nplt.title('Fraction of Best Convergences')\nplt.barh(['Random', 'K-Means++'], [random['ideals']/noof_trials, kmplusplus['ideals']/noof_trials], height=0.4);\nplt.xlim([0, 1]);\n\n\n\n\nThe SSE for the optimal cluster assignment for this particular dataset is around 175. We observe from the above charts that K-Means++ does indeed have a higher chance (0.8) of converging to this as compared to Vanilla K-Means (0.5).\n\n\nNumber of Iterations Comparison\n\nfig = plt.figure(figsize=(12, 5))\nfig.suptitle('Initialization Comparison | Number of Iterations')\n\nax1 = plt.subplot(121)\nax1.hist(random['iters'], bins=max(random['iters'])-min(random['iters']))\nax1.set_title('Random')\nax1.set_ylabel('Frequency')\nax1.set_xlabel('Number of Iterations')\n\nax2 = plt.subplot(122, sharex=ax1, sharey=ax1)\nax2.hist(kmplusplus['iters'], bins=max(kmplusplus['iters'])-min(kmplusplus['iters']))\nax2.set_title('K-Means++')\nax2.set_ylabel('Frequency')\nax2.set_xlabel('Number of Iterations')\n\nplt.show()\n\n\n\n\nFor Vanilla K-Means, we observe that the number of iterations has quite a bit of variation, with an average of 8.75 iterations to convergence.\nK-Means++ has a relatively smaller spread, with an average of 4.2 iterations to convergence (post intialization).\n\n\nTime Taken Comparison\n\nfig = plt.figure(figsize=(12, 5))\nfig.suptitle('Initialization Comparison | Time taken to converge')\n\nax1 = plt.subplot(121)\nax1.hist(random['time'], bins=20)\nax1.set_title('Random')\nax1.set_ylabel('Frequency')\nax1.set_xlabel('Time taken')\n\nax2 = plt.subplot(122, sharex=ax1, sharey=ax1)\nax2.hist(kmplusplus['time'], bins=20)\nax2.set_title('K-Means++')\nax2.set_ylabel('Frequency')\nax2.set_xlabel('Time taken (s)')\n\nplt.show()\n\n\n\n\n\nnp.array(kmplusplus['time']).mean()\n\n0.4248704458820016\n\n\nSimilar to the results for number of iterations, the time taken till convergence also has a wide spread for Random Initialization, with an average of 0.55s. K-Means++ has an average of 0.42s."
  },
  {
    "objectID": "pages/Not03.html#k-means",
    "href": "pages/Not03.html#k-means",
    "title": "Week 3: K-means",
    "section": "K-Means",
    "text": "K-Means\nK-Means with k = 3 for X = [-15, -10, 0, 5, 15, 20, 25] with mean clusters μ = [-15, 0, 5]\n\nplt.figure(figsize=(15, 7.5))\n# From the problem\nx = np.expand_dims(np.array([-15, -10, 0, 5, 15, 20, 25]), axis=1)\nclusters = np.expand_dims(np.array([-15, 0, 5]), axis=1)\nplt.subplot(2, 3, 1)\nplt.scatter(x[:,0], np.zeros(x.shape[0]));\nplt.scatter(clusters[:, 0], np.zeros(3), marker = 'x', s = 100, color = 'red', linewidth=1);\nplt.title('From the problem')\n\n# Initial Cluster Assignment\nz = cluster_assignment(x, clusters)\nplt.subplot(2, 3, 2)\nplt.scatter(x[:,0], np.zeros(x.shape[0]), c=z);\nplt.scatter(clusters[:, 0], np.zeros(3), marker = 'x', s = 100, color = 'red', linewidth=1);\nplt.title('Initial Cluster Assignment')\n\n# Recompute Cluster Centers\nclusters = recompute_clusters(x, z)\nplt.subplot(2, 3, 3)\nplt.scatter(x[:,0], np.zeros(x.shape[0]), c=z);\nplt.scatter(clusters[:, 0], np.zeros(3), marker = 'x', s = 100, color = 'red', linewidth=1);\nplt.title('Recompute Cluster Centers')\n\n# Next Cluster Assignment\nz = cluster_assignment(x, clusters)\nplt.subplot(2, 3, 4)\nplt.scatter(x[:,0], np.zeros(x.shape[0]), c=z);\nplt.scatter(clusters[:, 0], np.zeros(3), marker = 'x', s = 100, color = 'red', linewidth=1);\nplt.title('Next Cluster Assignment')\n\n# Again Recompute Cluster Centers\nclusters = recompute_clusters(x, z)\nplt.subplot(2, 3, 5)\nplt.scatter(x[:,0], np.zeros(x.shape[0]), c=z);\nplt.scatter(clusters[:, 0], np.zeros(3), marker = 'x', s = 100, color = 'red', linewidth=1);\nplt.title('Again Recompute Cluster Centers')\n\n# Cluster Assignment - No Change\n# Algorithm has converged\nz = cluster_assignment(x, clusters)\nplt.subplot(2, 3, 6)\nplt.scatter(x[:,0], np.zeros(x.shape[0]), c=z);\nplt.scatter(clusters[:, 0], np.zeros(3), marker = 'x', s = 100, color = 'red', linewidth=1);\nplt.title('Algorithm has converged');"
  },
  {
    "objectID": "pages/Not03.html#k-means-1",
    "href": "pages/Not03.html#k-means-1",
    "title": "Week 3: K-means",
    "section": "K-Means++",
    "text": "K-Means++\nFor the dataset below, k-means++ algorithm is run with k=2. Find the probability that \\mathbf{x}_2, \\mathbf{x}_1 are chosen as the initial clusters, in that order.\n\\left\\{\\mathbf{x_{1}} \\ =\\ \\begin{bmatrix}\n0\\\\\n2\n\\end{bmatrix} ,\\mathbf{\\ x_{2}} \\ =\\ \\begin{bmatrix}\n2\\\\\n0\n\\end{bmatrix} ,\\ \\mathbf{x_{3}} \\ =\\ \\begin{bmatrix}\n0\\\\\n0\n\\end{bmatrix} ,\\ \\mathbf{x_{4}} \\ =\\ \\begin{bmatrix}\n0\\\\\n-2\n\\end{bmatrix} ,\\ \\mathbf{x_{5}} \\ =\\ \\begin{bmatrix}\n-2\\\\\n0\n\\end{bmatrix}\\right\\}\n\ndataset = np.array([[0, 2], [2, 0], [0, 0], [0, -2], [-2, 0]])\nprobabilities = np.array([1/len(dataset) for _ in range(len(dataset))])\nprint('Initial Probablities:', probabilities)\nclusters = []\nanswer = 1\n\n# First we select x2 = [2,0]\nclusters.append(dataset[1])\nanswer *= probabilities[1]\n\n# Rescore based on selected clusters\nscores = np.array([min([np.linalg.norm(datapoint-cluster)**2 for cluster in clusters]) for datapoint in dataset])\n\n# Normalize scores to probability\nprobabilities = scores/scores.sum()\nprint('Probabilities after selecting x2: ', [round(i, 3) for i in probabilities])\n\n# Now we select x1 = [0,2]\nclusters.append(dataset[0])\nanswer *= probabilities[0]\n\nprint('Probability of selecting [x2 x1]:', round(answer, 3))\n\nInitial Probablities: [0.2 0.2 0.2 0.2 0.2]\nProbabilities after selecting x2:  [0.222, 0.0, 0.111, 0.222, 0.444]\nProbability of selecting [x2 x1]: 0.044"
  },
  {
    "objectID": "pages/Wk10.html",
    "href": "pages/Wk10.html",
    "title": "Support Vector Machines",
    "section": "",
    "text": "Note\n\n\n\nFeedback/Correction: Click here!.\nPDF Link: Click here!"
  },
  {
    "objectID": "pages/Wk10.html#defining-key-parameters",
    "href": "pages/Wk10.html#defining-key-parameters",
    "title": "Support Vector Machines",
    "section": "Defining Key Parameters",
    "text": "Defining Key Parameters\nWe begin by introducing essential parameters:\n\nWeight Vector: Let \\(\\mathbf{w^*} \\in \\mathbb{R}^d\\) be the weight vector such that \\((\\mathbf{w^{*T}x_i})y_i\\ge\\gamma\\) for all \\(i\\). This weight vector ensures the desired margin \\(\\gamma\\).\nBounding Radius: We define \\(R&gt;0 \\in \\mathbb{R}\\) such that for all \\(i\\), \\(||\\mathbf{x_i}||\\le R\\). This represents a constraint on the norm of input data.\n\nWith these parameters established, we can formulate the upper bound on the number of mistakes made by the perceptron learning algorithm: \\[\n\\text{\\#mistakes} \\le \\frac{R^2}{\\gamma^2}\n\\]"
  },
  {
    "objectID": "pages/Wk10.html#observations-and-objectives",
    "href": "pages/Wk10.html#observations-and-objectives",
    "title": "Support Vector Machines",
    "section": "Observations and Objectives",
    "text": "Observations and Objectives\n\nObservations\nWe make several observations regarding the perceptron learning algorithm:\n\nThe “quality” of the solution depends on the margin \\(\\gamma\\).\nThe number of mistakes is influenced by the margin associated with \\(\\mathbf{w^*}\\).\nThe weight vector \\(\\mathbf{w_{perc}}\\) need not be identical to \\(\\mathbf{w^*}\\).\n\n\n\nGoal: Margin Maximization\nGiven these observations, our overarching goal is to find the solution that maximizes the margin. It is crucial to note that a single dataset can have multiple linear classifiers with varying margins, as depicted in the diagram below:\n\n\n\nMultiple Classifiers\n\n\nTo formalize our objective, we aim to maximize the margin \\(\\gamma\\): \\[\n\\max_{\\mathbf{w},\\gamma} \\gamma\n\\]\nSubject to the following constraints: \\[\\begin{align*}\n(\\mathbf{w^T x_i})y_i &\\ge \\gamma \\quad \\text{for all } i \\\\\n||\\mathbf{w}||^2 &= 1\n\\end{align*}\\]"
  },
  {
    "objectID": "pages/Wk10.html#reformulating-the-objective",
    "href": "pages/Wk10.html#reformulating-the-objective",
    "title": "Support Vector Machines",
    "section": "Reformulating the Objective",
    "text": "Reformulating the Objective\nTo simplify our objective, we can express \\(\\gamma\\) in terms of the width of \\(\\mathbf{w}\\): \\[\n\\max_{\\mathbf{w}} \\text{width}(\\mathbf{w})\n\\]\nSubject to the constraint: \\[\n(\\mathbf{w^T x_i})y_i \\ge 1 \\quad \\text{for all } i\n\\]\n\nWidth Calculation\nThe width of the margin, denoted as \\(\\text{width}(\\mathbf{w})\\), can be calculated as the distance between two parallel margins. Consider two points \\(\\mathbf{x}\\) and \\(\\mathbf{z}\\) lying on opposite sides of the decision boundary such that \\(\\mathbf{w^T x}=-1\\) and \\(\\mathbf{w^T z}=1\\) or vice versa.\nLet \\(\\mathbf{x_1}\\) and \\(\\mathbf{x_2}\\) be two points on the margins and opposite sides of the decision boundary.\n\n\n\nMargin Width\n\n\nThe width is given by: \\[\\begin{align*}\n\\mathbf{x_1^T w} - \\mathbf{x_2^T w} &= 2 \\\\\n(\\mathbf{x_1}-\\mathbf{x_2})^T\\mathbf{w} &= 2\\\\\n||\\mathbf{x_1} - \\mathbf{x_2}||_2||\\mathbf{w}||_2 \\cos(\\theta) &= 2 \\\\\n\\therefore ||\\mathbf{x_1} - \\mathbf{x_2}||_2 &= \\frac{2}{||\\mathbf{w}||_2}\n\\end{align*}\\]\nHence, our objective can be restated as: \\[\n\\max_{\\mathbf{w}}  \\frac{2}{||\\mathbf{w}||^2_2} \\quad \\text{subject to} \\quad (\\mathbf{w^T x_i})y_i \\ge 1 \\quad \\text{for all } i\n\\]\nEquivalently, we can frame it as a minimization problem: \\[\n\\min_{\\mathbf{w}}  \\frac{1}{2}||\\mathbf{w}||^2_2 \\quad \\text{subject to} \\quad (\\mathbf{w^T x_i})y_i \\ge 1 \\quad \\text{for all } i\n\\]\nIn summary, finding the separating hyperplane with the maximum margin is equivalent to finding the one with the smallest possible normal vector \\(\\mathbf{w}\\)."
  },
  {
    "objectID": "pages/Wk10.html#lagrange-function",
    "href": "pages/Wk10.html#lagrange-function",
    "title": "Support Vector Machines",
    "section": "Lagrange Function",
    "text": "Lagrange Function\nFor the optimization problem described above, the Lagrange function, denoted as \\(\\mathcal{L}(\\mathbf{w}, \\boldsymbol{\\alpha})\\), is defined as follows: \\[\n\\mathcal{L}(\\mathbf{w}, \\boldsymbol{\\alpha}) = f(\\mathbf{w}) + \\boldsymbol{\\alpha}^T g(\\mathbf{w}) \\quad \\text{for all } \\mathbf{w}\n\\]\nHere, \\(\\boldsymbol{\\alpha}\\) is a vector of Lagrange multipliers, constrained to be non-negative (\\(\\boldsymbol{\\alpha} \\ge \\mathbf{0}\\)).\nMaximizing the Lagrange function with respect to \\(\\boldsymbol{\\alpha}\\) leads us to the following formulation: \\[\\begin{align*}\n\\max_{\\boldsymbol{\\alpha} \\ge \\mathbf{0}} \\mathcal{L}(\\mathbf{w}, \\boldsymbol{\\alpha}) &= \\max_{\\boldsymbol{\\alpha} \\ge \\mathbf{0}} \\left(f(\\mathbf{w}) + \\boldsymbol{\\alpha}^T g(\\mathbf{w})\\right) \\\\\n&=\n\\begin{cases}\n\\infty & \\text{if } g(\\mathbf{w}) &gt; \\mathbf{0} \\\\\nf(\\mathbf{w}) & \\text{if } g(\\mathbf{w}) \\le \\mathbf{0}\n\\end{cases}\n\\end{align*}\\]\nSince the Lagrange function is equivalent to \\(f(\\mathbf{w})\\) when \\(g(\\mathbf{w}) \\le \\mathbf{0}\\), we can rewrite our original problem as follows: \\[\\begin{align*}\n\\min_{\\mathbf{w}} f(\\mathbf{w}) &= \\min_{\\mathbf{w}} \\left[\\max_{\\boldsymbol{\\alpha} \\ge \\mathbf{0}} \\mathcal{L}(\\mathbf{w}, \\boldsymbol{\\alpha})\\right] \\\\\n&= \\min_{\\mathbf{w}} \\left[\\max_{\\boldsymbol{\\alpha} \\ge \\mathbf{0}} \\left(f(\\mathbf{w}) + \\boldsymbol{\\alpha}^T g(\\mathbf{w})\\right)\\right]\n\\end{align*}\\]\nIn general, interchanging the positions of the min and max functions is not permissible unless all involved functions are convex. In our case, both \\(f\\) and \\(g\\) are convex functions, allowing us to express this as: \\[\n\\min_{\\mathbf{w}} \\left[\\max_{\\boldsymbol{\\alpha} \\ge \\mathbf{0}} \\left(f(\\mathbf{w}) + \\boldsymbol{\\alpha}^T g(\\mathbf{w})\\right)\\right] \\equiv \\max_{\\boldsymbol{\\alpha} \\ge \\mathbf{0}} \\left[\\min_{\\mathbf{w}} \\left(f(\\mathbf{w}) + \\boldsymbol{\\alpha}^T g(\\mathbf{w})\\right)\\right]\n\\]"
  },
  {
    "objectID": "pages/Wk10.html#multiple-constraints",
    "href": "pages/Wk10.html#multiple-constraints",
    "title": "Support Vector Machines",
    "section": "Multiple Constraints",
    "text": "Multiple Constraints\nNow, extending our discussion to scenarios with \\(m\\) constraints, denoted as \\(g_i(\\mathbf{w}) \\le \\mathbf{0}\\) for \\(i \\in [1, m]\\), we can represent the problem as follows: \\[\\begin{align*}\n\\min_{\\mathbf{w}} f(\\mathbf{w}) &\\equiv \\min_{\\mathbf{w}} \\left[\\max_{\\boldsymbol{\\alpha} \\ge \\mathbf{0}} f(\\mathbf{w}) + \\sum _{i=1} ^m \\alpha _i g_i(\\mathbf{w})\\right] \\\\\n&\\equiv \\max_{\\boldsymbol{\\alpha} \\ge \\mathbf{0}} \\left[\\min_{\\mathbf{w}} f(\\mathbf{w}) + \\sum _{i=1} ^m \\alpha _i g_i(\\mathbf{w})\\right]\n\\end{align*}\\]"
  },
  {
    "objectID": "pages/Wk10.html#duality-revisited",
    "href": "pages/Wk10.html#duality-revisited",
    "title": "Support Vector Machines",
    "section": "Duality Revisited",
    "text": "Duality Revisited\nRevisiting the Lagrangian function: \\[\n\\min_{\\mathbf{w}} \\left [\\max_{\\boldsymbol{\\alpha} \\ge \\mathbf{0}} f(\\mathbf{w}) + \\boldsymbol{\\alpha}^T g(\\mathbf{w}) \\right ] \\equiv \\max_{\\boldsymbol{\\alpha} \\ge \\mathbf{0}} \\left [ \\min_{\\mathbf{w}} f(\\mathbf{w}) + \\boldsymbol{\\alpha}^T g(\\mathbf{w}) \\right ]\n\\]\nHere, the primal function is on the left-hand side, and the dual function is on the right. The solutions for the primal and dual functions are denoted as \\(\\mathbf{w}^∗\\) and \\(\\boldsymbol{\\alpha}^*\\), respectively. When these solutions are substituted into the equation, we obtain: \\[\n\\max_{\\boldsymbol{\\alpha} \\ge \\mathbf{0}} f(\\mathbf{w}^*) + \\boldsymbol{\\alpha}^T g(\\mathbf{w}^*) =  \\min_{\\mathbf{w}} f(\\mathbf{w}) + \\boldsymbol{\\alpha}^{*T} g(\\mathbf{w})\n\\]\nGiven that \\(g(\\mathbf{w}^*) \\le \\mathbf{0}\\), the left-hand side simplifies to \\(f(\\mathbf{w}^*)\\): \\[\nf(\\mathbf{w}^*) =  \\min_{\\mathbf{w}} f(\\mathbf{w}) + \\boldsymbol{\\alpha}^{*T} g(\\mathbf{w})\n\\]\nSubstituting \\(\\mathbf{w}^*\\) for \\(\\mathbf{w}\\) in the right-hand side yields a new right-hand side that is greater than or equal to the current one: \\[\nf(\\mathbf{w}^*) \\le f(\\mathbf{w}^*) + \\boldsymbol{\\alpha}^{*T} g(\\mathbf{w}^*)\n\\]\nHence, we can infer: \\[\n\\boldsymbol{\\alpha}^{*T} g(\\mathbf{w}^*) \\ge \\mathbf{0}\n\\]\nHowever, considering the constraints, where \\(\\boldsymbol{\\alpha}^{*} \\ge \\mathbf{0}\\) and \\(g(\\mathbf{w}^*)\\le \\mathbf{0}\\), we arrive at: \\[\n\\boldsymbol{\\alpha}^{*T} g(\\mathbf{w}^*) \\le \\mathbf{0}\n\\]\nFrom this, we deduce: \\[\n\\boldsymbol{\\alpha}^{*T} g(\\mathbf{w}^*) = \\mathbf{0}\n\\]\nExtending this equation for multiple constraints, we obtain: \\[\n\\alpha_i^{*} g(w^*_i) = 0 \\quad \\forall i\n\\]\nHence, if one of the two values is greater than zero, the other must be zero. Considering \\(g(\\mathbf{w}^*) = 1-(\\mathbf{w}^T\\mathbf{x}_i)y_i\\), we can express this as: \\[\n\\alpha_i^{*} (1-(\\mathbf{w}^T\\mathbf{x}_i)y_i) = 0 \\quad \\forall i\n\\]\nImportantly, when \\(\\alpha_i &gt; \\mathbf{0}\\), we deduce: \\[\n(\\mathbf{w}^T\\mathbf{x}_i)y_i = 1\n\\]\nThis implies that the \\(i^{th}\\) data point resides on the “Supporting” hyperplane and significantly contributes to the determination of \\(\\mathbf{w}^*\\).\nConsequently, data points with \\(\\alpha_i &gt; \\mathbf{0}\\) earn the title of Support Vectors, and this algorithm is known as the Support Vector Machine (SVM)."
  },
  {
    "objectID": "pages/Wk10.html#definition-of-support-vector-machines-svms",
    "href": "pages/Wk10.html#definition-of-support-vector-machines-svms",
    "title": "Support Vector Machines",
    "section": "Definition of Support Vector Machines (SVMs)",
    "text": "Definition of Support Vector Machines (SVMs)\nSupport Vector Machines (SVMs) stand as a category of supervised learning algorithms designed for classification and regression analysis. SVMs aim to identify the optimal hyperplane that maximizes the margin between data points from different classes. In scenarios where data is not linearly separable, SVMs employ kernel functions to map the data into a higher-dimensional space where a linear decision boundary can effectively segregate the classes.\nInsight: \\(\\mathbf{w}^*\\) represents a sparse linear combination of data points."
  },
  {
    "objectID": "pages/Wk10.html#hard-margin-svm-algorithm",
    "href": "pages/Wk10.html#hard-margin-svm-algorithm",
    "title": "Support Vector Machines",
    "section": "Hard-Margin SVM Algorithm",
    "text": "Hard-Margin SVM Algorithm\nThe Hard-Margin SVM algorithm is applicable only when the dataset is linearly separable with a margin parameter \\(\\gamma &gt; 0\\). Its key steps include:\n\nDirect or Kernelized Calculation of Q: Compute the matrix \\(Q=\\mathbf{X}^T\\mathbf{X}\\) directly or using a kernel, based on the dataset.\nGradient Descent: Employ the gradient of the dual formula, \\(\\boldsymbol{\\alpha}^T\\mathbf{1} - \\frac{1}{2}\\boldsymbol{\\alpha}^T\\mathbf{Y}^T Q\\mathbf{Y}\\boldsymbol{\\alpha}\\), in a gradient descent algorithm to iteratively find a satisfactory set of Lagrange multipliers \\(\\boldsymbol{\\alpha}\\). Initialize \\(\\boldsymbol{\\alpha}\\) as a zero vector in \\(\\mathbb{R}^n_+\\).\nPrediction: For prediction, follow these formulas:\n\nFor non-kernelized SVM:\n\n\\(\\textbf{label}(\\mathbf{x}_{\\text{test}}) = \\text{sign}( \\mathbf{w}^T\\mathbf{x}_{\\text{test}}) = \\text{sign}\\left(\\displaystyle\\sum _{i=1} ^n \\alpha _i y_i(\\mathbf{x}_i^T\\mathbf{x}_{\\text{test}})\\right)\\)\n\nFor kernelized SVM:\n\n\\(\\textbf{label}(\\mathbf{x}_{\\text{test}}) = \\text{sign}(\\mathbf{w}^T\\boldsymbol{\\phi}(\\mathbf{x}_{\\text{test}})) = \\text{sign}\\left(\\displaystyle\\sum _{i=1} ^n \\alpha _i y_i k(\\mathbf{x}_i^T\\mathbf{x}_{\\text{test}})\\right)\\)"
  },
  {
    "objectID": "pages/Not06.html",
    "href": "pages/Not06.html",
    "title": "Week 6: Ridge, Lasso Regression & Cross Validation Methods",
    "section": "",
    "text": "Colab Link: Click here!"
  },
  {
    "objectID": "pages/Not06.html#bias-variance-decomposition-of-mse",
    "href": "pages/Not06.html#bias-variance-decomposition-of-mse",
    "title": "Week 6: Ridge, Lasso Regression & Cross Validation Methods",
    "section": "Bias-Variance Decomposition of MSE",
    "text": "Bias-Variance Decomposition of MSE\nThe above formula for mean squared error can be rewritten as\n\\text{MSE}(\\mathbf{\\hat{w}}) = \\text{tr}(\\text{Var}(\\mathbf{\\hat{w}})) + ||\\text{Bias}(\\mathbf{\\hat{w}})||^2\nwhere \\text{tr}(\\text{Var}(\\mathbf{\\hat{w}})) is the trace of the covariance matrix, and \\text{Bias}(\\mathbf{\\hat{w}}) = \\mathbb{E}[\\mathbf{\\hat{w}}] - \\mathbf{w}\nis the bias, i.e, the expected difference between the estimator \\mathbf{\\hat{w}} and true value \\mathbf{w}.\n\nApplying Bias-Variance decomposition on ML Estimator of w\nThe ML Estimator has zero bias; so we get\n\n\\begin{align*}\n\\text{MSE}(\\mathbf{\\hat{w}}_{ML}) &= \\text{tr}(\\text{Var}(\\mathbf{\\hat{w}}_{ML})) \\\\\n&= \\sigma^2 \\times \\text{tr}((XX^T)^{-1})\n\\end{align*}\n\n\n\nDerivation of decomposition\nWe use the below well-known formula relating the covariance matrix \\text{Var}(X) to the first and second moments: \\mathbb{E}[X^2] = (\\mathbb{E}[X])^2 + \\text{Var}(X)\nTherefore we have,\n\n\\begin{alignat*}{2}\n&& \\text{MSE}(\\mathbf{\\hat{w}})\n& = \\mathbb{E}[||\\mathbf{\\hat{w}} - \\mathbf{w}||^2] \\\\\n&& & = (\\mathbb{E}[||\\mathbf{\\hat{w}} - \\mathbf{w}||])^2 + \\text{Var}(||\\mathbf{\\hat{w}} - \\mathbf{w}||)\\\\\n&& & = ||\\mathbb{E}[\\mathbf{\\hat{w}}] - \\mathbf{w}||^2 + \\text{Var}(||\\mathbf{\\hat{w}}||) \\\\\n&& & = ||\\text{Bias}(\\mathbf{\\hat{w}})||^2 + \\text{tr}(\\text{Var}(\\mathbf{\\hat{w}}))\n\\end{alignat*}\n\nSource: Wikipedia"
  },
  {
    "objectID": "pages/Not06.html#in-pursuit-of-a-better-estimator",
    "href": "pages/Not06.html#in-pursuit-of-a-better-estimator",
    "title": "Week 6: Ridge, Lasso Regression & Cross Validation Methods",
    "section": "In pursuit of a better estimator",
    "text": "In pursuit of a better estimator\nWe see that the goodness of the ML estimate depends on the variance of error σ^2 and \\text{tr}((XX^T)^{-1}). We would like to reduce this quantity.\nThe variance of error is inherent to the experimental setup, and reduction of this quantity therefore depends on qualitative improvements of instruments/peripherals used to collect data. So we cannot hope to reduce this quantity from a mathematical perspective.\nThe trace depends on (XX^T)^{-1}, which denotes the inverse covariance matrix of the data X. The covariance matrix captures how the features are related with each other, and it seems that this relation affects the goodness of our estimator. We may try to tweak this value to reduce the trace value, thereby increasing the goodness of our ML estimator.\n\nReducing the Trace value\nThe trace of a matrix X can also be represented as the sum of the eigenvalues of X.\nLet \\mathbf{λ} denote the set of eigenvalues of XX^T. Then \\{ \\frac{1}{λ_i} \\; ∀ \\; i \\; \\epsilon \\; \\{1, 2, .., n\\} \\} is the set of eigenvalues of (XX^T)^{-1} and its trace thus is the following:\n\\text{tr}((XX^T)^{-1}) = \\sum_{i=1}^{d}\\frac{1}{λ_i}\nNow, we’d like to reduce this value. A way forward would be to increase the eigenvalues of XX^T by doing the following:\n\\text{eigenvalues of } (XX^T + λI) = \\{ λ_i + λ \\; ∀ \\; i \\; \\epsilon \\; \\{1, 2, .., n\\} \\}\nwhich will result in a smaller trace value (due to increase in denominator): \\text{tr}((XX^T + λI)^{-1}) = \\sum_{i=1}^{d}\\frac{1}{λ_i + λ}\nHowever, this process reformulates the problem the estimator is trying to solve (trade-off) to the following:\n\\hat{\\mathbf{w}}_{\\lambda - ML} = (XX^T + λI)^{-1}Xy\nThis induces a non-zero bias in our estimator:\n\n\\begin{align*}\n\\text{Bias}(\\mathbf{\\hat{w}}_{\\lambda-ML})\n& = \\mathbb{E}[\\mathbf{\\hat{w}}_{\\lambda-ML}] - \\mathbf{w}\\\\\n& = [(XX^T + λI)^{-1} - (XX^T)^†](XX^T)\\mathbf{w}\n\\end{align*}\n\nThe difference between the MSE of our estimators is then:\n\n\\begin{align*}{3}\n&&& \\text{MSE}(\\mathbf{\\hat{w}}) - \\text{MSE}(\\mathbf{\\hat{w}}_{\\lambda-ML})\n&& = \\text{tr}(\\text{Var}(\\mathbf{\\hat{w}})) - \\text{tr}(\\text{Var}(\\mathbf{\\hat{w}}_{\\lambda-ML}))\n& \\quad - \\quad\\text{(A)} \\\\\n&&& && \\quad - || \\text{Bias}(\\mathbf{\\hat{w}}_{\\lambda-ML}) ||^2\n& \\quad - \\quad\\text{(B)} \\\\\n\\end{align*}\n\nNote that both \\text{(A)} and \\text{(B)} are positive quantities, hence the difference can either be positive or negative; The difference depends on the parameter \\lambda.\nThe existence theorem asserts that there exists some non-zero λ such that the MSE of \\hat{\\mathbf{w}}_{\\lambda - ML} is lower than that of \\hat{\\mathbf{w}}_{ML}, i.e we get \\text{(A)} &gt; \\text{(B)}."
  },
  {
    "objectID": "pages/Not06.html#closed-form-solution-for-ridge-estimator",
    "href": "pages/Not06.html#closed-form-solution-for-ridge-estimator",
    "title": "Week 6: Ridge, Lasso Regression & Cross Validation Methods",
    "section": "Closed-Form Solution for Ridge Estimator",
    "text": "Closed-Form Solution for Ridge Estimator\nIt is the same as the aforementioned \\hat{\\mathbf{w}}_{\\lambda - ML}:\n\\hat{\\mathbf{w}}_{\\lambda - ML} = (XX^T + λI)^{-1}Xy\nIt’s good to know that the inverse of (XX^T + λI) always exists since: 1. (XX^T) is positive semi-definite 2. \\lambda &gt; 0\nHence (XX^T + λI) is a positive definite matrix (i.e. all eigenvalues are strictly positive - full rank) - indicating its inverse exists.\n\nl = 10\nw_hat_ridge = np.linalg.inv(X @ X.T + l * np.eye(d)) @ X @ y"
  },
  {
    "objectID": "pages/Not06.html#solution-for-lasso-regression",
    "href": "pages/Not06.html#solution-for-lasso-regression",
    "title": "Week 6: Ridge, Lasso Regression & Cross Validation Methods",
    "section": "Solution for Lasso Regression",
    "text": "Solution for Lasso Regression\nThere exists no closed form solution for Lasso, due to the constraint boundaries having points which are not differentiable.\nHence we may use the method of gradient descent, with the help of subgradients to find the Lasso estimator.\nIn this notebook we use scikit-learn’s implementation of Lasso to demonstrate the effect of the Lasso objective on our estimator.\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nlambdas = np.logspace(-4, 3 , 200)\n\nfrom sklearn.linear_model import Lasso\n\ncoefs = []\nfor l in lambdas:\n  lasso = Lasso(alpha=l, max_iter=int(1e3)).fit(X.T, y)\n  coefs.append(lasso.coef_)\n\nfig, ax = plt.subplots(figsize=(15, 5))\n\nax.plot(lambdas, coefs)\nax.set_xscale(\"log\")\n# ax.set_xlim(ax.get_xlim()[::-1])  # reverse axis\nplt.xlabel(\"λ (log scale)\")\nplt.ylabel(\"weights\")\nplt.title(\"Lasso coefficients w.r.t λ\")\nplt.axis(\"tight\")\nplt.show()"
  },
  {
    "objectID": "pages/Not06.html#validation",
    "href": "pages/Not06.html#validation",
    "title": "Week 6: Ridge, Lasso Regression & Cross Validation Methods",
    "section": "Validation",
    "text": "Validation\n\nWe partition our dataset (features and labels) into 2 - the train and validation sets respectively.\nWe construct a set of candidate values for λ and find their corresponding estimators \\hat{\\mathbf{w}}_{\\lambda - ML} using the train dataset.\nWe then use these estimators on the validation dataset and find the MSE.\nWe choose the λ that gives the least MSE.\n\n\ncandidate_lambdas = list(np.logspace(-9, 2, 1000))\n\nX_train, X_val, y_train, y_val = X[:, :int(0.66*n)], X[:, int(0.66*n):], y[:int(0.66*n)], y[int(0.66*n):]\n\ndef validate(X_train, X_val, y_train, y_val, l):\n\n  w_hat_l_ml = np.linalg.pinv(X_train @ X_train.T + l * np.eye(d)) @ X_train @ y_train\n  l_mse = np.linalg.norm(X_val.T @ w_hat_l_ml - y_val)**2 / len(y_val)\n\n  return l_mse\n\nlst = [validate(X_train, X_val, y_train, y_val, l) for l in candidate_lambdas]\n\nfig, ax = plt.subplots(figsize=(15, 5))\n\nax.plot(candidate_lambdas, lst)\nax.set_xscale(\"log\")\n\nbest_lambda, best_loss = min(zip(candidate_lambdas, lst), key=lambda i: i[-1])\nplt.axvline(x=best_lambda, color='red')\n\nplt.xlabel(\"λ (log scale)\")\nplt.ylabel(\"Validation Loss\")\nplt.title(\"Validation\")\nplt.axis(\"tight\");\n\nprint(f'Best Lambda: {best_lambda}')\n\nBest Lambda: 0.0986265846131283"
  },
  {
    "objectID": "pages/Not06.html#k-fold-cross-validation",
    "href": "pages/Not06.html#k-fold-cross-validation",
    "title": "Week 6: Ridge, Lasso Regression & Cross Validation Methods",
    "section": "K-Fold Cross Validation",
    "text": "K-Fold Cross Validation\n\nPartition dataset into K sets.\nFor each candidate λ:\n\nFor i=1 to K rounds:\n\nChoose the ith partition as the validation set.\nConsider the union of the remaining sets as the train set.\nFollow aforementioned Cross Validation procedure to obtain MSE for chosen λ\n\nReturn the average MSE\n\nChoose the λ that gives the minimum average MSE.\n\n\ncandidate_lambdas = list(np.logspace(-9, 2, 1000))\n\ndef KFold(K, l):\n\n  # Construct Folds\n  folds = []\n  n = X.shape[1]//K\n\n  for i in range(K):\n    folds.append((X[:, i*n:(i+1)*n], y[i*n:(i+1)*n]))\n\n  l_mse = np.array([])\n  # Cross validate over every validation partition\n\n  for i in range(K):\n\n    X_val, y_val = folds[i]\n\n    X_train, y_train = np.array([[] for _ in range(d)]), np.array([])\n\n    for j in range(K):\n      if i != j:\n        X_j, y_j = folds[j]\n        X_train = np.column_stack((X_train, X_j))\n        y_train = np.concatenate((y_train, y_j))\n\n    l_mse = np.append(l_mse, validate(X_train, X_val, y_train, y_val, l))\n\n  return(l_mse.mean())\n\nlst = []\nfor l in candidate_lambdas:\n  a = KFold(10, l)\n  lst.append(a)\n  # print(l, '|', round(a, 10))\n\n\nfig, ax = plt.subplots(figsize=(15, 5))\n\nax.plot(candidate_lambdas, lst)\nax.set_xscale(\"log\")\n\nbest_lambda, best_loss = min(zip(candidate_lambdas, lst), key=lambda i: i[-1])\nplt.axvline(x=best_lambda, color='red')\n\nplt.xlabel(\"λ (log scale)\")\nplt.ylabel(\"K-Fold Loss\")\nplt.title(\"K-Fold CV with K=10\")\nplt.axis(\"tight\");\n\nprint(f'Best Lambda: {best_lambda}')\n\nBest Lambda: 1.0399609139541203e-06"
  },
  {
    "objectID": "pages/Not06.html#leave-one-out-cross-validation",
    "href": "pages/Not06.html#leave-one-out-cross-validation",
    "title": "Week 6: Ridge, Lasso Regression & Cross Validation Methods",
    "section": "Leave-one-out Cross Validation",
    "text": "Leave-one-out Cross Validation\n\nJust K-Fold Validation, but with K set to the number of samples in our dataset.\nIn other words, for n rounds, we choose just 1 element as the validation set, and the rest to be our train set.\nIt is a computationally expensive procedure to perform, although it results in a reliable and unbiased estimate of model performance.\n\n\ncandidate_lambdas = list(np.logspace(-9, 2, 1000))\nlst = [KFold(100, l) for l in candidate_lambdas]\n\nfig, ax = plt.subplots(figsize=(15, 5))\n\nax.plot(candidate_lambdas, lst)\nax.set_xscale(\"log\")\n\nbest_lambda, best_loss = min(zip(candidate_lambdas, lst), key=lambda i: i[-1])\nplt.axvline(x=best_lambda, color='red')\n\nplt.xlabel(\"λ (log scale)\")\nplt.ylabel(f\"K-Fold (K={n}) Loss\")\nplt.title(\"LOOCV\")\nplt.axis(\"tight\");\n\nprint(f'Best Lambda: {best_lambda}')\n\nBest Lambda: 5.659170163246243e-07"
  },
  {
    "objectID": "pages/Not08.html",
    "href": "pages/Not08.html",
    "title": "Week 5: Naive Bayes and Gaussian Naive Bayes Algorithm",
    "section": "",
    "text": "Colab Link: Click here!"
  },
  {
    "objectID": "pages/Not08.html#introduction-to-naive-bayes-algorithm",
    "href": "pages/Not08.html#introduction-to-naive-bayes-algorithm",
    "title": "Week 5: Naive Bayes and Gaussian Naive Bayes Algorithm",
    "section": "Introduction to Naive Bayes Algorithm",
    "text": "Introduction to Naive Bayes Algorithm\nThe Naive Bayes algorithm is a classification technique based on Bayes’ theorem with the assumption of independence between every pair of features. This notebook demonstrates the implementation of the Naive Bayes algorithm on a binary dataset and introduces the concept of Laplace Smoothing to avoid zero probability estimates.\n\nDataset Description\nThe dataset consists of n binary feature vectors \\{\\mathbf{x}_{1} ,\\dotsc ,\\mathbf{x}_{n} \\}, where each feature vector \\mathbf{x}_{i} \\in \\{0,1\\}^{d}, and their corresponding labels \\{y_{1} ,\\dotsc ,y_{n} \\}, where each label y_{i} \\in \\{0,1\\}.\nHere’s a representation of the dataset:\n\\begin{equation*}\n\\mathbf{X} =\\begin{bmatrix}\n1 & 1 & 0 & 0\\\\\n0 & 1 & 0 & 1\\\\\n0 & 0 & 0 & 1\n\\end{bmatrix} \\quad \\mathbf{y} \\ =\\ \\begin{bmatrix}\n1\\\\\n1\\\\\n0\\\\\n0\n\\end{bmatrix}\n\\end{equation*}\nWe can represent this dataset in Python as follows:\n\nimport numpy as np\n\nX = np.array([[1,0,0],[1,1,0],[0,0,0],[0,1,1]]).T\ny = np.array([1,1,0,0])\n\n\nX, y\n\n(array([[1, 1, 0, 0],\n        [0, 1, 0, 1],\n        [0, 0, 0, 1]]),\n array([1, 1, 0, 0]))\n\n\n\n\nLaplace Smoothing\nIn some cases, estimating the Gaussian Mixture Model (GMM) parameters for our dataset may result in parameters with zero values. To avoid this, we apply Laplace Smoothing.\n\nX = np.hstack((X, np.array([[1,1,1],[1,1,1],[0,0,0],[0,0,0]]).T))\ny = np.hstack((y.T, np.array([1,0,1,0])))\n\n\nX, y\n\n(array([[1, 1, 0, 0, 1, 1, 0, 0],\n        [0, 1, 0, 1, 1, 1, 0, 0],\n        [0, 0, 0, 1, 1, 1, 0, 0]]),\n array([1, 1, 0, 0, 1, 0, 1, 0]))\n\n\nAfter applying Laplace smoothing, we ensure that the parameters do not end up as zero probability estimates.\n\n\nCharacteristics of Naive Bayes Algorithm\nThe Naive Bayes Algorithm is characterized by: 1. Class Conditional Independence Assumption: Each feature is assumed to be independent of each other given the class label. 2. Bayes Rule: It is used to compute the posterior probability from the prior probability, the likelihood, and the evidence.\nBased on the first characteristic, we get \\displaystyle 2d+1 parameters where \\displaystyle d is the number of dimensions of the feature set.\n\n\nParameter Estimation using Maximum Likelihood Estimation (MLE)\nThe estimates of the parameters are given by,\n\\displaystyle \\hat{p} =\\displaystyle \\frac{1}{n}\\sum _{i=1}^{n} y_{i}\n\\displaystyle \\hat{p}_{j}^{y} =\\frac{{\\displaystyle \\sum _{i=1}^{n}\\mathbb{1} (f_{j}^{i} =1,y_{i} =y)}}{{\\displaystyle \\sum _{i=1}^{n}\\mathbb{1} (y_{i} =y)}} \\ \\ \\forall j\\in \\{1,2,\\dotsc ,d\\}\\ \\ \\forall y\\in \\{0,1\\}\nWe can use the following code to calculate the estimate of \\hat{p}:\n\np_hat = y.mean()\np_hat\n\n0.5\n\n\nWe can define a Python function estimator to calculate these estimates:\n\ndef estimator(X, y, c, d):\n    return X[d, y == c].mean()\n\nAnd we can use this function to compute \\hat{p}_{j}^{y} for all j and y:\n\np_hat_0_1 = estimator(X, y, 0, 0)\np_hat_0_2 = estimator(X, y, 0, 1)\np_hat_0_3 = estimator(X, y, 0, 2)\np_hat_1_1 = estimator(X, y, 1, 0)\np_hat_1_2 = estimator(X, y, 1, 1)\np_hat_1_3 = estimator(X, y, 1, 2)\n\n\np_hat_0_1, p_hat_0_2, p_hat_0_3, p_hat_1_1, p_hat_1_2, p_hat_1_3\n\n(0.25, 0.5, 0.5, 0.75, 0.5, 0.25)\n\n\n\n\nPrediction using Naive Bayes\nWe predict y_{test} =1 if the following inequality holds:\n\\begin{equation*}\n\\left(\\prod _{i=1}^{d} (\\hat{p}_{i}^{1} )^{f_{i}} (1-\\hat{p}_{i}^{1} )^{1-f_{i}}\\right)\\hat{p} \\geq \\left(\\prod _{i=1}^{d} (\\hat{p}_{i}^{0} )^{f_{i}} (1-\\hat{p}_{i}^{0} )^{1-f_{i}}\\right) (1-\\hat{p} )\n\\end{equation*}\nOtherwise, we predict y_{test} =0.\nWe can implement a Python function predict_probs to calculate the probabilities for each class:\n\ndef predict_probs(x, X, y):\n    d = X.shape[0]\n    p0, p1 = 1, 1\n\n    for i in range(d):\n        p0 *= (estimator(X, y, 0, i)**(x[i]))*((1-estimator(X, y, 0, i))**(1-(x[i])))\n        p1 *= (estimator(X, y, 1, i)**(x[i]))*((1-estimator(X, y, 1, i))**(1-(x[i])))\n\n    p0 *= p_hat\n    p1 *= p_hat\n\n    return p1, p0\n\nAnd then we can use this function to compute the probabilities for each data point in the dataset:\n\nfor i in range(X.shape[1]):\n    p1, p0 = predict_probs(X[:, i], X, y)\n    print(f\"y_{i}: Probability for Class 1: {np.round(p1, 3)};\\tProbability for Class 0: {np.round(p0, 3)}\")\n\ny_0: Probability for Class 1: 0.141;    Probability for Class 0: 0.031\ny_1: Probability for Class 1: 0.141;    Probability for Class 0: 0.031\ny_2: Probability for Class 1: 0.047;    Probability for Class 0: 0.094\ny_3: Probability for Class 1: 0.016;    Probability for Class 0: 0.094\ny_4: Probability for Class 1: 0.047;    Probability for Class 0: 0.031\ny_5: Probability for Class 1: 0.047;    Probability for Class 0: 0.031\ny_6: Probability for Class 1: 0.047;    Probability for Class 0: 0.094\ny_7: Probability for Class 1: 0.047;    Probability for Class 0: 0.094"
  },
  {
    "objectID": "pages/Not08.html#gaussian-naive-bayes-algorithm",
    "href": "pages/Not08.html#gaussian-naive-bayes-algorithm",
    "title": "Week 5: Naive Bayes and Gaussian Naive Bayes Algorithm",
    "section": "Gaussian Naive Bayes Algorithm",
    "text": "Gaussian Naive Bayes Algorithm\nThe Gaussian Naive Bayes algorithm extends the Naive Bayes algorithm to continuous data, where the features are assumed to be drawn from a Gaussian distribution.\nGiven a dataset \\{\\mathbf{x}_{1} ,\\dotsc ,\\mathbf{x}_{n} \\} where \\mathbf{x}_{i} \\in \\mathbb{R}^{d}, let \\{y_{1} ,\\dotsc ,y_{n} \\} be the labels, where y_{i} \\in \\{0,1\\}.\nHere’s a representation of the dataset:\n\\begin{equation*}\n\\mathbf{X} =\\begin{bmatrix}\n-2 & -4 & -3 & -4 & -6 & -5\\\\\n4 & 4 & 5 & 1 & 1 & 2\n\\end{bmatrix} \\quad \\mathbf{y} \\ =\\ \\begin{bmatrix}\n1\\\\\n1\\\\\n1\\\\\n0\\\\\n0\\\\\n0\n\\end{bmatrix}\n\\end{equation*}\nWe can represent this dataset in Python as follows:\n\nX = np.array([[-3,4], [-4,4], [-2,3], [-4,1], [-5,1], [-5,0]])\ny = np.array([1,1,1,0,0,0])\n\n\nX.T, y\n\n(array([[-3, -4, -2, -4, -5, -5],\n        [ 4,  4,  3,  1,  1,  0]]),\n array([1, 1, 1, 0, 0, 0]))\n\n\n\nData Visualization\nWe can visualize the data points using a scatter plot:\n\nimport matplotlib.pyplot as plt\n\nplt.scatter(X.T[0, y == 1], X.T[1, y == 1], c=\"r\")\nplt.scatter(X.T[0, y == 0], X.T[1, y == 0], c=\"g\");\nplt.grid()\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.axhline(c='k')\nplt.axvline(c='k');\n\n\n\n\n\n\nParameter Estimation\nWe assume that the data points are drawn from a Gaussian distribution, i.e., P(\\mathbf{x}|y=1)\\sim\\mathcal{N}(\\mu_1,\\Sigma_1) and P(\\mathbf{x}|y=0)\\sim\\mathcal{N}(\\mu_0,\\Sigma_0). We further assume that the covariances are equal.\nThe parameters to be estimated are \\hat{p}, \\mu_0, \\mu_1, \\Sigma_0, and \\Sigma_1.\nUsing Maximum Likelihood Estimation, we get the following results:\n\\begin{align*}\n\\hat{p}&=\\frac{1}{n}\\sum_{i=1}^ny_i \\\\\n\\hat{\\mu}_1 &= \\frac{\\displaystyle \\sum_{i=1}^n\\mathbb{1}(y_i=1)*x_i}{\\displaystyle \\sum_{i=1}^n\\mathbb{1}(y_i=1)} \\\\\n\\hat{\\mu}_0 &= \\frac{\\displaystyle \\sum_{i=1}^n\\mathbb{1}(y_i=0)*x_i}{\\displaystyle \\sum_{i=1}^n\\mathbb{1}(y_i=0)} \\\\\n\\hat{\\Sigma}_1 &= \\frac{1}{n} \\displaystyle \\sum_{i=1}^n(\\mathbb{1}(y_i=1)*x_i-\\hat{\\mu}_1)(\\mathbb{1}(y_i=1)*x_i-\\hat{\\mu}_1)^T \\\\\n\\hat{\\Sigma}_0 &= \\frac{1}{n} \\displaystyle \\sum_{i=1}^n(\\mathbb{1}(y_i=0)*x_i-\\hat{\\mu}_0)(\\mathbb{1}(y_i=0)*x_i-\\hat{\\mu}_0)^T\n\\end{align*}\n\n\nGaussian Naive Bayes Implementation\nWe can implement the Gaussian Naive Bayes algorithm in Python as follows:\n\nfrom scipy.stats import multivariate_normal\n\nclass GaussianNaiveBayes:\n    def __init__(self):\n        self.classes = None\n        self.class_priors = None\n        self.class_means = None\n        self.class_covs = None\n\n    def fit(self, X, y):\n        self.classes = np.unique(y)\n        n_classes = len(self.classes)\n        n_features = X.shape[1]\n\n        self.class_priors = np.zeros(n_classes)\n        self.class_means = np.zeros((n_classes, n_features))\n        self.class_covs = np.zeros((n_classes, n_features, n_features))\n\n        for i, c in enumerate(self.classes):\n            X_c = X[y == c]\n            self.class_priors[i] = X_c.shape[0] / X.shape[0]\n            self.class_means[i] = np.mean(X_c, axis=0)\n            self.class_covs[i] = np.dot((X_c - self.class_means[i]).T, (X_c - self.class_means[i])) / X_c.shape[0]\n\n    def predict(self, X):\n        posteriors = self._calculate_posteriors(X)\n        return self.classes[np.argmax(posteriors, axis=0)]\n\n    def predict_proba(self, X):\n        posteriors = self._calculate_posteriors(X)\n        return np.exp(posteriors) / np.sum(np.exp(posteriors), axis=0)\n\n    def _calculate_posteriors(self, X):\n        posteriors = []\n        for i, c in enumerate(self.classes):\n            prior = np.log(self.class_priors[i])\n            likelihood = multivariate_normal.logpdf(X, self.class_means[i], self.class_covs[i])\n            posterior = prior + likelihood\n            posteriors.append(posterior)\n        posteriors = np.array(posteriors)\n        return posteriors\n\nWe can fit the model to our data:\n\nmodel = GaussianNaiveBayes()\nmodel.fit(X, y)\n\n\nmodel.class_means.T\n\narray([[-4.66666667, -3.        ],\n       [ 0.66666667,  3.66666667]])\n\n\n\nmodel.class_covs\n\narray([[[ 0.22222222,  0.11111111],\n        [ 0.11111111,  0.22222222]],\n\n       [[ 0.66666667, -0.33333333],\n        [-0.33333333,  0.22222222]]])\n\n\n\n\nPrediction using Gaussian Naive Bayes\nWe predict y_{i} =1 if the following inequality holds:\n\\begin{align*}\nf(x_{i} ;\\hat{\\mu }_{1} ,\\hat{\\Sigma }_{1} )\\hat{p} & \\geq f(x_{i} ;\\hat{\\mu }_{0} ,\\hat{\\Sigma }_{0} )(1-\\hat{p} )\\\\\ne^{-(x_{i} -\\hat{\\mu }_{1} )^{T}\\hat{\\Sigma }_{1}^{-1} (x_{i} -\\hat{\\mu }_{1} )}\\hat{p} & \\geq e^{-(x_{i} -\\hat{\\mu }_{0} )^{T}\\hat{\\Sigma }_{0}^{-1} (x_{i} -\\hat{\\mu }_{0} )} (1-\\hat{p} )\\\\\n-(x_{i} -\\hat{\\mu }_{1} )^{T}\\hat{\\Sigma }_{1}^{-1} (x_{i} -\\hat{\\mu }_{1} )+\\log (\\hat{p} ) & \\geq -(x_{i} -\\hat{\\mu }_{0} )^{T}\\hat{\\Sigma }_{0}^{-1} (x_{i} -\\hat{\\mu }_{0} )+\\log (1-\\hat{p} )\n\\end{align*}\n\\begin{equation*}\nx_{i}^{T}\\left(\\hat{\\Sigma }_{1}^{-1} -\\hat{\\Sigma }_{0}^{-1}\\right) x_{i} -2\\left(\\hat{\\mu }_{1}^{T}\\hat{\\Sigma }_{1}^{-1} -\\hat{\\mu }_{0}^{T}\\hat{\\Sigma }_{0}^{-1}\\right) x_{i} +\\left(\\hat{\\mu }_{0}^{T}\\hat{\\Sigma }_{0}^{-1}\\hat{\\mu }_{0} -\\hat{\\mu }_{1}^{T}\\hat{\\Sigma }_{1}^{-1}\\hat{\\mu }_{1}\\right) +log\\left(\\frac{1-\\hat{p}}{\\hat{p}}\\right) \\geq 0\n\\end{equation*}\nWe can use the predict_proba method of our model to compute the probabilities for each data point in the dataset:\n\nfor i in range(X.shape[0]):\n    p0, p1 = model.predict_proba(X[i].reshape((1,-1))).reshape((-1,))\n    print(f\"y_{i}: Probability for Class 1: {np.round(p1, 3)};\\tProbability for Class 0: {np.round(p0, 8)}\")\n\ny_0: Probability for Class 1: 1.0;  Probability for Class 0: 0.0\ny_1: Probability for Class 1: 1.0;  Probability for Class 0: 0.0\ny_2: Probability for Class 1: 1.0;  Probability for Class 0: 2e-08\ny_3: Probability for Class 1: 0.0;  Probability for Class 0: 1.0\ny_4: Probability for Class 1: 0.0;  Probability for Class 0: 1.0\ny_5: Probability for Class 1: 0.0;  Probability for Class 0: 1.0\n\n\n\n\nDecision Boundary Visualization\nWe can visualize the decision boundary of our model using a contour plot:\n\ndef plot_decision_boundary(model, X, y):\n    # Define the range of the grid\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    h = 0.02  # step size in the mesh\n\n    # Generate a grid of points\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n\n    # Predict the class for each point in the grid\n    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    # Create a contour plot\n    plt.contourf(xx, yy, Z, alpha=0.8)\n\n    # Plot the training data points\n    markers = ['o', 'x', 's', '^', 'v']  # markers for each class\n    colors = ['r', 'g']\n    for i, class_label in enumerate(np.unique(y)):\n        plt.scatter(X[y == class_label, 0], X[y == class_label, 1], marker=markers[i], label=class_label, c=colors[i])\n\n    plt.xlabel('Feature 1')\n    plt.ylabel('Feature 2')\n    plt.title('Decision Boundary')\n    plt.legend()\n    plt.show()\n\nAnd then we can use this function to plot the decision boundary of our model:\n\nplot_decision_boundary(model, X, y)"
  },
  {
    "objectID": "pages/Not08.html#real-world-dataset-wine-dataset",
    "href": "pages/Not08.html#real-world-dataset-wine-dataset",
    "title": "Week 5: Naive Bayes and Gaussian Naive Bayes Algorithm",
    "section": "Real World Dataset: Wine Dataset",
    "text": "Real World Dataset: Wine Dataset\nIn this section, we will explore the Wine dataset, which is a real-world dataset used for classification tasks. The dataset contains various attributes related to wine samples, and our goal is to classify these samples into two categories: Class 0 and Class 1.\n\nDataset Description\n\nFeatures: The dataset includes several features, but for this analysis, we will focus on two specific attributes: ‘proline’ and ‘hue.’ These attributes represent different characteristics of the wine samples.\nLabels: The target variable, denoted as ‘y,’ assigns each sample to one of two classes, Class 0 or Class 1.\n\n\n\nData Preprocessing\nBefore using the dataset, we perform some data preprocessing steps to standardize the features. Standardization is a common practice in machine learning to ensure that all features have the same scale. This can improve the performance of our models.\n\nfrom sklearn.datasets import load_wine\nfrom sklearn.preprocessing import StandardScaler\n\n# Load the Wine dataset\nX, y = load_wine(return_X_y=True, as_frame=True)\n\n# Select samples belonging to Class 0 and Class 1\nX, y = X[y &lt; 2], y[y &lt; 2]\n\n# Choose 'proline' and 'hue' as the features of interest\nX = X[['proline', 'hue']]\n\n# Standardize the features\nstd_scaler = StandardScaler()\nX = std_scaler.fit_transform(X)\n\n\n\nExploratory Data Analysis\nTo gain insights into the dataset, we can visualize the samples based on their features. The following scatter plot represents the wine samples in a 2D space, with ‘proline’ on the x-axis and ‘hue’ on the y-axis. Different colors indicate the two classes, making it easier to observe any potential patterns or separations between the classes.\n\n# Scatter plot to visualize the data\nplt.scatter(X[:, 0], X[:, 1], c=y)\nplt.xlabel('Proline')\nplt.ylabel('Hue')\nplt.title('Scatter Plot of Wine Dataset')\n\nText(0.5, 1.0, 'Scatter Plot of Wine Dataset')\n\n\n\n\n\n\n\nModelling for Wine Dataset\nNow that we have preprocessed the data and gained some initial insights, we can build a classification model using Gaussian Naive Bayes.\n\nfrom scipy.stats import multivariate_normal\n\n# Create an instance of the Gaussian Naive Bayes model\nwine_model = GaussianNaiveBayes()\n\n# Fit the model to the preprocessed dataset\nwine_model.fit(X, y)\n\n\n\nDecision Boundary\nTo understand how the model separates the two classes, we can visualize the decision boundary. The decision boundary represents the region where the model assigns different class labels. In our case, it shows how the model distinguishes Class 0 from Class 1 based on the ‘proline’ and ‘hue’ features.\n\n# Plot the decision boundary of the Gaussian Naive Bayes model\nplot_decision_boundary(wine_model, X, y)\n\n\n\n\nThis visualization helps us understand how the model’s predictions are made and how it classifies new wine samples based on their ‘proline’ and ‘hue’ attributes. It also allows us to see any regions where the model might make uncertain predictions.\nIn summary, the Wine dataset provides a real-world example of applying the Gaussian Naive Bayes algorithm for classification, and visualizations help us understand the model’s behavior in a 2D feature space."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "To introduce the main methods and models used in machine learning problems of regression, classification and clustering. To study the properties of these models and methods and learn about their suitability for different problems."
  },
  {
    "objectID": "about.html#about-the-course",
    "href": "about.html#about-the-course",
    "title": "About",
    "section": "",
    "text": "To introduce the main methods and models used in machine learning problems of regression, classification and clustering. To study the properties of these models and methods and learn about their suitability for different problems."
  },
  {
    "objectID": "about.html#what-youll-learn",
    "href": "about.html#what-youll-learn",
    "title": "About",
    "section": "What you’ll learn",
    "text": "What you’ll learn\n\nDemonstrating In depth understanding of machine learning algorithms - model, objective or loss function, optimization algorithm and evaluation criteria.\nTweaking machine learning algorithms based on the outcome of experiments - what steps to take in case of underfitting and overfitting.\nBeing able to choose among multiple algorithms for a given task.\nDeveloping an understanding of unsupervised learning techniques."
  },
  {
    "objectID": "about.html#course-instructors",
    "href": "about.html#course-instructors",
    "title": "About",
    "section": "Course Instructors",
    "text": "Course Instructors\nProf. Arun Rajkumar\nAssistant Professor, Department of Computer Sciences & Engineering, IIT Madras"
  },
  {
    "objectID": "about.html#author",
    "href": "about.html#author",
    "title": "About",
    "section": "Author",
    "text": "Author\n\nSherry\n\n\nCo-Authors\n\nA Aniruddha\nVivek Sivaramakrishnan"
  },
  {
    "objectID": "about.html#study-material",
    "href": "about.html#study-material",
    "title": "About",
    "section": "Study Material",
    "text": "Study Material\nThe primary study material for this course is the set of videos and assignments posted on the course page. The prescribed textbook for this course is:\n\nPattern Classification by David G. Stork, Peter E. Hart, and Richard O. Duda\nPattern Recognition and Machine Learning by Christopher M. Bishop\nThe Elements of Statistical Learning: Data Mining, Inference, and Prediction by Trevor Hastie, Robert Tibshirani, and Jerome Friedman\n\nThe learners are advised to make best use of the interaction sessions with the course support members to clarify their doubts.\nTo know more about course syllabus, Instructors and course contents, please click on the below link\nhttps://ds.study.iitm.ac.in/course_pages/BSCCS2007.html\nPlease click on the below tab to view the Course Specific Calendar:\nhttps://calendar.google.com/calendar/u/2?cid=Y19vODg1amdtMTVrbjJzZW01dGlkM2szcjhnNEBncm91cC5jYWxlbmRhci5nb29nbGUuY29t"
  }
]