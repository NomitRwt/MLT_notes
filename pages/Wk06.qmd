---
title: "Supervised Learning - Regression - Ridge/LASSO"
---

# Goodness of Maximum Likelihood Estimator for linear regression
Given a dataset $\{x_1, \ldots, x_n\}$ where $x_i \in \mathbb{R}^d$, let $\{y_1, \ldots, y_n\}$ be the labels, where $y_i \in \mathbb{R}$.
$$
y|X = w^Tx + \epsilon
$$
where $\epsilon \sim \mathcal{N}(0,\sigma^2)$ and $w \in \mathbb{R}^d$. Let $\hat{w}_{ML}$ signify the maximum likelihood parameter for linear regression.
$$
\hat{w}_{ML}=w^*=(XX^T)^+Xy
$$
$\therefore$ To measure how good our parameter is, we use the follow:
$$
\mathbb{E} [|| \hat{w}_{ML} - w ||^2_2]
$$
This is known as the Mean Squared Error (MSE) and turns out to be equal to
$$
\mathbb{E} [|| \hat{w}_{ML} - w ||^2_2] = \sigma^2 *trace((XX^T)^{-1})
$$

# Cross-validation for minimizing MSE
Let the eigenvalues of $XX^T$ be $\{\lambda_1, \ldots, \lambda_d\}$. Hence the eigenvalues of $(XX^T)^{-1}$ are $\{\frac{1}{\lambda_1}, \ldots, \frac{1}{\lambda_d}\}$. 

$\therefore$ The MSE is,
$$
\mathbb{E} [|| \hat{w}_{ML} - w ||^2_2] = \sigma^2 \sum_{i=1}^d  \frac{1}{\lambda_i}
$$
Consider the following estimator,
$$
\hat{w}_{new}=(XX^T + \lambda I)^+Xy
$$
where $\lambda \in \mathbb{R}$ and $I \in \mathbb{R}^{d\times d}$ is the Identity Matrix. Using this we get,
$$
trace((XX^T + \lambda I)^{-1}) = \sum_{i=1}^d  \frac{1}{\lambda_i + \lambda}
$$
According to the Existence Theorem,