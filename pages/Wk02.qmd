---
title: "Unsupervised Learning - Representation learning - Kernel PCA"
---

PDF Link: [notes](../notes/Wk02.pdf)

# Introduction
For a given dataset $\mathbf{X} \in \mathbb{R} ^{d \times n}$, the covariance matrix is $\mathbf{C} \in \mathbb{R} ^{d \times d}$. PCA for this dataset can have the following two problems:

* **Time Complexity**: The algorithimic complexity of finding the eigenvalues and eigenvectors of $\mathbf{C}$ is $O(d ^3)$. Hence, as $d$ grows, the time taken becomes very large.
* **Non-Linear Dataset**: The dataset may lie in a non-linear subspace. As PCA tries to get linear combination of Principal Components, non-linear datasets may result in non-optimal outputs.

# Reducing the Time Complexity to find Eigenvalues and Eigenvectors
Let's take a dataset $\mathbf{X}$ with a large number of features($d$) i.e. $[d >> n]$ where 

- $d$: no. of features
- $n$: no. of datapoints
$$
\mathbf{X}=\left [
\begin{array}{ccccc}
    | & | & | & & | \\
    \mathbf{x}_1 & \mathbf{x}_2 & \mathbf{x}_3 & \ldots & \mathbf{x}_n \\
    | & | & | & & |
\end{array}
\right ]
\quad \mathbf{X} \in \mathbb{R} ^{d \times n} 
$$

The covariance matrix $\mathbf{C}$ of $\mathbf{X}$ is given by,
$$
\mathbf{C}=\frac{1}{n}(\mathbf{X}  \mathbf{X} ^T) \text{\quad where } \mathbf{C} \in \mathbb{R}^{d \times d} \quad \ldots[1]
$$
Let $\mathbf{w}_k$ be the the eigenvector corresponding to the $k ^{th}$ largest eigenvalue $\lambda _k$ of $\mathbf{C}$.

We know
$$
\mathbf{C}\mathbf{w}_k=\lambda _k \mathbf{w}_k
$$
Substituting $\mathbf{C}$ from $[1]$ in the above equation, and solving for $\mathbf{w}_k$, we get
$$
\mathbf{w}_k = \sum _{i=1} ^n (\frac{\mathbf{x}_i ^T \mathbf{w}_k}{n \lambda _k})\cdot{\mathbf{x}_i}
$$
$\therefore$ $\mathbf{w}_k$ is a linear combination of datapoints! Hence, we can say
$$
\mathbf{w}_k= \mathbf{X} \alpha _k \text{\quad for some } \alpha _k \quad \ldots [2]
$$
Let $\mathbf{X} ^T \mathbf{X} = \mathbf{K}$ where $\mathbf{K} \in \mathbb{R} ^{n \times n}$. We shall use this to solve for $\alpha _k$. After some algebra (Refer to lectures), we get:
$$
\mathbf{K} \alpha _k = (n \lambda _k) \alpha _k
$$
We know that the non-zero eigenvalues of $\mathbf{X}\mathbf{X}^T$ and $\mathbf{X}^T \mathbf{X}$ are the same according to the Spectral Theorem.

**Explanation for the above**: If $\lambda \ne 0$ is an eigenvalue of $\mathbf{X}\mathbf{X}^T$ with eigenvector $\mathbf{w}$, then $\mathbf{X}\mathbf{X}^T\mathbf{w}=\lambda \mathbf{w}$.

Multiply both sides by $\mathbf{X}^T$ to get $$\mathbf{X}^T \mathbf{X}(\mathbf{X}^T\mathbf{w})=\lambda \mathbf{X}^T\mathbf{w}$$
But then we see that $\lambda$ is still an eigenvalue for $\mathbf{X}^T\mathbf{X}$ and the corresponding eigenvector is simply $\mathbf{X}^T\mathbf{w}$.

Let $\beta _k$ be the the eigenvector corresponding to the $k ^{th}$ largest eigenvalue $n \lambda _k$ of $\mathbf{K}$.

On solving the eigen equation for $\mathbf{K}$, we get
$$
\alpha _k= \frac{\beta _k}{\sqrt{n \lambda _k}} \quad \ldots [3]
$$
Using equations [2] and [3], we can get the eigenvalues and eigenvectors of $\mathbf{C}$ using $\mathbf{K}$ and hence, bringing the time complexity from $O(d ^3)$ to $O(n^3)$.

# Finding PCA for Non-Linear Relationships
## Transforming Features
We solve the problem of non-linear relationships by mapping them to higher dimensions.
$$
\mathbf{x} \to \phi(\mathbf{x}) \quad \mathbb{R} ^d \to \mathbb{R} ^D \quad \text{where } [D >> d]
$$
To compute $D$:

Let $\mathbf{x}=\left [
\begin{array} {cc}
    f_1 & f_2
\end{array}
\right ]$ be features of a dataset containing datapoints lying on a curve of degree two in a two-dimensional space.

To make it linear from quadratic, we map the features to 
$\phi(\mathbf{x})=\left [
\begin{array} {cccccc}
    1 & f_1^2 & f_2^2 & f_1f_2 & f_1 & f_2
\end{array}
\right ]$

Mapping $d$ features to the polygonial power $p$ gives $^{d+p} C_d$ new features.

**Issue**: Finding $\phi(\mathbf{x})$ may be very hard.

Solution for this issue is in the next point.

## Kernel Functions
A function that maps $k: \mathbb{R}^d \times \mathbb{R}^d \to \mathbb{R}$, and is a "valid", is called a Kernel Function.

**Proof of a "Valid" Kernel:**

* **Method 1**: Exhibit the map to $\phi$ explicitly. [may be hard]
* **Method 2**: Using Mercer's Theorem:
	+ $k: \mathbb{R}^d \times \mathbb{R}^d \to \mathbb{R}$ is a "valid" kernel if and only if:
		- $k$ is symmetric i.e $k(\mathbf{x},\mathbf{x}') = k(\mathbf{x}',\mathbf{x})$
		- For any dataset $\{\mathbf{x}_1,\mathbf{x}_2,\ldots,\mathbf{x}_n  \}$, the matrix $\mathbf{K} \in \mathbb{R}^{n \times n}$, where $\mathbf{K}_{ij} = k(i,j)$, is Positive Semi-Definite.

**Two Popular Kernel Functions**:

* **Polynomial Kernel**: $k(\mathbf{x},\mathbf{x}') = (\mathbf{x}^T\mathbf{x} + 1)^p$
* **Radial Basis Function Kernel or Gaussian Kernel**: $exp(-\displaystyle \frac{||\mathbf{x}-\mathbf{x}'||^2}{2\sigma^2})$

# Kernel PCA
Let's take a dataset $\mathbf{X}$ with a large number of features($d$) i.e. $[d >> n]$ where

- $d$: no. of features
- $n$: no. of datapoints
$$
\mathbf{X}=\left [
\begin{array}{ccccc}
    | & | & | & & | \\
    \mathbf{x}_1 & \mathbf{x}_2 & \mathbf{x}_3 & \ldots & \mathbf{x}_4 \\
    | & | & | & & |
\end{array}
\right ]
$$

* **Step 1**: Calculate $\mathbf{K} \in \mathbb{R}^{n \times n}$ using a kernel function where $\mathbf{K}_{ij}=k(\mathbf{x}_i,\mathbf{x}_j)$.
* **Step 2**: Center the kernel using the following formula.
    $$
    \mathbf{K}^C=\mathbf{K}-\mathbf{I}\mathbf{K}-\mathbf{K}\mathbf{I}+\mathbf{I}\mathbf{K}\mathbf{I}
    $$
    where $\mathbf{K}^C$ is the centered kernel, and $\mathbf{I} \in \mathbb{R}^{n \times n}$ where all the elements are $\frac{1}{n}$.
* **Step 3**: Compute the eigenvectors $\{\beta _1, \beta _2, \ldots, \beta _l\}$ and eigenvalues $\{n\lambda _1, n\lambda _2, \ldots, n\lambda _l\}$ of $\mathbf{K}^C$ and normalize to get
    $$
    \forall u \quad \alpha _u = \frac{\beta _u}{\sqrt{n \lambda _u}}
    $$
* **Step 4**: Compute $\displaystyle \sum _{j=1} ^{n} \alpha _{kj} \mathbf{K}^C_{ij} \hspace{1em} \forall k$
    $$
    \phi(\mathbf{x}_i)^T\mathbf{w} \in \mathbb{R}^{d} \to 
    \left [
\begin{array}{cccc}
    \displaystyle \sum _{j=1} ^{n} \alpha _{1j} \mathbf{K}^C_{ij} & \displaystyle \sum _{j=1} ^{n} \alpha _{2j} \mathbf{K}^C_{ij} & \ldots & \displaystyle \sum _{j=1} ^{n} \alpha _{nj} \mathbf{K}^C_{ij}
\end{array}
\right ]
    $$

# Credits
* Professor Arun Rajkumar: The content as well as the notations are from his slides and lecture.
